"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[30554],{64861:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"kick-off","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/kick-off","source":"@site/blog-60daysofIA/2024-02-15/kickoff-blog.md","title":"Kick-off #60Days of IA","description":"Combine the power of AI, cloud-scale data, and cloud-native app development to create highly differentiated digital experiences. Develop adaptive, responsive, and personalized experiences by building and modernizing intelligent applications with Azure.","date":"2024-02-15T09:00:00.000Z","formattedDate":"February 15, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":4.69,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-02-15T09:00","slug":"kick-off","title":"Kick-off #60Days of IA","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"Combine the power of AI, cloud-scale data, and cloud-native app development to create highly differentiated digital experiences. Develop adaptive, responsive, and personalized experiences by building and modernizing intelligent applications with Azure.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"nextItem":{"title":"1. Harnessing the power of Intelligent Apps","permalink":"/Cloud-Native/60DaysOfIA/harnessing-the-power-of-intelligent-apps"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/kick-off\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Join us on a learning journey to build intelligent apps on Azure. Read all about the upcoming #BuildIntelligentApps initiative on this post!\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/kick-off\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" /> \\r\\n  <meta name=\\"twitter:description\\" content=\\"Join us on a learning journey to build intelligent apps on Azure. Read all about the upcoming #BuildIntelligentApps initiative on this post!\\" /> \\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/kick-off\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\nLet\u2019s ride the buzz of AI with the focus on building intelligent apps using cloud-native technologies. Build \'#IntelligentApps\' brings to you a learning journey to build your skills on creating differentiated experiences while modernizing your applications. It\u2019s time to \'learn it all\'. \\r\\n\\r\\n## What We\u2019ll Cover\\r\\n\\r\\n* What is Build Intelligent Apps?\\r\\n* How Can I *participate*?\\r\\n* How Can I *skill up*? (in just 60 Days)\\r\\n* **Exercise:** Take the [Build Intelligent Apps Skills Challenge](https://aka.ms/build-ia/csc)\\r\\n\\r\\n![Build intelligent apps](../../static/img/60-days-of-ia/60-days-of-ia-cloud-skills-banner.jpg)\\r\\n\\r\\n## Get Ready To Build #IntelligentApps starting February 19!\\r\\n\\r\\nToday, we kick off with content and activities for you to skill up on all things Intelligent Apps or AI Apps on Azure with content, events, and community interactions! Read on to learn about what is coming!\\r\\n\\r\\n## Explore Our Initiatives\\r\\n\\r\\nWe have a number of initiatives planned for the month to help you learn and skill up on relevant technologies. Click on the links to visit the relevant pages for each.\\r\\n\\r\\n* [#60Days of IA](https://aka.ms/build-ia/60days) - 8 themed weeks of blogs on AI led application development\\r\\n* [Learn Live Series](https://aka.ms/FallForIA/LearnLive) \u2013 8 weekly live episodes on \'Kubernetes\' and \'Serverless\'\\r\\n* [Ask The Expert](https://aka.ms/build-ia/ATE-series) \u2013 join live Q&A sessions with Product Engineering and Advocacy teams\\r\\n* [Cloud Skills Challenge](https://aka.ms/build-ia/csc) \u2013 skill up by competing with peers to complete modules\\r\\n\\r\\n![Build intelligent apps](../../static/img/60-days-of-ia/60-days-of-ia-cloud-skills-modules.png)\\r\\n\\r\\n:::info\\r\\n## Register for the events!\\r\\n\\r\\nWhat are 4 things you can do today, to jumpstart your learning journey?\\r\\n\\r\\n* **Register** for live Q&A sessions (free, online) \\r\\n  * February 29 \u2013 [Ask The Expert: Intelligent Apps with Azure Kubernetes Service](https://aka.ms/intelligent-apps/ate-aks/?ocid=buildia24_60days_blogs)\\r\\n  * March 7 \u2013 [Ask The Expert: Intelligent Apps with Azure Cosmos DB](https://aka.ms/intelligent-apps/ate-cosmos/?ocid=buildia24_60days_blogs)\\r\\n  * March 21 - [Ask The Expert: Intelligent Apps with Azure AI](https://aka.ms/intelligent-apps/ate-ai/?ocid=buildia24_60days_blogs) \\r\\n  * April 4 \u2013 [Ask The Expert: Intelligent Apps with Azure Functions](https://aka.ms/intelligent-apps/ate-functions/?ocid=buildia24_60days_blogs)\\r\\n* **Register** for the [Learn Live Series: Kubernetes Edition](https://aka.ms/intelligent-apps/aks-learnlive?ocid=buildia24_LL_website&ocid=buildia24_60days_blogs) \u2013 weekly live learning \\r\\n  * February 21 \u2013 [Episode 1: Deploying Intelligent Apps with OpenAI on AKS](https://aka.ms/learn-live-building-intelligent-apps-aks-ep1?ocid=buildia24_60days_blogs) \\r\\n  * February 28 \u2013 [Episode 2: Bring Your Own AI Models to Intelligent Apps on AKS with KAITO](https://aka.ms/learn-live-building-intelligent-apps-aks-ep2?ocid=buildia24_60days_blogs)\\r\\n  * March 6 \u2013 [Episode 3: Enhance Observability of Your Intelligent Apps on AKS](https://aka.ms/learn-live-building-intelligent-apps-aks-ep3?ocid=buildia24_60days_blogs)\\r\\n  * March 13 \u2013 [Episode 4: Taking Your Intelligent App Global with AKS](https://aka.ms/learn-live-building-intelligent-apps-aks-ep4?ocid=buildia24_60days_blogs)\\r\\n* **Register** for the [Azure Kubernetes Day at KubeCon, EU](https://aka.ms/aks-day?ocid=buildia24_60days_blogs) to meet the product engineering teams in-person and learn about the new product capabilities for intelligent apps.\\r\\n* **Complete** the [Cloud Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs) to earn a Microsoft Learn badge \u2013 ends on *April 15*!\\r\\n:::\\r\\n\\r\\n## #60Days Of Intelligent Apps\\r\\n\\r\\n[#60Days of IA](https://aka.ms/build-ia/60days) is a series of blog posts grouped into themed weeks - taking you from core concepts to end-to-end solution examples in 60 days. Each blog will provide conceptual lessons paired with exercises and resources to help you reinforce learnings and take next steps.\\r\\n\\r\\nThis series takes you through learning journey in\u202f**eight stages**, each building on the previous week to help you skill up in a beginner-friendly way:\\r\\n\\r\\n* **Week 1**: Power of [Intelligent Applications](https://azure.microsoft.com/en-us/blog/build-next-generation-ai-powered-applications-on-microsoft-azure/?ocid=buildia24_60days_blogs)\\r\\n* **Week 2**: [Azure Kubernetes Service](https://learn.microsoft.com/en-us/azure/aks/?ocid=buildia24_60days_blogs) is the platform of choice for intelligent apps\\r\\n* **Week 3**: [Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction?ocid=buildia24_60days_blogs) is the database of choice for intelligent apps\\r\\n* **Week 4**: [Azure AI](https://learn.microsoft.com/en-us/azure/ai-services/ai-services-and-ecosystem?ocid=buildia24_60days_blogs) is your choice for [Responsible AI](https://www.microsoft.com/en-us/ai/responsible-ai?ocid=buildia24_60days_blogs)\\r\\n* **Week 5**: Building an intelligent [serverless](https://azure.microsoft.com/en-us/solutions/serverless/?ocid=buildia24_60days_blogs) event driven [functions app](https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview?pivots=programming-language-csharp?ocid=buildia24_60days_blogs)\\r\\n* **Week 6**: Build intelligent microservices with [serverless containers](https://learn.microsoft.com/en-us/azure/container-apps/overview?ocid=buildia24_60days_blogs)\\r\\n* **Week 7**: [Platform engineering](https://learn.microsoft.com/en-us/platform-engineering/?ocid=buildia24_60days_blogs) needs for your intelligent apps\\r\\n* **Week 8**: Managing cost for your intelligent apps \\r\\n\\r\\nWe will start with defining intelligent apps and then expand on how to build with cloud-native technologies like [Azure Kubernetes Service](https://azure.microsoft.com/en-us/products/kubernetes-service/?WT.mc_id=javascript-99907-ninarasi&ocid=buildia24_60days_blogs), [Azure Container Apps](https://azure.microsoft.com/en-us/products/container-apps/?WT.mc_id=javascript-99907-ninarasi&?ocid=buildia24_60days_blogs) and [Azure Functions](https://azure.microsoft.com/en-us/products/functions?WT.mc_id=javascript-99907-ninarasi&?ocid=buildia24_60days_blogs), as well as integrate AI and cloud-scale data. You will learn how to build end-to-end scenarios for real world application development based on [reference architectures](https://learn.microsoft.com/en-us/azure/architecture/??ocid=buildia24_60days_blogs). Before we dive deep on intelligent apps, here is a high-level overview of the **Intelligent Apps** landscape on Azure for you to leverage the most comprehensive, trusted cloud to prime the customer and employee experiences.\\r\\n\\r\\n![intelligent apps on Azure](../../static/img/60-days-of-ia/blogs/2024-02-15/intelligent-apps-on-azure.png)\\r\\n\\r\\nBring your applications to a modern application platform in the cloud, which leverages a cloud data platform at scale and agile development methods with DevOps is the best way to prime the customer and employee experiences. Azure offers the latest apps, data, AI and is the most comprehensive, trusted cloud.\\r\\n\\r\\n![intelligent apps](../../static/img/60-days-of-ia/blogs/2024-02-15/intelligent-apps.png)\\r\\n\\r\\n**Containers on Azure**\u202fservices offer you a wide range of capabilities, from simplicity to control to suit your different needs.\\r\\n\\r\\n![containers on azure](../../static/img/fallforia/blogs/2023-09-17/Containers-on-Azure.jpg)\\r\\n\\r\\n![containers on azure](../../static/img/fallforia/blogs/2023-09-17/Containers-on-Azure-2.jpg)\\r\\n\\r\\nTo start with the basics for developing [Kubernetes](https://azure.microsoft.com/en-us/products/kubernetes-service/?WT.mc_id=javascript-99907-ninarasi&ocid=buildia24_60days_blogs) applications, explore [#30Days of CloudNative](https://azure.github.io/Cloud-Native/cnny-2023).\\r\\n\\r\\nCloud-native development when paired with **serverless computing** enhances your solution architecture for building cost optimized, resilient applications.\\r\\n\\r\\n![serverless on Azure](../../static/img/60-days-of-ia/blogs/2024-02-15/serverless-on-azure.jpg)\\r\\n\\r\\nTo start with the basics for [serverless computing](https://azure.microsoft.com/solutions/serverless/?WT.mc_id=javascript-99907-ninarasi&?ocid=buildia24_60days_blogs), explore [#30DaysOfServerless](https://azure.github.io/Cloud-Native/blog).\\r\\n\\r\\n## Let\u2019s Get Started\\r\\n\\r\\nNow you know everything! We hope you are as excited as we are to dive into a full month of active learning and doing! Don\'t forget to\u202f[subscribe](https://azure.github.io/Cloud-Native/60DaysOfIA/rss.xml)\u202ffor updates in your favorite feed reader.\u202f**And, look out for our first Intelligent Apps blog on Monday, February 19!**"},{"id":"harnessing-the-power-of-intelligent-apps","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/harnessing-the-power-of-intelligent-apps","source":"@site/blog-60daysofIA/2024-02-19/harnessing-the-power-of-intelligent-apps.md","title":"1. Harnessing the power of Intelligent Apps","description":"Combine the power of AI, cloud-scale data, and cloud-native app development to create highly differentiated digital experiences. Develop adaptive, responsive, and personalized experiences by building and modernizing intelligent applications with Azure.","date":"2024-02-19T09:00:00.000Z","formattedDate":"February 19, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.185,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-02-19T09:00","slug":"harnessing-the-power-of-intelligent-apps","title":"1. Harnessing the power of Intelligent Apps","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"Combine the power of AI, cloud-scale data, and cloud-native app development to create highly differentiated digital experiences. Develop adaptive, responsive, and personalized experiences by building and modernizing intelligent applications with Azure.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"Kick-off #60Days of IA","permalink":"/Cloud-Native/60DaysOfIA/kick-off"},"nextItem":{"title":"2. Power Up: Crafting an Intelligent Energy Forecasting Application Using Azure Kubernetes Service","permalink":"/Cloud-Native/60DaysOfIA/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/harnessing-the-power-of-intelligent-apps\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this article, we\u2019ll spotlight the success stories of some organizations that have leveraged Microsoft Azure to create and deploy intelligent apps in their workflows and products.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/harnessing-the-power-of-intelligent-apps\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this article, we\u2019ll spotlight the success stories of some organizations that have leveraged Microsoft Azure to create and deploy intelligent apps in their workflows and products.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/harnessing-the-power-of-intelligent-apps\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![intelligent apps on Azure](../../static/img/60-days-of-ia/blogs/2024-02-19/harnessing-the-power-of-intelligent-apps.jpg)\\r\\n\\r\\n## Harnessing the Power of Intelligent Apps\\r\\n\\r\\nOrganizations are increasingly adopting advanced technologies to drive innovation and elevate operational efficiency. Intelligent apps\u2014applications that integrate machine learning (ML), data analytics, and predictive or generative artificial intelligence (AI) to create differentiated digital experiences\u2014are one way to achieve this. According to Gartner\xae, \u201cby 2026, 30% of new applications will use AI to drive personalized adaptive user interfaces, up from less than 5% today\u201d<sup>1</sup>.\\r\\n\\r\\nIntelligent apps tend to fall into one of three categories:\\r\\n\\r\\n* **Outcome-based apps** \u2014 These apps focus on user intent, predictions, and task automation to enable better decision-making.\\r\\n* **Functionality-based apps** \u2014 These apps use ML, AI, or APIs to generate content. They examine user patterns to provide personalized recommendations or feedback.\\r\\n* **Feature-based apps** \u2014 These apps have AI or ML components built in, which means they rely on neural networks and internal LLMs to run advanced algorithms.\\r\\n\\r\\nBecause Intelligent apps help organizations leverage business intelligence and other data to drive and automate organizational decision-making, they\u2019re becoming a pivotal part of modern business strategies. In this article, we\u2019ll spotlight the success stories of some organizations that have leveraged Microsoft Azure to create and deploy intelligent apps in their workflows and products.\\r\\n\\r\\n___\\r\\n\\r\\n## Intelligent Apps in Action\\r\\n\\r\\nIntelligent Apps offer tangible business outcomes by automating complex processes, enhancing decision-making, and providing personalized experiences. By leveraging Azure services, the organizations discussed below have experienced a paradigm shift in their operations \u2014 and a boost in productivity and agility.\\r\\n\\r\\n### How Intelligent Apps Power the Ultimate LEGO Experience\\r\\n\\r\\nDenmark\u2019s ultimate LEGO experience center, LEGO House, found challenges in maintaining its on-premises data center. To keep serving its custom-built digital experiences, the business upgraded its facilities with [Azure Kubernetes Service](https://azure.microsoft.com/en-us/products/kubernetes-service/?ocid=buildia24_60days_blogs) (AKS) in 2023.\\r\\n\\r\\nThis shift in approach to the cloud was a boon for responsiveness \u2014 LEGO House could take on visitor feedback to swiftly update experiences and develop new ones. The containerized, component-based setup on AKS also allowed LEGO House\u2019s digital tech stack to become more scalable and flexible, transforming development efficiency and maintenance.\\r\\n\\r\\nLEGO House continued its partnership with Microsoft to launch experiences like City Architect \u2014 powered by [Azure IoT Edge Device](https://azure.microsoft.com/en-us/products/iot-edge?ocid=buildia24_60days_blogs) \u2014 and Robo Lab. These innovations allowed visitors to interact with digitally projected landscapes and build robots, fostering principles of programming. AKS streamlines integration and supports element reuse, enhancing efficiency and creativity.\\r\\n\\r\\nThe results were remarkable \u2014 improved stability, higher uptime, and positive visitor feedback. Azure services made life easier for LEGO House\u2019s developers and gave the entire LEGO ecosystem a strong foundation for growth. Specifically, by allowing the reuse of elements, AKS provides a common foundation for all LEGO experience builds. The organization\u2019s next move is to rebuild the entire House on the [Azure AI](https://azure.microsoft.com/en-us/solutions/ai?ocid=buildia24_60days_blogs) platform and AKS. Read more about how LEGO [modernized interactive experiences across LEGO House with Azure Kubernetes Service](https://customers.microsoft.com/en-us/story/1703088157691224129-lego-house-azure-kubernetes-service-media-and-entertainment-denmark?ocid=buildia24_60days_blogs).\\r\\n\\r\\n### Using Intelligent Apps to Make Cars Smarter With TomTom\\r\\n\\r\\nTomTom\u2019s navigation solutions have a proven track record of innovating the driving experience. Today, the company continues to adapt to drivers\u2019 evolving needs. Using [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-services/openai-service/?ocid=buildia24_60days_blogs), [Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction?ocid=buildia24_60days_blogs), and [AKS](https://azure.microsoft.com/en-us/products/kubernetes-service/?ocid=buildia24_60days_blogs) to develop Digital Cockpit, TomTom has created smarter, AI-powered vehicles, facilitating huge advancements in user experience.\\r\\n\\r\\nDigital Cockpit is an AI-driven, conversational in-car infotainment system that allows drivers to interact naturally with the vehicle. It can perform tasks like navigation, controlling onboard systems, and even managing work tasks during electric vehicle recharging.\\r\\n\\r\\nLet\u2019s look closer at the Azure services that drive Digital Cockpit:\\r\\n\\r\\n* **Azure OpenAI Service** \u2014 The Azure OpenAI Service supports generative AI chatbots that provide natural-sounding voices and accurate transcription of spoken audio.\\r\\n* **Azure Cosmos DB** \u2014 Azure Cosmos DB, a globally distributed database, retains customer conversations and preferences, allowing the system to continuously learn and tailor driver experiences.\\r\\n* **AKS** \u2014 AKS accelerates service deployment and scaling, enhancing overall architecture efficiency.\\r\\n\\r\\nInternally, integrating Azure services resulted in a significant improvement in TomTom\u2019s development efficiency. For example, the team working on the prototype no longer required ten engineers \u2014 three team members were sufficient to complete the task. Additionally, response times decreased from 12 to 2.5 seconds, and the system demonstrated a 95 percent success rate in understanding complex driver requests. Read more about how [TomTom brings AI-powered, talking cars to life with Azure](https://customers.microsoft.com/en-us/story/1723808815413508250-tomtom-azure-netherlands?ocid=buildia24_60days_blogs).\\r\\n\\r\\n:::info\\r\\nTry the [Intelligent Apps Skills Challenges](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs) to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n### How Gluwa Leverages Intelligent Apps to Make Banking More Accessible\\r\\n\\r\\nSan Francisco-based startup Gluwa is on a mission to address the financial gap for the unbanked and underbanked, estimated at 1.4 billion people globally. Combining blockchain technology and Azure services, Gluwa connects investors with individuals in emerging markets. \\r\\n\\r\\nGluwa harnesses the capabilities of various Azure services to power its blockchain services. [Azure Container Instances](https://azure.microsoft.com/en-us/products/container-instances?ocid=buildia24_60days_blogs) and AKS play a pivotal role in peer-to-peer discovery, fostering a dynamic and efficient environment. [Azure App Configuration](https://azure.microsoft.com/en-us/products/app-configuration?ocid=buildia24_60days_blogs) streamlines the centralization of app configurations, ensuring seamless control and adaptability.\\r\\n\\r\\nFor secure media delivery, Gluwa relies on the powerful combination of [Azure Content Delivery Network](https://azure.microsoft.com/en-us/products/cdn?ocid=buildia24_60days_blogs) and [Azure Storage](https://learn.microsoft.com/en-us/azure/storage/common/storage-introduction?ocid=buildia24_60days_blogs), which bolsters reliability and safeguards sensitive data. Using [Azure DevOps](https://azure.microsoft.com/en-us/products/devops?ocid=buildia24_60days_blogs) to manage intricate lifecycle builds, Gluwa streamlined the development process. [Azure App Services](https://azure.microsoft.com/en-us/products/app-service?ocid=buildia24_60days_blogs) serve as the backbone for Gluwa\u2019s APIs, complemented by [Azure Redis](https://azure.microsoft.com/en-us/products/cache?ocid=buildia24_60days_blogs) for optimal cache distribution, to enhance overall performance. Finally, [Azure SQL](https://azure.microsoft.com/en-us/products/azure-sql/database?ocid=buildia24_60days_blogs) and Azure Cosmos DB are scalable database solutions that support Gluwa\u2019s infrastructure, ensuring adaptability and responsiveness in meeting evolving demands within the blockchain landscape.\\r\\n\\r\\nGluwa\u2019s decentralized financial platform, Gluwa Invest, encourages stable coin investments, while the Creditcoin blockchain records loans, providing transparency and immutability. Together, they\u2019ve facilitated nearly 4.27 million loan transactions, totaling over $79.7 million. Gluwa turned to Azure\u2019s reliable, scalable cloud foundation to make these innovative and socially impactful initiatives a reality. Read more about how [Gluwa uses blockchain to help investors fund loans to the unbanked](https://customers.microsoft.com/en-us/story/1709609183358710412-gluwa-azure-banking-and-capital-markets-usa?ocid=buildia24_60days_blogs).\\r\\n\\r\\n___\\r\\n\\r\\n## Summary\\r\\n\\r\\nAzure services have empowered organizations developing intelligent apps by offering scalability, flexibility, and seamless integration of ML, data analytics, and AI.\\r\\n\\r\\nAzure\u2019s impact extends beyond immediate efficiency gains, empowering developers to iterate, learn, and keep creating new experiences. Businesses that build with Azure services can streamline collaboration across ecosystems, unlocking collective vision and applied creativity.\\r\\n\\r\\nExplore Microsoft [Customer Stories](https://customers.microsoft.com/en-us/home?sq=aks&ff=&p=1&so=story_publish_date%20desc&ocid=buildia24_60days_blogs) for deeper insights into transformative, AI-powered solutions. Finally, don\u2019t forget to mark your calendar for [Azure Kubernetes Day](https://aka.ms/aks-day?ocid=buildia24_60days_blogs) at [KubeCon EU 2024](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/) to get the latest on cutting-edge developments in cloud technology.\\r\\n\\r\\n<sup>1</sup> Source: [Gartner, Demand Grows for Intelligent Applications Powered by AI, September 27, 2023](https://www.gartner.com/en/articles/demand-grows-for-intelligent-applications-powered-by-ai). GARTNER is a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and is used herein with permission. All rights reserved."},{"id":"power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service","source":"@site/blog-60daysofIA/2024-02-27/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service.md","title":"2. Power Up: Crafting an Intelligent Energy Forecasting Application Using Azure Kubernetes Service","description":"In this article, we\u2019ll will guide you through creating an intelligent app that leverages Azure technologies, including Azure Kubernetes Service (AKS), to build an application that forecasts energy usage and pricing.","date":"2024-02-27T09:00:00.000Z","formattedDate":"February 27, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":5.895,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-02-27T09:00","slug":"power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service","title":"2. Power Up: Crafting an Intelligent Energy Forecasting Application Using Azure Kubernetes Service","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this article, we\u2019ll will guide you through creating an intelligent app that leverages Azure technologies, including Azure Kubernetes Service (AKS), to build an application that forecasts energy usage and pricing.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"1. Harnessing the power of Intelligent Apps","permalink":"/Cloud-Native/60DaysOfIA/harnessing-the-power-of-intelligent-apps"},"nextItem":{"title":"2.1 Forecasting Energy Usage with Intelligent Apps Part 1: Laying the Groundwork with AKS, KAITO, and LLaMA","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this article, we\u2019ll spotlight the success stories of some organizations that have leveraged Microsoft Azure to create and deploy intelligent apps in their workflows and products.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this article, we\u2019ll spotlight the success stories of some organizations that have leveraged Microsoft Azure to create and deploy intelligent apps in their workflows and products.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\nAt the forefront of recent technological innovation are intelligent apps: apps that use machine learning (ML), artificial intelligence (AI), and data analytics. These apps support smarter, data-driven decisions, making them particularly useful in sectors like energy management, where efficiency and long-term planning are critical.\\r\\n\\r\\nOur upcoming series will guide you through creating an intelligent app that leverages Azure technologies, including [Azure Kubernetes Service](https://azure.microsoft.com/products/kubernetes-service?ocid=buildia24_60days_blogs) (AKS), to build an application that forecasts energy usage and pricing. \\r\\n\\r\\nYour app will harness AKS for hosting and AI to analyze historical energy consumption data. Then, you\u2019ll integrate the Kubernetes AI Toolchain Operator (KAITO) with with XGBoost and LLaMA 2 to build an intelligent app that underscores the importance of green energy practices and demonstrates the versatility and efficacy of Azure services.\\r\\n\\r\\nWe invite you to join us on this three-part educational series, where you\u2019ll learn the skills needed to construct your own intelligent apps. But, this series is more than a technical walkthrough: It\u2019s an opportunity to engage with cutting-edge technologies and contribute to meaningful advancements in energy management.\\r\\n\\r\\nWhether you\u2019re an experienced developer or new to the AI and ML sphere, this series will give you a glimpse into the future of application development and the strategic impact of Azure technologies in driving forward-thinking solutions.\\r\\n\\r\\n## The Synergy of Azure Kubernetes Service and Intelligent Apps\\r\\n\\r\\nUsing AKS as the backbone of intelligent apps has numerous benefits \u2014 especially when deploying your AI-driven application. AKS provides a managed, cloud-based container orchestration service that simplifies deploying, managing, and scaling AI-backed applications, making it ideal for a project like the one you\u2019ll create in this series.\\r\\n\\r\\nOne of the primary advantages of AKS is its ability to handle distributed applications with evolving demands. For AI-driven apps, the ability to scale resources based on computational demands is crucial. Because AKS allows for automatic scaling, intelligent apps have the necessary resources during peak analysis times without wasting resources during quieter periods. But this dynamic scalability isn\u2019t just about handling loads efficiently: It\u2019s also cost-effective, ensuring that you pay only for the resources you use.\\r\\n\\r\\nIntegrating the KAITO operator with AKS further enhances the deployment of AI models like LLaMA 2 by simplifying the complexities of managing AI workloads. KAITO, designed specifically for Kubernetes environments, acts as a bridge between the advanced AI models and the scalable, managed infrastructure provided by AKS. It offers custom resource definitions (CRDs) tailored for AI applications, facilitating the deployment, updating, and management of AI models within the Kubernetes ecosystem.\\r\\n\\r\\nThis seamless integration enables developers to focus more on the application logic and less on the underlying infrastructure, accelerating the development cycle and reducing the time to market for innovative AI solutions.\\r\\n\\r\\nAKS and KAITO create a robust, flexible, and efficient environment for developing and deploying intelligent applications. This combination not only leverages the cloud\u2019s power and scalability but also optimizes the deployment of AI models, making it easier for developers to bring complex, data-driven applications to life.\\r\\n\\r\\n:::info\\r\\nRegister for **[Intelligent Apps on AKS: Episode 2](https://developer.microsoft.com/en-us/reactor/events/21815/?ocid=buildia24_60days_blogs)**, a live hands-on training with an expert on how to use AKS to run your own AI models with KAITO.\\r\\n:::\\r\\n\\r\\n## Laying the Groundwork with Azure Kubernetes Service\\r\\n\\r\\nIn the [first installment of this series](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1), you\u2019ll roll up your sleeves and set up an AKS environment. This step is foundational to the rest of the series, laying the groundwork for deploying and managing your application \u2014 and accessing the full scalability and flexibility that AKS offers.\\r\\n\\r\\nThe article starts with a straightforward step-by-step guide on establishing the AKS environment, ensuring you have a solid base for the exciting journey ahead. This tutorial is succinct to maintain clarity and speedy development, offering links to additional resources for well-documented steps. \\r\\n\\r\\nNext, you\u2019ll meet KAITO, a tool that streamlines deploying AI applications in Kubernetes environments. The core of this article is configuring [the KAITO operator](https://github.com/Azure/kaito) to work seamlessly with the LLaMA 2 model, providing hands-on instructions, code samples, and screenshots to guide you through each step.\\r\\n\\r\\n## Adding Intelligence to the App\\r\\n\\r\\nThe [second part of this series](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2) dives into the more practical aspects of building the Intelligent App. You\u2019ll leverage an open-source energy dataset alongside powerful tools like XGBoost and a custom Python API to craft a forecasting model that predicts future energy demands with speed and precision.\\r\\n\\r\\nIntegrating these tools with AKS and Azure Container Registry highlights the high-impact relationship between robust data processing capabilities and scalable cloud infrastructure. Hands-on examples and streamlined code will guide you through setting up the environment, processing the dataset, and deploying the forecasting model.\\r\\n\\r\\nThis practical application reinforces the theoretical foundations laid in Part 1 and sets the stage for advanced analytics and AI-driven predictions. As you progress through the tutorial, the focus will remain on simplicity and efficiency, ensuring that even complex AI-related processes become accessible.\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n## Building a Web Interface\\r\\n\\r\\nAs the concluding installment of our series, [part 3](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-3) assembles all the pieces by introducing a user-friendly web interface. Here, users can input or upload their energy usage data and parameters, after which the Intelligent App will generate future predictions on usage and pricing.\\r\\n\\r\\nThis web front end serves as the direct point of interaction with your AKS-hosted application, seamlessly displaying the reports and predictions the AI model produces.\\r\\n\\r\\nAfter deploying this interface in the AKS environment established in [part 1](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1), you\u2019ll experience the complete cycle of developing an intelligent, data-driven application and appreciate how straightforward it is to engineer intelligent apps that can deliver tangible, user-centric outcomes.\\r\\n\\r\\n## Ready to Get Started?\\r\\n\\r\\nTogether, these three articles guide you through creating an innovative, AI-driven energy forecasting app. Setting up a scalable AKS environment with integrated cutting-edge AI models, processing open-source energy data for insightful predictions, and deploying a user-friendly web interface will equip you with the tools you need to build your own Intelligent Apps.\\r\\n\\r\\nStay tuned for each part of the series and get ready to dive into the world of Azure, AI, and application development with us. Join us in this exciting venture and harness the power of technology to make a difference. Register for the **[Intelligent Apps on AKS: Episode 2](https://aka.ms/learn-live-building-intelligent-apps-aks-ep2?ocid=buildia24_60days_blogs)**\u202f to experience live hands-on training with an expert on how to use AKS to run your own AI models with KAITO."},{"id":"forecasting-energy-usage-with-intelligent-apps-1","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1","source":"@site/blog-60daysofIA/2024-03-05/forecasting-energy-usage-with-intelligent-apps-1.md","title":"2.1 Forecasting Energy Usage with Intelligent Apps Part 1: Laying the Groundwork with AKS, KAITO, and LLaMA","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","date":"2024-03-05T09:00:00.000Z","formattedDate":"March 5, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.14,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-05T09:00","slug":"forecasting-energy-usage-with-intelligent-apps-1","title":"2.1 Forecasting Energy Usage with Intelligent Apps Part 1: Laying the Groundwork with AKS, KAITO, and LLaMA","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"2. Power Up: Crafting an Intelligent Energy Forecasting Application Using Azure Kubernetes Service","permalink":"/Cloud-Native/60DaysOfIA/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service"},"nextItem":{"title":"2.2 Forecasting Energy Usage with Intelligent Apps Part 2","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/forecasting-energy-usage-with-intelligent-apps-1\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\'ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-1\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\'ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-1\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Forecasting Energy Usage with Intelligent Apps: Laying the Groundwork with AKS, KAITO, and LLaMA](../../static/img/60-days-of-ia/blogs/2024-03-05/2-1-1.jpg)\\r\\n\\r\\n*This three-part series demonstrates how to create an Intelligent App that forecasts future energy consumption and pricing based on historical data. In this first article, you\u2019ll set up an Azure Kubernetes Service (AKS) environment, install KAITO, and set up KAITO to work with the LLaMA 2 model.*\\r\\n\\r\\n## Forecasting Energy Usage with Intelligent Apps Part 1: Laying the Groundwork with AKS, KAITO, and LLaMA\\r\\n\\r\\nIntelligent Apps leverage artificial intelligence (AI) and machine learning (ML) technologies to enhance traditional applications with advanced capabilities. They enable businesses to make smarter decisions, automate tasks, and drive innovation by extracting actionable insights from vast amounts of data.\\r\\n\\r\\nIn this series, you\u2019ll create an Intelligent App powered by [Azure Kubernetes Service](https://azure.microsoft.com/en-ca/products/kubernetes-service) (AKS) to forecast energy usage and cost. Each article will demonstrate the use of core Azure technologies, particularly AKS, to build an application that generates forecasts based on AI capabilities applied to user input and historical data analysis.\\r\\n\\r\\nLet\u2019s get started!\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nTo follow this tutorial, ensure you have the following:\\r\\n\\r\\n- An [Azure Subscription](https://azure.microsoft.com/en-us/free/search) that supports the GPU-enabled [Standard_NC12s_v3 instance type](https://learn.microsoft.com/en-us/azure/virtual-machines/ncv3-series) in the selected region. You might need to request an increase in vCPU quota.\\r\\n - Basic understanding of [AKS](https://azure.microsoft.com/en-us/products/kubernetes-service) and Kubernetes\\r\\n\\r\\n### Building an Intelligent App with Azure Kubernetes Service and KAITO\\r\\n\\r\\nThis first article walks you through setting up an AKS environment and the Kubernetes AI Toolchain Operator (KAITO) to automate AI/ML model deployment in the AKS cluster.\\r\\n\\r\\n#### Downloading the LLaMA 2 Model\\r\\n\\r\\nA fundamental piece in your Intelligent App\u2019s architecture is the target model. Here, you\u2019ll use LLaMA 2, an open-source project developed by Meta in partnership with Microsoft.\\r\\n\\r\\nLLaMA 2 is a large-scale training and inference framework for ML models. It provides a distributed computing infrastructure that enables executing ML tasks across multiple nodes or clusters, using parallelism and optimization techniques to improve performance.\\r\\n\\r\\nTo configure your model, download LLaMA 2 by following the instructions in [this document](https://github.com/Azure/kaito/tree/main/presets/models/llama2).  Ensure you download the LLaMA 2 7B Chat (llama2-7b-chat) model.\\r\\n\\r\\n#### Configuring the AKS Cluster and KAITO\\r\\n\\r\\n![engergy-usage-aks model](../../static/img/60-days-of-ia/blogs/2024-03-05/2-1-2.jpg)\\r\\n\\r\\nCreating an AKS environment is the first step for onboarding large AI inference models onto Kubernetes. Later, you\u2019ll integrate the node provisioner controller with AKS APIs, letting you dynamically add GPU nodes to the cluster to promote scalability and optimal resource use.\\r\\n\\r\\nAdditionally, AKS facilitates testing service endpoints within the cluster, providing a reliable environment for validating and fine-tuning AI inference services.\\r\\n\\r\\nKAITO is an open-source operator that transforms how you deploy AI models on Kubernetes. It streamlines the process, automating critical tasks like infrastructure provisioning and resource optimization. It intelligently selects the optimal hardware configuration for your specific model, using available CPU and GPU resources on AKS. KAITO eliminates the manual setup complexities, accelerating your deployment time and reducing associated costs.\\r\\n\\r\\nTo set up an AKS cluster and install KAITO, follow [this tutorial](https://github.com/Azure/kaito/blob/main/docs/installation.md), adjusting the KAITO installation steps to match the **llama2-7b** model you downloaded earlier.\\r\\n\\r\\n:::info\\r\\nCheckout the **[Intelligent Apps on AKS: Episode 2](https://aka.ms/learn-live-building-intelligent-apps-aks-ep2?ocid=buildia24_60days_blogs)**, a hands-on training with an expert on how to use AKS to run your own AI Models with KAITO.\\r\\n:::\\r\\n\\r\\n#### Pushing LLaMA 2 Chat Model to Azure Container Registry\\r\\n\\r\\nNow that you have AKS with the KAITO installation, you need to push the local model image to the AKS cluster.\\r\\n\\r\\nCreate an Azure Container Registry (ACR) resource using Azure CLI with the following command, replacing `<YOUR-ACR-NAME>` with a new ACR name:\\r\\n\\r\\n```\\r\\naz acr create --name <YOUR-ACR-NAME> --resource-group $RESOURCE_GROUP --sku Standard --location $LOCATION\\r\\n```\\r\\n\\r\\nNow, push your local LLaMA 2 model\u2019s Docker image to the ACR hosted at `<YOUR-ACR-NAME>.azurecr.io` by running:\\r\\n\\r\\n```\\r\\ndocker push <YOUR-ACR-NAME>.azurecr.io/llama2_7b_chat_model:latest\\r\\n```\\r\\n\\r\\nFinally, run the command to update the AKS cluster to attach it to your ACR, allowing the cluster to pull the model container image from `<YOUR-ACR-NAME>.azurecr.io`:\\r\\n\\r\\n```\\r\\naz aks update -g $RESOURCE_GROUP -n $MY_CLUSTER --attach-acr <YOUR-ACR-NAME>\\r\\n```\\r\\n\\r\\n#### Starting the Inference Service\\r\\n\\r\\nAfter installing KAITO, run the following command to start a `llama-2-7b` inference service, replacing `<YOUR-ACR-NAME>` with the ACR name you created previously:\\r\\n\\r\\n```\\r\\n$ cat examples/kaito_workspace_llama2_7b.yaml\\r\\napiVersion: kaito.sh/v1alpha1\\r\\nkind: Workspace\\r\\nmetadata:\\r\\n  name: workspace-llama-2-7b\\r\\nresource:\\r\\n  instanceType: \\"Standard_NC12s_v3\\"\\r\\n  labelSelector:\\r\\n    matchLabels:\\r\\n      apps: llama-2-7b-chat\\r\\ninference:\\r\\n  preset:\\r\\n    name: \\"llama-2-7b-chat\\"\\r\\n    accessMode: private\\r\\n    presetOptions:\\r\\n      image: <YOUR-ACR-NAME>.azurecr.io/llama2_chat_model:latest\\r\\n      imagePullSecrets:\\r\\n        - energy-usage-secret\\r\\n\\r\\n$ kubectl apply -f examples/kaito_workspace_llama2_7b-chat.yaml\\r\\n```\\r\\n\\r\\nKubernetes uses this YAML code to instantiate a workspace resource with the specified configurations. This enables deploying and managing inference workloads within the cluster.\\r\\n\\r\\nYou can monitor the workspace status by executing the command below. The model deployment has been completed once the `WORKSPACEREADY` column becomes `True`:\\r\\n\\r\\n```\\r\\n$ kubectl get workspace workspace-llama-2-7b-chat\\r\\n| NAME | INSTANCE | RESOURCEREADY | INFERENCEREADY | WORKSPACEREADY | AGE |\\r\\n| workspace-llama-2-7b-chat | Standard_NC12s_v3 | True | True | True | 4d2h |\\r\\n```\\r\\n\\r\\n**Note**: Achieving machine and workspace readiness may take up to 20 minutes.\\r\\n\\r\\nNow, run the command below to find the inference service\u2019s cluster IP:\\r\\n\\r\\n```\\r\\n$ kubectl get svc workspace-llama-2-7b-chat\\r\\n| NAME | TYPE | CLUSTER-IP | EXTERNAL-IP | PORT(S) | AGE |\\r\\n| workspace-llama-2-7b-chat | ClusterIP | <CLUSTERIP> | <none> | 80/TCP,29500/TCP | 4d2h |\\r\\n```\\r\\n\\r\\nFinally, run a curl pod to test the service endpoint in the cluster:\\r\\n\\r\\n```\\r\\nexport CLUSTERIP=$(kubectl get svc workspace-llama-2-7b-chat -o jsonpath=\\"{.spec.clusterIPs[0]}\\")\\r\\n\\r\\n$ kubectl run -it --rm --restart=Never curl --image=curlimages/curl -- curl -X POST http://$CLUSTERIP/generate -H \\"accept: application/json\\" -H \\"Content-Type: application/json\\" -d \\"{\\\\\\"input_data\\\\\\": {\\\\\\"input_string\\\\\\":[[ {\\\\\\"role\\\\\\": \\\\\\"user\\\\\\", \\\\\\"content\\\\\\": \\\\\\"What is the capital of India?\\\\\\"}]]}, \\\\\\"parameters\\\\\\": {\\\\\\"max_gen_len\\\\\\": 64, \\\\\\"temperature\\\\\\": 0}}\\"\\r\\n```\\r\\n\\r\\nYou should receive these results:\\r\\n\\r\\n```\\r\\n{\\"results\\":[[{\\"role\\":\\"User\\",\\"content\\":\\"What is the capital of India?\\"},{\\"role\\":\\"Assistant\\",\\"content\\":\\" The capital of India is New Delhi.\\"}]]}\\r\\n```\\r\\n\\r\\n**Note**: You can test with your own questions, but there may be inaccuracies within the response. This is because AKS hasn\u2019t fine-tuned the model for your scenario.\\r\\n\\r\\nThat\u2019s it! You\u2019ve successfully established your AKS environment and familiarized yourself with setting up KAITO to deploy the LLaMA 2 model within your Kubernetes environment. You\u2019re now ready to analyze a model and make predictions using Azure\u2019s AI services.\\r\\n\\r\\n## Next Steps\\r\\n\\r\\nIn this article, you established an AKS cluster and configured KAITO to integrate with the LLaMA 2 model for advanced ML capabilities. In part 2, you\u2019ll use AKS and KAITO to analyze historical energy consumption data with advanced ML models. You\u2019ll create a dynamic web interface for users to input data, generate predictions, and visualize results seamlessly.\\r\\n\\r\\nBe sure to join the [Cloud Skill Challenge](https://azure.github.io/Cloud-Native/Build-IA/CloudSkills) to level up your cloud computing skills and gain hands-on experience. You can also register for the [next episode](https://aka.ms/learn-live-building-intelligent-apps-aks-ep3?ocid=buildia24_60days_blogs) on **Intelligent Apps with Azure Kubernetes Service**, an instructor led live learning experience to deploy your app on AKS. And, join the AKS product and engineering team at *KubeCon EU 2024*\u2014the premier conference for cloud-native technologies, for **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days**."},{"id":"forecasting-energy-usage-with-intelligent-apps-2","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2","source":"@site/blog-60daysofIA/2024-03-05/forecasting-energy-usage-with-intelligent-apps-2.md","title":"2.2 Forecasting Energy Usage with Intelligent Apps Part 2","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","date":"2024-03-05T09:05:00.000Z","formattedDate":"March 5, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":11.065,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-05T09:05","slug":"forecasting-energy-usage-with-intelligent-apps-2","title":"2.2 Forecasting Energy Usage with Intelligent Apps Part 2","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"2.1 Forecasting Energy Usage with Intelligent Apps Part 1: Laying the Groundwork with AKS, KAITO, and LLaMA","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1"},"nextItem":{"title":"2.3 Forecasting Energy Usage with Intelligent Apps Part 3","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-3"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/forecasting-energy-usage-with-intelligent-apps-2\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\'ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-2\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\'ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-2\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Graphic of a bar chart with a magnifying glass in front of it. To the left of the magnifying glass is a lightning bolt. At the bottom of the graphic is text that reads, \\"Forecasting Energy Usage with Intelligent Apps: Making Predictions.\\"](../../static/img/60-days-of-ia/blogs/2024-03-05/2-2-1.png)\\r\\n\\r\\n*This three-part series demonstrates how to create an Intelligent App that forecasts future energy consumption and pricing based on historical data. In this second article, you\u2019ll build out an app that analyzes historical data on energy consumption to build a forecasting model that forecasts future energy usage/pricing based on parameters input by the user.*\\r\\n\\r\\n## Forecasting Energy Usage with Intelligent Apps Part 2: Making Predictions\\r\\n\\r\\nIn [Part 1 of this series](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1), you set up an [Azure Kubernetes Service](https://azure.microsoft.com/products/kubernetes-service?ocid=buildia24_60days_blogs) (AKS) cluster, installed [Kubernetes AI Toolchain Operator](https://azure.microsoft.com/updates/preview-ai-toolchain-operator-addon-for-aks/?ocid=buildia24_60days_blogs) (KAITO), and configured KAITO with Llama 2. In this article, you\u2019ll use the groundwork from Part 1 to build the Intelligent App.\\r\\n\\r\\nUsing historical data analysis and an open-source dataset, you\u2019ll construct a model capable of predicting future energy usage and pricing with the flexibility of user-defined parameters.\\r\\n\\r\\nLet\u2019s get started!\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nTo follow this tutorial, ensure you:\\r\\n\\r\\n - Completed Part 1 of this series\\r\\n - Have a Jupyter notebook or a preferred Python integrated development environment (IDE), like [Visual Studio Code](https://code.visualstudio.com/), downloaded\\r\\n - [kubectl](https://kubernetes.io/docs/tasks/tools/), the Kubernetes command-line tool, installed \\r\\n - The [Forecast API source code](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/tree/main/Part%202) downloaded\\r\\n\\r\\nFor a sneak peek of the final product, check out the [complete project code](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/tree/main/Part%202).\\r\\n\\r\\n:::info\\r\\nCheckout the [Intelligent Apps on AKS: Episode 3](https://aka.ms/learn-live-building-intelligent-apps-aks-ep3?ocid=buildia24_60days_blogs), a technical deep dive hands-on training with an expert on how OpenCost, Prometheus, and Grafana with AKS can improve intelligent apps. \\r\\n:::\\r\\n\\r\\n### Predicting Energy Consumption and Pricing with an Intelligent App\\r\\n\\r\\nThe Intelligent App will analyze historical data on energy consumption to build a regression model. For this tutorial, you\u2019ll use an open-source [Kaggle dataset](https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather) named \u201cHourly energy demand generation and weather.\u201d\\r\\n\\r\\nAt a high level, you\u2019ll take the following steps to build the Intelligent App:\\r\\n\\r\\n - **Dataset analysis and feature engineering**\u2014You\u2019ll start by examining the dataset. For this tutorial, you want to understand the relationships between energy consumption, pricing, and influencing factors like time. You\u2019ll engineer new features from the raw data. This includes working hours, the day of the week, and the month.\\r\\n - **Model training**\u2014You\u2019ll use XGBoost as a suitable regression model, as it accounts for both time elements and external factors. You\u2019ll train it using the processed Kaggle dataset. The goal is to teach our model to identify patterns and make reliable predictions of electricity prices.\\r\\n - **User input**\u2014The app will let users provide parameters, such as a date, time, amount of electricity generated from burning, and more. The model will use these inputs to generate predictions.\\r\\n - **Report generation**\u2014Llama2 will come into play here. It will help you create a clear and informative report summarizing the user\u2019s input, the model\u2019s predictions, and any insights derived from the analysis.\\r\\n\\r\\n**Note:** To keep this article focused, we\u2019ll assume you have a pre-cleaned dataset with engineered features. If you\u2019d like to see the details of this process, [refer to this notebook](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/blob/main/Part%202/data/predicting-future-energy-pricing.ipynb).\\r\\n\\r\\nWith the steps above completed, you need to split the data into training and testing sets using the code below. Doing so allows you to predict \u201cprice actual\u201d \u2014 the target feature for this tutorial.\\r\\n\\r\\n```\\r\\n# Define the target feature \\r\\ntarget = \'price actual\' \\r\\n\\r\\n# Split data into feature matrix (X) and target vector (y) \\r\\nX, y = df.drop(columns=target), df[target] \\r\\n\\r\\n# Split the data into training and testing sets \\r\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \\r\\n```\\r\\n\\r\\nNext, use the code below to define a function to train your model.\\r\\n\\r\\n```\\r\\ndef train_xgboost_regressor(X_train, y_train, X_test, y_test): \\r\\n    \\"\\"\\" \\r\\n    Train an XGBoost regressor using cross-validation, tune hyperparameters, and evaluate the model. \\r\\n\\r\\n    Parameters: \\r\\n    X_train (DataFrame): The DataFrame containing the training features. \\r\\n    y_train (Series): The Series containing the training target variable. \\r\\n    X_test (DataFrame): The DataFrame containing the testing features. \\r\\n    y_test (Series): The Series containing the testing target variable. \\r\\n\\r\\n    Returns: \\r\\n    float: Mean squared error (MSE) of the model. \\r\\n    float: R-squared (R2) score of the model. \\r\\n    \\"\\"\\" \\r\\n\\r\\n    # Define the XGBoost regressor \\r\\n    xgb_regressor = xgb.XGBRegressor() \\r\\n\\r\\n    # Define parameter grid for hyperparameter tuning \\r\\n    param_grid = { \\r\\n        \'learning_rate\': [0.05, 0.1], \\r\\n        \'max_depth\': [3, 5, 7], \\r\\n        #\'min_child_weight\': [1, 3, 5], \\r\\n        #\'subsample\': [0.6, 0.8, 1.0], \\r\\n        #\'colsample_bytree\': [0.6, 0.8, 1.0], \\r\\n        #\'gamma\': [0, 0.1, 0.2] \\r\\n    }\\r\\n\\r\\n    # Perform GridSearchCV for hyperparameter tuning \\r\\n    grid_search = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid, cv=5, scoring=\'neg_mean_squared_error\', verbose=1) \\r\\n    grid_search.fit(X_train, y_train) \\r\\n\\r\\n    # Get the best parameters \\r\\n    best_params = grid_search.best_params_ \\r\\n\\r\\n    # Initialize XGBoost regressor with best parameters \\r\\n    best_xgb_regressor = xgb.XGBRegressor(**best_params) \\r\\n\\r\\n    # Perform cross-validation \\r\\n    cv_scores = cross_val_score(best_xgb_regressor, X_train, y_train, cv=5, scoring=\'neg_mean_squared_error\') \\r\\n\\r\\n    # Train the XGBoost regressor on the full training set \\r\\n    best_xgb_regressor.fit(X_train, y_train) \\r\\n\\r\\n    # Make predictions on the test set \\r\\n    y_pred = best_xgb_regressor.predict(X_test) \\r\\n\\r\\n    # Save the model \\r\\n    save_model(best_xgb_regressor, \'xgb_model.joblib\') \\r\\n\\r\\n    # Calculate MSE and R2 score \\r\\n    rmse = root_mean_squared_error(y_test, y_pred) \\r\\n    r2 = r2_score(y_test, y_pred) \\r\\n\\r\\n    return rmse, r2 \\r\\n\u202f \\r\\n\\r\\nrmse, r2 = train_xgboost_regressor(X_train, y_train, X_test, y_test) \\r\\nprint(\\"Test RMSE:\\", rmse) \\r\\nprint(\\"Test R2 Score:\\", r2) \\r\\n```\\r\\nThe output of the code snippet above should look like the following: \\r\\n\\r\\n![Screenshot of output code in the terminal. It reads, \\"Fitting 5 folds for each of 6 candidates, totalling 30 fits\\r\\nTest RMSW: 5.738308690044228\\r\\nTest R2 Score: 0.8378464489048971](../../static/img/60-days-of-ia/blogs/2024-03-05/2-2-2.png)\\r\\n\\r\\nAfter fitting the model with cross-validation and hyperparameter tuning, you\u2019ll see an average root mean squared error (RMSE) of approximately 5.74 and an R-squared (R2) score of about 0.84 on the test data. So, the R-squared values show promising performance in predicting energy prices!\\r\\n\\r\\nNext, you\u2019ll create the Forecast API, set up its communication with Llama2, and deploy it onto your AKS cluster.\\r\\n\\r\\n:::info\\r\\nComplete the [Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs) to compete for the leaderboard and earn a Microsoft Learn Badge. \\r\\n:::\\r\\n\\r\\n#### Setting Up the Backend Service\\r\\n\\r\\nIn the sections that follow, you\u2019ll deploy a simple Python service named \u201cForecast API.\u201d This service will have an endpoint called `predict-chat` that will interact with the Llama2 Chat inference service within your AKS cluster. But first, you\u2019ll set up a backend service.  \\r\\n\\r\\nUnzip the [source code](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/tree/main/Part%202) that accompanies this article. It contains the files necessary to run your Python API using the Flask library, including the following:\\r\\n\\r\\n```\\r\\nName \\r\\n---- \\r\\napp.py \\r\\ndeployment.yaml \\r\\nDockerfile \\r\\nrequirements.txt \\r\\nservice.yaml \\r\\n```\\r\\n\\r\\nOpen the `app.py` file:\\r\\n\\r\\n```\\r\\nfrom flask import Flask, request, jsonify \\r\\nimport os \\r\\nimport requests \\r\\n\\r\\napp = Flask(__name__) \\r\\n\\r\\nGENERATE_ENDPOINT_CHAT = os.environ.get(\'GENERATE_ENDPOINT_CHAT\') \\r\\n\\r\\n@app.route(\'/predict-chat\', methods=[\'POST\']) \\r\\ndef predict_chat(): \\r\\n    try: \\r\\n        data = request.get_json() \\r\\n\\r\\n        gen_fossil_brown_coal = data.get(\'gen_fossil_brown_coal\') or 582.0 \\r\\n        gen_fossil_gas = data.get(\'gen_fossil_gas\') or 5537.0 \\r\\n        gen_fossil_hard_coal = data.get(\'gen_fossil_hard_coal\') or 4039.0 \\r\\n        gen_fossil_oil = data.get(\'gen_fossil_oil\') or 331.0 \\r\\n        gen_hydro = data.get(\'gen_hydro\') or 454.0 \\r\\n        gen_other_renewable = data.get(\'gen_other_renewable\') or 97.0 \\r\\n        gen_wind_onshore = data.get(\'gen_wind_onshore\') or 7556.0 \\r\\n        total_load_actual = data.get(\'total_load_actual\') or 31648.0 \\r\\n        price = data.get(\'price\') or 40.61 \\r\\n        max_gen_len = data.get(\'max_gen_len\') or 1024 \\r\\n        temperature = data.get(\'temperature\') or 0.0 \\r\\n\\r\\n        prompt = f\\"\\"\\"Display the following report table based on user inputs in tabular text format and write a single-paragraph report summarizing the electricity usage and forecast price: \\r\\n\\r\\n        ### Table \\r\\n\\r\\n        Generation from fossil brown coal/lignite: {gen_fossil_brown_coal} MW \\r\\n        Generation from fossil gas: {gen_fossil_gas} MW \\r\\n        Generation from fossil hard coal:  {gen_fossil_hard_coal} MW \\r\\n        Generation from fossil oil: {gen_fossil_oil} MW \\r\\n        Generation from hydro pumped storage: {gen_hydro} MW \\r\\n        Generation from other renewable sources: {gen_other_renewable} MW \\r\\n        Generation from onshore wind: {gen_wind_onshore} MW \\r\\n\\r\\n        ### Totals: \\r\\n        Total actual load: {total_load_actual} MW \\r\\n\\r\\n        Forecast price: {price} EUR/MWh \\r\\n\\r\\n        ### Short analysis: \\r\\n        Please write a short analysis on the data above.\\"\\"\\" \\r\\n\\r\\n        generate_response = requests.post(GENERATE_ENDPOINT_CHAT, json={ \\r\\n            \\"input_data\\": {\\"input_string\\":[[ {\\"role\\": \\"user\\", \\"content\\": prompt}]]}, \\r\\n            \\"parameters\\": {\\"max_gen_len\\": max_gen_len, \\"temperature\\": temperature} \\r\\n        }) \\r\\n\\r\\n        if generate_response.status_code == 200: \\r\\n            return generate_response.json(), 200 \\r\\n        else: \\r\\n            return jsonify({\'error\': \'Failed to invoke generate endpoint\'}), 500 \\r\\n\\r\\n    except Exception as e: \\r\\n        return jsonify({\'error\': str(e)}), 500 \\r\\n\\r\\nif __name__ == \'__main__\': \\r\\n    app.run(debug=True) \\r\\n```\\r\\n\\r\\nLet\u2019s review what the code inside the `app.py` file is doing.\\r\\n - The `app.py` file is an API endpoint for generating a report on electricity usage and forecasted price based on user-provided input data.\\r\\n - When a POST request is received by the `/predict-chat` endpoint, the application extracts the input data from the request, including parameters like generation from different energy sources, total actual load, price, and additional settings like `max_gen_len` and `temperature`. Note that for now, the app sends dummy values. In Part 3, you\u2019ll update your app to take real values from the user and predict the electricity price using the prediction model.\\r\\n - Then, the app constructs a prompt containing the input data and sends a request to another endpoint, specified by the `GENERATE_ENDPOINT_CHAT` environment variable, to generate a response. The response includes the report table in tabular text format and a short data analysis.\\r\\n - If the generation request is successful, the application returns a report produced with the generative capabilities of Llama 2 Chat, which is hosted as an inference service in your AKS cluster.\\r\\n\\r\\n ##### Building and Pushing to ACR\\r\\n\\r\\n Run the following commands to build the image locally and push it to your Azure Container Registry (ACR). Be sure to replace username and password with your `username` and `password`.\\r\\n\\r\\n ```\\r\\nsudo docker build --no-cache -f Dockerfile -t forecast-api -t <YOUR-ACR-NAME>.azurecr.io/forecast-api:latest . \\r\\n\\r\\ndocker login <YOUR-ACR-NAME>.azurecr.io --username <username> --password-stdin <password> \\r\\n\\r\\ndocker push <YOUR-ACR-NAME>.azurecr.io/forecast-api:latest \\r\\n``` \\r\\n\\r\\n##### Connecting to AKS\\r\\nStart by making sure you\u2019re logged in to Azure and that you have the correct AKS credentials by running the following: \\r\\n\\r\\n```\\r\\naz login --tenant <YOUR-AZURE-TENANT-ID> \\r\\n```\\r\\n\\r\\nNext, run the following commands:\\r\\n\\r\\n```\\r\\nexport RESOURCE_GROUP=<YOUR-RESOURCE-GROUP> \\r\\nexport MY_CLUSTER=<YOUR-AKS-CLUSTER-NAME> \\r\\nexport LOCATION=<YOUR-LOCATION> \\r\\nexport SUBSCRIPTION=<YOUR-AZURE-SUBSCRIPTION> \\r\\n\\r\\naz account set --subscription $SUBSCRIPTION \\r\\naz aks get-credentials --resource-group $RESOURCE_GROUP --name $MY_CLUSTER \\r\\n```\\r\\n\\r\\n##### Deployment\\r\\nBefore deploying, you need to retrieve the cluster IP of the Llama2 7B chat workspace running on your AKS cluster. Run the following command in your terminal:\\r\\n\\r\\n```\\r\\n> kubectl get svc \\r\\n```\\r\\n\\r\\nCopy the inference service\u2019s cluster IP:\\r\\n\\r\\n```\\r\\nNAME                       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)            AGE \\r\\nworkspace-llama-2-7b-chat  ClusterIP   <CLUSTERIP>  <none>        80/TCP,29500/TCP   10m \\r\\n```\\r\\n\\r\\nNext, open the source code directory. Modify the `deployment.yaml` file, replacing the `WORKSPACE-LLAMA-CHAT-CLUSTER-IP` placeholder below with your inference service cluster IP you copied above:  \\r\\n\\r\\n```\\r\\napiVersion: apps/v1 \\r\\nkind: Deployment \\r\\nmetadata: \\r\\n  name: forecast-api-deployment \\r\\nspec: \\r\\n  replicas: 1 \\r\\n  selector: \\r\\n    matchLabels: \\r\\n      app: forecast-api \\r\\n  template: \\r\\n    metadata: \\r\\n      labels: \\r\\n        app: forecast-api \\r\\n    spec: \\r\\n      containers: \\r\\n      - name: forecast-api \\r\\n        image: <YOUR-ACR-NAME>.azurecr.io/forecast-api:latest \\r\\n        ports: \\r\\n        - containerPort: 5000 \\r\\n        env: \\r\\n        - name: GENERATE_ENDPOINT_CHAT \\r\\n          value: \\"http://<WORKSPACE-LLAMA-CHAT-CLUSTER-IP>/chat\\" \\r\\n```\\r\\n\\r\\nThen, save the updated `deployment.yaml` file. \\r\\n\\r\\nExecute the following commands to deploy the service to your AKS cluster:\\r\\n\\r\\n```\\r\\nsnap install kubectl --classic \\r\\nkubectl apply -f deployment.yaml \\r\\nkubectl apply -f service.yaml \\r\\n```\\r\\n\\r\\n##### Invoking the service\\r\\nNow that the Forecast API service is deployed, try it out!\\r\\n\\r\\nRun the command below and grab your Forecast API external IP:\\r\\n\\r\\n```\\r\\n> kubectl get svc \\r\\n\\r\\nNAME                                 TYPE           CLUSTER-IP    EXTERNAL-IP   \\r\\nforecast-api-service                 LoadBalancer   <CLUSTER-IP>  <FORECAST-API-IP> \\r\\nworkspace-llama-2-7b-chat            ClusterIP      <CLUSTER-IP>  <none>        \\r\\nworkspace-llama-2-7b-chat-headless   ClusterIP      None          <none>  \\r\\n```\\r\\n\\r\\nNow, run the curl command below to test your Forecast API service. Be sure to replace the `<FORECAST-API-IP>` placeholder with your Forecast API external IP:\\r\\n\\r\\n```\\r\\ncurl --location \'http://<FORECAST-API-IP>/predict-chat\' \\\\ \\r\\n--header \'Content-Type: application/json\' \\\\ \\r\\n--data \'{ \\r\\n    \\"gen_fossil_brown_coal\\": 582.0, \\r\\n    \\"gen_fossil_gas\\": 5537.0, \\r\\n    \\"gen_fossil_hard_coal\\": 4039.0, \\r\\n    \\"gen_fossil_oil\\": 331.0, \\r\\n    \\"gen_hydro\\": 454.0, \\r\\n    \\"gen_other_renewable\\": 97.0, \\r\\n    \\"gen_wind_onshore\\": 7556.0, \\r\\n    \\"total_load_actual\\": 31648.0, \\r\\n    \\"price\\": 40.61, \\r\\n    \\"max_seq_len\\": 0, \\r\\n    \\"max_gen_len\\": 2048, \\r\\n    \\"temperature\\": 0.5 \\r\\n}\' \\r\\n```\\r\\n\\r\\nThe Forecast API service will process the request containing the data from energy sources and price, invoke the inference service, and return the response to the user:\\r\\n\\r\\n```\\r\\n\\"content\\": \\"Based on the user inputs provided, the following is the table of electricity generation and forecast price: \\r\\n\\r\\n| Generation Source | MW | \\r\\n| --- | --- | \\r\\n| Fossil Brown Coal/Lignite | 582.0 | \\r\\n| Fossil Gas | 5537.0 | \\r\\n| Fossil Hard Coal | 4039.0 | \\r\\n| Fossil Oil | 331.0 | \\r\\n| Hydro Pumped Storage | 454.0 | \\r\\n| Other Renewable Sources | 97.0 | \\r\\n| Onshore Wind | 7556.0 | \\r\\n\\r\\nTotal Actual Load | 31648.0 | \\r\\n\\r\\nForecast Price | 40.61 EUR/MWh | \\r\\n\\r\\nBased on the data provided, the electricity generation from various sources shows a predominance of fossil fuels, with fossil gas being the largest contributor at 5537.0 MW, followed by fossil hard coal at 4039.0 MW, and fossil brown coal/lignite at 582.0 MW. Onshore wind is the largest renewable source of electricity generation at 7556.0 MW.\\r\\n```\\r\\n\\r\\nThat\u2019s it! Note how the generative capabilities of Llama2 7B chat model can transform the cold numbers of your input into an intelligent analysis. \\r\\n\\r\\n## Next Steps\\r\\n\\r\\nIn this second installment of the series, you analyzed historical data, used a dataset to construct a model capable of predicting future energy pricing, and used LLaMA2 to generate a report on the energy usage. Continue to Part 3, where you\u2019ll build a basic web interface for the Intelligent App, display the report generated by model, and deploy the app. \\r\\n\\r\\nTo keep your learning going, participate in the [Cloud Skill Challenges](https://aka.ms/intelligent-apps/csc), check out the [Ask The Expert session](https://aka.ms/intelligent-apps/ate-aks?ocid=buildia24_60days_blogs) with the AKS product team, and register for **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days** at KubeCon EU to stay abreast of the latest developments in cloud technology."},{"id":"forecasting-energy-usage-with-intelligent-apps-3","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-3","source":"@site/blog-60daysofIA/2024-03-05/forecasting-energy-usage-with-intelligent-apps-3.md","title":"2.3 Forecasting Energy Usage with Intelligent Apps Part 3","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","date":"2024-03-05T09:10:00.000Z","formattedDate":"March 5, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":8.315,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-05T09:10","slug":"forecasting-energy-usage-with-intelligent-apps-3","title":"2.3 Forecasting Energy Usage with Intelligent Apps Part 3","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"2.2 Forecasting Energy Usage with Intelligent Apps Part 2","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2"},"nextItem":{"title":"3. Cosmos DB and Intelligent Apps: A Match Made for Innovation","permalink":"/Cloud-Native/60DaysOfIA/cosmos-db-and-intelligent-apps-a-match-made-for-innovation"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/forecasting-energy-usage-with-intelligent-apps-3\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Join us on a learning journey to build intelligent apps on Azure. Read all about the upcoming #BuildIntelligentApps initiative on this post!\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-3\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"This three-part series demonstrates how to create an Intelligent App that forecasts future energy consumption and pricing based on historical data.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-3\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Graphic with a bar chart in a computer-like window in the top-right corner. To the left of the graph is a circle with a lightning bolt in it. At the bottom of the graphic is text that reads, \\"Forecasting Energy Usage with Intelligent Apps: Adding a Website Interface.\\"](../../static/img/60-days-of-ia/blogs/2024-03-05/2-3-1.png)\\r\\n\\r\\n*This three-part series demonstrates how to create an Intelligent App that forecasts future energy consumption and pricing based on historical data. In this final installment of the series, you\u2019ll create a basic web interface that enables the user to input energy usage data and parameters, output the results and the model-generated report into the web interface for easy viewing. Finally, you\u2019ll deploy using the AKS environment set up in Part 1. *\\r\\n\\r\\n## Forecasting Energy Usage with Intelligent Apps Part 3: Adding a Web Interface\\r\\n\\r\\nIn [Part 1 of this series](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1), you set up an [Azure Kubernetes Service](https://azure.microsoft.com/products/kubernetes-service?ocid=buildia24_60days_blogs) (AKS) cluster and prepared it for automated deployment with the help of [Kubernetes AI Toolchain Operator](https://azure.microsoft.com/updates/preview-ai-toolchain-operator-addon-for-aks/?ocid=buildia24_60days_blogs) (KAITO) and Llama 2. Then, in [Part 2](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2), you built a model that predicts future energy usage/pricing based on parameters input by the user and set up the Intelligent App\u2019s back end.\\r\\n\\r\\nIn this third and final article of the series, you\u2019ll create a primary web interface that empowers users to input energy usage data and parameters to generate forecasts. Through this interface, users can gain insights into future energy demands, aiding in strategic decision-making and resource allocation.\\r\\n\\r\\nLet\u2019s dive in!\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nTo follow along, ensure you have:\\r\\n\\r\\n - Completed Parts [1](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1) and [2](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2) of this series\\r\\n - A code editor like [Visual Studio Code](https://code.visualstudio.com/)\\r\\n - Python 3.10 or higher\\r\\n - The new Forecast web app [source code](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/tree/main/Part%202) downloaded\\r\\n\\r\\nFor a sneak peek of the final product, check out the [complete project code](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/tree/main/Part%203).\\r\\n\\r\\n### Building an Intelligent App with Azure Kubernetes Service and KAITO\\r\\n\\r\\nIn this tutorial, you\u2019ll create a basic web interface that enables the user to input or upload energy usage data and parameters to generate future predictions of usage/pricing. Then, you\u2019ll output the results and the report generated from the model into the web interface for easy viewing. Finally, you\u2019ll deploy the Intelligent App using the AKS environment you set up in Part 1.\\r\\n\\r\\n:::info\\r\\nCheckout the **[Intelligent Apps on AKS: Episode 4](https://aka.ms/learn-live-building-intelligent-apps-aks-ep4?ocid=buildia24_60days_blogs)**, a technical deep dive hands-on training with an expert on how to use AKS and Azure to take your intelligent app global.\\r\\n:::\\r\\n\\r\\n#### Creating the Web Interface\\r\\n\\r\\nTo develop your web interface, you\u2019ll use [streamlit](https://streamlit.io/)\u2014a Python framework for creating web apps. This combination offers flexibility and ease of development, enabling seamless data processing and integration of visualization components.\\r\\n\\r\\n*The User Interface*\\r\\n\\r\\nThe core of your web interface is a streamlit form where users can input relevant parameters. The form includes fields for adding data related to electricity generation from different sources. Upon submitting the form, users trigger the prediction process. \\r\\n\\r\\nLocate the directory of the source code you have downloaded and open the [app.py file](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/blob/main/Part%202/app.py). It centralizes the logic needed by the new Forecast app to process user input and produce the price prediction and analysis.\\r\\n\\r\\nFor simplicity, let\u2019s review just the most pertinent parts of the file:\\r\\n\\r\\n```\\r\\n# omitted for brevity \\r\\n\\r\\ndef generate_report(user_input, price):  \\r\\n\\r\\n    # Get the IP address from the environment variable \\r\\n    IP_ADDRESS = os.environ.get(\'ENERGYFORECASTAPI_IP\') \\r\\n\\r\\n    # Endpoint URL \\r\\n    url = f\'http://{IP_ADDRESS}/predict-chat\' \\r\\n\\r\\n    # Request payload \\r\\n\\r\\n    payload = { \\r\\n\\r\\n# omitted for brevity \\r\\n\\r\\n    } \\r\\n\\r\\n    # Header \\r\\n    headers = {\'Content-Type\': \'application/json\'} \\r\\n\\r\\n    # Perform the request \\r\\n    response = requests.post(url, json=payload, headers=headers) \\r\\n\\r\\n    # Check the response \\r\\n    if response.status_code == 200: \\r\\n        print(\\"Response:\\", response.json()) \\r\\n        json_data = response.json() \\r\\n        report = json_data[\'results\'][0][1][\'content\'] \\r\\n        return report \\r\\n    else: \\r\\n        st.header(\'Error\', divider=\'rainbow\') \\r\\n        print(\\"Error:\\", response.text) \\r\\n        return response.text \\r\\n\\r\\ndef get_forecast_price(user_input): \\r\\n    model = load(\'xgb_model.joblib\')        \\r\\n    price = np.float64(model.predict(user_input)[0]) \\r\\n    price = np.around(price, 2) \\r\\n    return price \\r\\n\\r\\nst.title(\\"Predicting Energy Pricing\\") \\r\\nst.write(\\"This Intelligent App analyzes data on energy consumption and predicts the electricity price for the next hour. It then creates a report summarizing the electricity usage and price.\\") \\r\\n\\r\\nwith st.form(\\"my_form\\"): \\r\\n\\r\\n# some parts were omitted for brevity \\r\\n\\r\\n       user_input = [[np.float64(generation_fossil_brown_coal_lignite), np.float64(generation_fossil_gas),  \\r\\n                      np.float64(generation_fossil_hard_coal), np.float64(generation_fossil_oil), \\r\\n                      np.float64(generation_hydro_pumped_storage_consumption), np.float64(generation_other_renewable), \\r\\n                      np.float64(generation_wind_onshore), np.float64(total_load_actual), hour, weekday, month, business_hour, \\r\\n                      weekend]] \\r\\n        \\r\\n       price = get_forecast_price(user_input) \\r\\n\\r\\n       st.header(\'Forecast Price\', divider=\'rainbow\') \\r\\n       st.write(f\\"{str(round(price, 2))} EUR/MW\\") \\r\\n\\r\\n       report = generate_report(user_input, price) \\r\\n\\r\\n       st.header(\'Analysis\', divider=\'rainbow\') \\r\\n       st.write(report) \\r\\n```\\r\\n\\r\\n##### Building and Pushing to ACR\\r\\nOpen your terminal at the root directory of the Forecast app\u2019s source code you downloaded earlier. Run the pair of commands below to initiate and use a Python virtual environment:\\r\\n\\r\\n```\\r\\npython -m venv .env \\r\\n\\r\\n.env\\\\Scripts\\\\activate \\r\\n```\\r\\n\\r\\nThen, run the following command to complete the installation of dependencies of your Python project: \\r\\n\\r\\n```\\r\\npip install -r requirements.txt\\r\\n```\\r\\n\\r\\nExecute the following commands to build the image locally and push it to your Azure Container Registry (ACR). Be sure to replace `<username>` and `<password>` with your username and password.\\r\\n\\r\\n```\\r\\nsudo docker build --no-cache -f Dockerfile -t forecast-web -t <YOUR-ACR-NAME>.azurecr.io/forecast-web:latest . \\r\\n\\r\\ndocker login <YOUR-ACR-NAME>.azurecr.io --username <username> --password-stdin <password> \\r\\n\\r\\ndocker push <YOUR-ACR-NAME>.azurecr.io/forecast-web:latest \\r\\n```\\r\\n\\r\\n##### Connecting to AKS\\r\\n\\r\\nStart by making sure you\u2019re logged in to Azure and that you have the correct AKS credentials by running the following command: \\r\\n\\r\\n```\\r\\naz login --tenant <YOUR-AZURE-TENANT-ID>\\r\\n```\\r\\n\\r\\nNext, run the following commands to enable access to your AKS cluster via your terminal: \\r\\n\\r\\n```\\r\\nexport RESOURCE_GROUP=<YOUR-RESOURCE-GROUP> \\r\\nexport MY_CLUSTER=<YOUR-AKS-CLUSTER-NAME> \\r\\nexport LOCATION=<YOUR-LOCATION> \\r\\nexport SUBSCRIPTION=<YOUR-AZURE-SUBSCRIPTION> \\r\\n\\r\\naz account set --subscription $SUBSCRIPTION \\r\\naz aks get-credentials --resource-group $RESOURCE_GROUP --name $MY_CLUSTER \\r\\n```\\r\\n\\r\\n#### Deployment\\r\\n\\r\\nBefore deploying, you need to retrieve the cluster IP of the Forecast API service running on your AKS cluster. Execute the following command in your terminal:\\r\\n\\r\\n```\\r\\n> kubectl get svc forecast-api-service \\r\\n```\\r\\n\\r\\nCopy the inference service\u2019s cluster IP:\\r\\n\\r\\n```\\r\\nNAME                   TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)        AGE \\r\\nforecast-api-service   LoadBalancer   <CLUSTERIP>  <EXTERNAL-IP>   80:32306/TCP   46h \\r\\n```\\r\\n\\r\\nNext, modify the `deployment.yaml` file using the code below, replacing the `<ENERGY-FORECAST-API-IP>` placeholder below with the Forecast API service\u2019s cluster IP value you copied above:\\r\\n\\r\\n```\\r\\napiVersion: apps/v1 \\r\\nkind: Deployment \\r\\nmetadata: \\r\\n  name: forecast-web-deployment \\r\\nspec: \\r\\n  replicas: 1 \\r\\n  selector: \\r\\n    matchLabels: \\r\\n      app: forecast-web \\r\\n  template: \\r\\n    metadata: \\r\\n      labels: \\r\\n        app: forecast-web \\r\\n    spec: \\r\\n      containers: \\r\\n      - name: forecast-web \\r\\n        image: openaidemoacr.azurecr.io/forecast-web:latest \\r\\n        ports: \\r\\n        - containerPort: 8501 \\r\\n        env: \\r\\n        - name: ENERGYFORECASTAPI_IP \\r\\n          value: <ENERGY-FORECAST-API-IP> \\r\\n```\\r\\n\\r\\nThen, save the updated `deployment.yaml` file.\\r\\n\\r\\nExecute the following commands to provision a new pod and deploy the service to your AKS cluster:\\r\\n\\r\\n```\\r\\nsnap install kubectl --classic \\r\\nkubectl apply -f deployment.yaml \\r\\nkubectl apply -f service.yaml \\r\\n```\\r\\n\\r\\n**Note:** After the deployment commands have been applied, the Forecast web app may take a few minutes to get up and running.\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge. \\r\\n:::\\r\\n\\r\\n##### Running the Web App\\r\\n\\r\\nNow that the Forecast web app is deployed, let\u2019s try it out!\\r\\n\\r\\nRun the command below and grab your app\u2019s external IP:\\r\\n\\r\\n```\\r\\n> kubectl get svc forecast-web-service \\r\\n\\r\\nNAME                   TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)        \\r\\nforecast-web-service   LoadBalancer   10.0.81.68   <EXTERNAL-IP>   80:30805/TCP \\r\\n```\\r\\n\\r\\nNow, paste the `<External-IP>` into a new web browser tab to test your Forecast web app:\\r\\n\\r\\n![Screenshot of the Predicting Energy Pricing app open in a browser.](../../static/img/60-days-of-ia/blogs/2024-03-05/2-3-2.png)\\r\\n\\r\\nFill in the form with the energy fields, plus the date and time, and hit **Submit**.\\r\\n\\r\\nOnce you submit the form, you\u2019ll see predictions for energy prices categorized and a detailed report summarizing the electricity usage and price.\\r\\n\\r\\nOnce the form is submitted, the Forecast web queries the model trained in Part 2 and obtains the forecast price. Then, it accesses the Forecast API service, which is hosted in your AKS cluster, to produce the summary report using the generative capabilities of the Llama2 Chat model:\\r\\n\\r\\n![Screenshot of the results in the Forecast  app. It includes an analysis of generation sources and their respective usage, a total for the actual load, and a price forecast.](../../static/img/60-days-of-ia/blogs/2024-03-05/2-3-3.png)\\r\\n\\r\\n### Why Build Intelligent Apps with KAITO?\\r\\n\\r\\nKAITO provides significant advantages when building an AI project. One key benefit is the drastic reduction in time and effort required to deploy AI models. This is because KAITO automates many complex tasks that traditionally demand significant manual intervention.\\r\\n\\r\\nWithout utilizing KAITO, building an AI project within Kubernetes could present several challenges:\\r\\n\\r\\n - You need to manually handle complex tasks, like provisioning infrastructure resources, deploying models, managing endpoints, and optimizing resource utilization. The manual approach takes substantial time and effort and increases the likelihood of errors and inconsistencies across deployments. \\r\\n - The absence of automated infrastructure provisioning may result in suboptimal resource allocation and higher operational costs. \\r\\n\\r\\nBut with the help of KAITO, you can swiftly deploy hosted models from a variety of open-source repositories or custom models\u2014all without the need for extensive expertise in Kubernetes infrastructure management. Moreover, KAITO facilitates the seamless provisioning of infrastructure resources tailored to the specific requirements of AI workloads, optimizing cost efficiency and operational effectiveness.\\r\\n\\r\\nFor more details, refer to this [Microsoft Ignite presentation](https://www.youtube.com/watch?v=9EvA9gbTS9M&t=676s).\\r\\n\\r\\n### Conclusion\\r\\n\\r\\nIn this article, you created a web interface using Streamlit, Docker, and Kubernetes, allowing users to input data and generate insights into energy usage patterns.\\r\\n\\r\\nAzure technologies provide solutions for reducing carbon footprint and promoting sustainability. The [Carbon Aware Keda Operator](https://github.com/Azure/carbon-aware-keda-operator) is one such innovation designed to reduce the carbon footprint of Kubernetes resources. \\r\\n\\r\\nNow that you\u2019ve had hands-on experience in building an Intelligent App, join the [Cloud Skill Challenges](https://aka.ms/intelligent-apps/csc) and check out the [Ask The Expert session](https://aka.ms/intelligent-apps/ate-aks?ocid=buildia24_60days_blogs) with the AKS product team to keep up with the latest in cloud computing. And don\u2019t forget about the **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days** at KubeCon EU, a great opportunity to network with AKS and Azure experts. Let\u2019s work together to drive innovation!"},{"id":"cosmos-db-and-intelligent-apps-a-match-made-for-innovation","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/cosmos-db-and-intelligent-apps-a-match-made-for-innovation","source":"@site/blog-60daysofIA/2024-03-07/cosmos-db-and-intelligent-apps-a-match-made-for-innovation.md","title":"3. Cosmos DB and Intelligent Apps: A Match Made for Innovation","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","date":"2024-03-07T09:00:00.000Z","formattedDate":"March 7, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":5.8,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-07T09:00","slug":"cosmos-db-and-intelligent-apps-a-match-made-for-innovation","title":"3. Cosmos DB and Intelligent Apps: A Match Made for Innovation","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"2.3 Forecasting Energy Usage with Intelligent Apps Part 3","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-3"},"nextItem":{"title":"3.1 Dynamic Repricing of Products Using Intelligent Apps Part 1","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/cosmos-db-and-intelligent-apps-a-match-made-for-innovation\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/cosmos-db-and-intelligent-apps-a-match-made-for-innovation\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/cosmos-db-and-intelligent-apps-a-match-made-for-innovation\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Cosmos DB and Intelligent Apps: A Match Made for Innovation](../../static/img/60-days-of-ia/blogs/2024-03-07/3-1.jpeg)\\r\\n\\r\\n## Cosmos DB and Intelligent Apps: A Match Made for Innovation\\r\\n\\r\\nIntelligent Apps represent the next frontier in application development. Merging machine learning (ML), data analytics, and artificial intelligence (AI), Intelligent Apps help drive and automate informed decisions within everyday workflows. These applications can offer predictive insights and personalized experiences by understanding user intent, making predictions, and automating tasks.\\r\\n\\r\\nThe core of Intelligent Apps lies in their ability to harness vast amounts of data, analyze it for patterns, and use these insights to improve decision-making processes, enhance user experiences, and streamline operations.\\r\\n\\r\\n[Azure Cosmos DB](https://azure.microsoft.com/free/cosmos-db?ocid=buildia24_60days_blogs) plays an instrumental role in building these advanced applications. Its scalability, multi-model support, and seamless integration with Azure AI Services make it a solid foundation for Intelligent Apps. Using Cosmos DB, you can manage and analyze large volumes of diverse data worldwide with minimal latency, ensuring the apps you build are intelligent, highly responsive, and globally available. Moreover, the service\u2019s ability to handle real-time data updates and queries empowers Intelligent Apps to deliver dynamic, up-to-the-minute insights and actions.\\r\\n\\r\\nOur three-part series demonstrates how to use Cosmos DB alongside Azure AI Services to create an Intelligent App that forecasts price fluctuations based on historical pricing and product data. In completing this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\r\\n\\r\\nJoin us as we embark on this journey to unlock the potential of Intelligent Apps with Cosmos DB!\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Data Skills Challenge](https://aka.ms/intelligent-apps/data-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge. \\r\\n:::\\r\\n\\r\\n### Building an Intelligent Forecasting Demo for E-Commerce\\r\\n\\r\\nIn the competitive e-commerce landscape, the ability to adapt pricing in real time based on demand and historical data is a valuable asset. So, this project focuses on developing a forecasting model that leverages AI/ML capabilities to predict future price changes. By integrating this model into your projects, you can enhance your applications with data-driven decision-making tools that respond effectively to market trends.\\r\\n\\r\\nAt the heart of this project is Azure Cosmos DB, chosen for its robust data management and analysis features. Cosmos DB facilitates the handling of large datasets required for accurate forecasting, providing a scalable, globally distributed database environment that supports real-time updates and queries. This capability is crucial for applying AI algorithms to historical price data, enabling the app to generate timely predictions that can inform pricing strategies.\\r\\n\\r\\n### Laying the Groundwork with Cosmos DB\\r\\n\\r\\n[Part 1 of our series](https://azure.github.io/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1) starts with the foundation: setting up an Azure Cosmos DB environment tailored for the intelligent forecasting application. We\u2019ll guide you through the initial steps of creating and configuring your Cosmos DB instance to ensure it\u2019s ready to handle the complexities of historical pricing data.\\r\\n\\r\\nThis installment reviews how to populate your database with relevant data that will serve as the backbone for the dynamic repricing model. Once the Cosmos DB environment is established and filled with historical pricing data, you\u2019ll be in a strong position to start leveraging Azure AI Services to analyze this data and predict future price trends.\\r\\n\\r\\nBut the first article isn\u2019t just about setting up a database: It\u2019s about preparing the stage for a sophisticated application that can dynamically adjust e-commerce prices. Through this exercise, you\u2019ll learn the importance of a well-structured data foundation and how it enables the creation of more responsive and intelligent e-commerce platforms.\\r\\n\\r\\n### Analyzing Data with Azure AI Services\\r\\n\\r\\nIn part 2 of this series, the spotlight turns to Azure AI Services. You\u2019ll explore how to harness Azure\u2019s powerful AI capabilities to sift through the dataset, identifying patterns and trends that are key to understanding future price fluctuations.\\r\\n\\r\\nThis stage is all about bridging the gap between raw data and actionable insights, demonstrating how to apply AI capabilities to accurately forecast prices. We\u2019ll walk step-by-step through integrating Azure AI Services with Cosmos DB, helping you create a seamless workflow that brings the dynamic repricing model to life.\\r\\n\\r\\nThis hands-on exploration will equip you with the skills to implement intelligent forecasting within your own e-commerce platforms\u2014something that helps you make data-driven decisions on inventory pricing. By the end of part 2, you\u2019ll have a fully operational forecasting model capable of predicting price trends based on historical data.\\r\\n\\r\\n### Building the Web Interface\\r\\n\\r\\nIn part 3 of this series, you\u2019ll create a simple, yet effective web interface for the Intelligent App. This interface will serve as the window through which you can easily view and interact with the results of the dynamic repricing tool. We\u2019ll guide you through the development process, showcasing how to use popular web technologies to build an interface.\\r\\n\\r\\nThis web interface is critical in making the Intelligent App not just a powerful analytical tool but also a practical solution for e-commerce businesses. By providing a clear and intuitive way to access and understand the pricing forecasts, you can efficiently make informed decisions about pricing.\\r\\n\\r\\nThis final piece of the series ties together all the components of the project and highlights the importance of user experience in the deployment of Intelligent Apps.\\r\\n\\r\\n:::info\\r\\nCheck out the [Azure Cosmos DB Ask The Expert](https://aka.ms/intelligent-apps/ate-cosmos?ocid=buildia24_60days_blogs) session to learn how to build RAG solutions, manage chat history by seamlessly connecting with Azure OpenAI, as well as explore the power of Azure Cosmos DB\'s copilot. The experts will also cover how to seamlessly integrate your operational and transactional data with AI frameworks and sdks like Semantic Kernel, Langchain, and LlamaIndex.\\r\\n:::\\r\\n\\r\\n### Harnessing Cosmos DB for Intelligent Apps\\r\\n\\r\\nIn this exploration of how to build an Intelligent App with Cosmos DB, you\u2019ll have completed a project that showcases the power of Azure services and demonstrates the practical applications of these technologies in forecasting for e-commerce. And by walking through the steps needed to use Cosmos DB alongside Azure AI Services, you\u2019re walking away with a blueprint for building apps that can dynamically adjust pricing based on historical data and market trends.\\r\\n\\r\\nStay tuned for our series to dive deeper into the creation of this forecasting tool. Whether you\u2019re looking to enhance your technical skills or implement intelligent solutions in your own projects, following along will shine light onto the value of using Cosmos DB for Intelligent Apps."},{"id":"dynamic-repricing-of-products-using-intelligent-apps-part-1","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1","source":"@site/blog-60daysofIA/2024-03-08/dynamic-repricing-of-products-using-intelligent-apps-part-1.md","title":"3.1 Dynamic Repricing of Products Using Intelligent Apps Part 1","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable. In the first article of this series, you\u2019ll set up and populate the Cosmos DB database with data to use in the later parts of the series.","date":"2024-03-08T09:00:00.000Z","formattedDate":"March 8, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":7.14,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-08T09:00","slug":"dynamic-repricing-of-products-using-intelligent-apps-part-1","title":"3.1 Dynamic Repricing of Products Using Intelligent Apps Part 1","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable. In the first article of this series, you\u2019ll set up and populate the Cosmos DB database with data to use in the later parts of the series.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"3. Cosmos DB and Intelligent Apps: A Match Made for Innovation","permalink":"/Cloud-Native/60DaysOfIA/cosmos-db-and-intelligent-apps-a-match-made-for-innovation"},"nextItem":{"title":"3.2 Dynamic Repricing of Products Using Intelligent Apps Part 2","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-2"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/dynamic-repricing-of-products-using-intelligent-apps-part-1\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable. In the first article of this series, you\u2019ll set up and populate the Cosmos DB database with data to use in the later parts of the series.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-1\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable. In the first article of this series, you\u2019ll set up and populate the Cosmos DB database with data to use in the later parts of the series.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-1\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Cosmos DB and Intelligent Apps: A Match Made for Innovation](../../static/img/60-days-of-ia/blogs/2024-03-08/3-1-1.jpeg)\\r\\n\\r\\n*This three-part series demonstrates how to use Azure Cosmos DB to build an Intelligent App that uses historical pricing and product data to forecast future price fluctuations for specific products. In the first article of this series, you\u2019ll set up and populate the Cosmos DB database with data to use in the later parts of the series.*\\r\\n\\r\\n## Dynamic Repricing of Products Using Intelligent Apps Part 1: Setting Up and Populating Cosmos DB with Data\\r\\n\\r\\nIntelligent Apps leverage data and artificial intelligence (AI) to provide smart, personalized, and adaptive experiences for users. AI and machine learning (ML) techniques like natural language processing (NLP), computer vision, and deep learning help understand context, intent, and user preferences to deliver relevant and timely insights and actions.\\r\\n\\r\\nSome examples of Intelligent Apps include:\\r\\n\\r\\n- **Virtual assistants**\u2014Interactive applications that understand and execute user commands\\r\\n- **Chatbots**\u2014Automated messaging systems that provide information or assistance\\r\\n- **Recommendation** systems\u2014Algorithms that suggest relevant items based on user preferences and behavior\\r\\n\\r\\nIn this three-part series, you\u2019ll create an Intelligent App powered by Azure Cosmos DB and AI/ML capabilities that dynamically suggests changes to product prices based on demand and historical trends. This app will help optimize revenue and customer satisfaction by adjusting product prices according to market conditions and customer behavior.\\r\\n\\r\\n### Laying the Groundwork for an Intelligent App with Cosmos DB\\r\\n\\r\\nFirst, you\u2019ll set up an Azure Cosmos DB database and populate it with product data and historical information about sales and demand. In part 2, you\u2019ll analyze this data using AI and ML to forecast and suggest price changes.\\r\\n\\r\\n#### Prerequisites\\r\\n\\r\\nTo follow this tutorial, ensure you have the following:\\r\\n\\r\\n- [An Azure account](https://azure.microsoft.com/free/?ocid=buildia24_60days_blogs)\\r\\n- A [Kaggle account](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2F) to download the [dataset](https://www.kaggle.com/datasets/sujaykapadnis/price-quote-data/data) this tutorial uses\\r\\n\\r\\n#### Create an Azure Cosmos DB Account\\r\\n\\r\\nAzure Cosmos DB is a fully managed multi-model database that ensures fast access to data, easy scalability, reliable uptime, and strong data consistency. Cosmos DB supports various data models and APIs, including SQL, MongoDB, Cassandra, Gremlin, and table storage, making it easy to query and manipulate data using familiar tools and languages.\\r\\n\\r\\nAlthough you already have an Azure account, you also need to create an Azure Cosmos DB account by following the steps below:\\r\\n\\r\\n1. Sign in to the [Azure portal](https://portal.azure.com/).\\r\\n\\r\\n2. Click **Create a resource** on the upper-left side of the page.\\r\\n\\r\\n3. Search for \u201cAzure Cosmos DB\u201d and select it. On the **Azure Cosmos DB** page, select **Create**.\\r\\n\\r\\n4. Enter the settings for your new account:\\r\\n\\r\\n    - Select your desired subscription.\\r\\n\\r\\n    - Create a new resource group or select an existing one if you have one you\u2019d like to use.\\r\\n\\r\\n    - Enter a unique account name.\\r\\n\\r\\n    - Select **SQL (Core)** as the API. This is the default API for Azure Cosmos DB and allows you to use SQL syntax to query and manage your data.\\r\\n\\r\\n    - Select a **Location** for the account.\\r\\n\\r\\n    - Click **Review + create**.\\r\\n\\r\\n5. Review your account settings and click **Create** to create the account.\\r\\n\\r\\n:::info\\r\\nComplete the **[Data Skills Challenge](https://aka.ms/intelligent-apps/data-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n#### Create a Database and a Container\\r\\n\\r\\nNext, you\u2019ll create a database and container within Azure Cosmos DB. Databases facilitate management, billing, and scaling, while a container is a schema-agnostic grouping of items (documents) with a partition key and a provisioned throughput. The partition property determines how the data is distributed across physical partitions for scalability and performance.\\r\\n\\r\\nTo create a database and container, follow the steps below:\\r\\n\\r\\n1. From the Azure portal, navigate to your Azure Cosmos DB account and select **Data Explorer** on the left menu. In the **Data Explorer**, select **New Database** on the top menu.\\r\\n\\r\\n2. In the **Add Database** panel, enter a name for the new database, like \u201cProductsDB.\u201d\\r\\n\\r\\n3. Check **Provision database throughput** if you want to enable shared throughput for the database. This shares the throughput (RU/s) you provision among all containers in the database. You can also activate or deactivate autoscale, which automatically adjusts the throughput based on your application\u2019s usage patterns.\\r\\n\\r\\n4. Select **OK** to create the database.\\r\\n\\r\\n5. In **Data Explorer**, expand the **ProductsDB** database and select **New Container** on the top menu. Then, open the **Add Container** panel and create a new container:\\r\\n\\r\\n    - Enter \u201cProducts\u201d as the container name.\\r\\n\\r\\n    - Enter \u201c/ITEM_ID\u201d as the container\u2019s partition key. This will [partition](https://learn.microsoft.com/en-us/azure/cosmos-db/partitioning-overview) the data by its `ITEM_ID` property, since columns with a wide range of values make excellent partition keys.\\r\\n\\r\\n    - Use the default value of 400 throughput units. If you\u2019d like, you can also deactivate autoscale for the container.\\r\\n\\r\\n6. Select **OK** to create the container.\\r\\n\\r\\n#### Populate the Container\\r\\n\\r\\nNow that you\u2019ve created your database and container, you need to populate them with some data. For this demonstration, you\u2019ll use a CSV file that contains [UK inflation data](https://www.ons.gov.uk/economy/inflationandpriceindices/datasets/consumerpriceindicescpiandretailpricesindexrpiitemindicesandpricequotes). The dataset contains over 100,000 rows of data representing 600 products sold in UK shops over 12 months.\\r\\n\\r\\nTo populate the container with this data, follow these steps:\\r\\n\\r\\n1. Download the [CSV file](https://www.kaggle.com/datasets/sujaykapadnis/price-quote-data/data).\\r\\n\\r\\n2. In the Azure portal, navigate to your Azure Cosmos DB account and select **Data Explorer** on the left menu.\\r\\n\\r\\n3. In **Data Explorer**, expand the **ProductsDB** database and the **Products** container, and select **Items**.\\r\\n\\r\\n##### *Upload the CSV File*\\r\\n\\r\\nNow, upload the CSV file:\\r\\n\\r\\n1. From the top menu, select **Upload Item**.\\r\\n\\r\\n2. In the **Upload Item** panel, select **Browse**, and choose the CSV file you downloaded previously.\\r\\n\\r\\n3. Select **Upload** to upload the file to the container.\\r\\n\\r\\n4. After the upload finishes, you should see the items in the container, each representing a row in the CSV file. You can select an item to view its properties and values in JSON format.\\r\\n\\r\\n##### *Verify the Data in the Container*\\r\\n\\r\\nTo verify that the data in the container is correct and consistent, you can use the SQL query editor in the Data Explorer.\\r\\n\\r\\n1. Select **New SQL Query**.\\r\\n\\r\\n2. The query editor lets you execute SQL queries against the data in the container. For example, run the following query to get the container\u2019s item count:\\r\\n\\r\\n    ```SELECT VALUE COUNT(1) FROM c```\\r\\n\\r\\n    You should get a result of `10000`, which matches the number of rows in the CSV file.\\r\\n\\r\\n  3. You can also run queries to check the data quality and integrity, like the following:\\r\\n      - **Get the distinct values of ITEM_ID** \u2014 `SELECT DISTINCT VALUE c.ITEM_ID FROM c`\\r\\n      \\r\\n      - **Get the average price of each product** \u2014 `SELECT c.ITEM_ID, c.ITEM_DESC, AVG(c.PRICE) AS avg_price FROM c GROUP BY c.ITEM_ID, c.ITEM_DESC`\\r\\n      - **Get the price trend of a product over time** \u2014 `SELECT c.QUOTE_DATE, c.PRICE FROM c WHERE c.ITEM_ID = \'210102\' ORDER BY c.QUOTE_DATE`\\r\\n4. You can also use the built-in charts to visualize the query results. In the top-right corner of the query editor, select **Chart** and choose the chart type you want to use, such as line, bar, or pie.\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nIn this article, you configured an Azure Cosmos DB database and populated it with data about product price changes. You also verified the data in the container using SQL queries and charts.\\r\\n\\r\\nIn the [next part of the series](https://azure.github.io/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-2), you\u2019ll learn how to use Azure\u2019s AI and ML capabilities to analyze the data and suggest product price forecasts. \\r\\n\\r\\nIf you want to challenge yourself and learn more about Azure, Cosmos DB, and AI/ML, we encourage you to participate in the **[Data Cloud Skill Challenge](https://azure.github.io/Cloud-Native/Build-IA/CloudSkills)**. You can also register for **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days** at the premier conference for cloud-native technologies, *KubeCon EU 2024*."},{"id":"dynamic-repricing-of-products-using-intelligent-apps-part-2","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-2","source":"@site/blog-60daysofIA/2024-03-08/dynamic-repricing-of-products-using-intelligent-apps-part-2.md","title":"3.2 Dynamic Repricing of Products Using Intelligent Apps Part 2","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","date":"2024-03-08T09:01:00.000Z","formattedDate":"March 8, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.785,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-08T09:01","slug":"dynamic-repricing-of-products-using-intelligent-apps-part-2","title":"3.2 Dynamic Repricing of Products Using Intelligent Apps Part 2","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"3.1 Dynamic Repricing of Products Using Intelligent Apps Part 1","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1"},"nextItem":{"title":"3.3 Dynamic Repricing of Products Using Intelligent Apps Part 3: Graphing and Displaying Price Forecasts in a Web Interface","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-3"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/dynamic-repricing-of-products-using-intelligent-apps-part-2\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-2\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-2\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Dynamic Repricing of Products Using Intelligent Apps Part 2: Price Forecasting with AI/ML](../../static/img/60-days-of-ia/blogs/2024-03-08/3-2-1.jpeg)\\r\\n\\r\\n*This three-part series demonstrates how to use Azure Cosmos DB to build an Intelligent App that uses historical pricing and product data to forecast future price fluctuations for specific products. In this installment, you\u2019ll use artificial intelligence and machine learning to build the price forecasting model.*\\r\\n\\r\\n## Dynamic Repricing of Products Using Intelligent Apps Part 2: Price Forecasting with AI/ML\\r\\n\\r\\n[In Part 1 of this series](https://azure.github.io/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1), you set up and populated an [Azure Cosmos DB](https://azure.microsoft.com/free/cosmos-db?ocid=buildia24_60days_blogs) database, laying the groundwork for your Intelligent Application. You also imported your data to a Cosmos DB instance.\\r\\n\\r\\nIn this second article, you\u2019ll use this data alongside Azure\u2019s machine learning (ML) and artificial intelligence (AI) capabilities to build a model that analyzes pricing trends and predicts future prices for a fictional e-commerce business.\\r\\n\\r\\n### Analyzing Price Trends to Predict Future Prices\\r\\n\\r\\nThe ability to forecast pricing is a game-changer. With the power of foresight, businesses can preemptively adjust their pricing strategies in line with market expectations.\\r\\n\\r\\nIn this tutorial, we\u2019ll give you a step-by-step guide to generating a predictive ML model for an e-commerce business, using Azure\u2019s suite of ML tools.\\r\\n\\r\\n#### Prerequisites\\r\\n\\r\\nBefore you begin, make sure you have the following:\\r\\n\\r\\n- An active [Azure Account](https://azure.microsoft.com/free/?ocid=buildia24_60days_blogs)\\r\\n- A Cosmos DB instance with the [pricing data](https://www.kaggle.com/datasets/sujaykapadnis/price-quote-data/data) you set up in Part 1\\r\\n- Access to [Azure Machine Learning Studio](https://studio.azureml.net/)\\r\\n- An [Azure Machine Learning workspace](https://learn.microsoft.com/azure/machine-learning/tutorial-azure-ml-in-a-day?view=azureml-api-2&ocid=buildia24_60days_blogs)\\r\\n- A [Jupyter notebook set up](https://learn.microsoft.com/azure/machine-learning/quickstart-create-resources?view=azureml-api-2#create-a-new-notebook&ocid=buildia24_60days_blogs) in your workspace\\r\\n- Familiarity with [Azure Machine Learning](https://azure.microsoft.com/products/machine-learning?ocid=buildia24_60days_blogs) concepts\\r\\n- Basic Python programming knowledge and understanding of ML concepts\\r\\n\\r\\n**Note**: You should add and run all code in this article into your Jupyter Notebook in the order in which it appears.\\r\\n\\r\\n:::info\\r\\nCheck out the Azure **[Cosmos DB Ask The Expert](https://aka.ms/intelligent-apps/ate-cosmos?ocid=buildia24_60days_blogs)** session to learn how to build RAG solutions, manage chat history by seamlessly connecting with *Azure OpenAI*, as well as explore the power of *Azure Cosmos DB\'s copilot*. The experts will also cover how to seamlessly integrate your operational and transactional data with AI frameworks and sdks like Semantic Kernel, Langchain, and LlamaIndex. \\r\\n:::\\r\\n\\r\\n#### Extract Historical Pricing Data from Cosmos DB\\r\\n\\r\\nStart by extracting historical pricing data from Cosmos DB, where you stored it in Part 1. For this tutorial, you\u2019ll extract items with names ending in `JACKET`. Because the dataset is relatively small, a simple `like` query will do. However, when working with larger data sets, you should consider additional upfront data cleaning and categorizing, to ensure you can query your database efficiently.\\r\\n\\r\\nRun the code below to extract the data:\\r\\n\\r\\n```\\r\\nfrom azure.cosmos import CosmosClient, exceptions\\r\\nimport pandas as pd\\r\\n```\\r\\n```\\r\\n# Initialize a Cosmos client\\r\\nendpoint = \\"your_cosmos_db_endpoint\\"\\r\\nkey = \'your_cosmos_db_key\'\\r\\nclient = CosmosClient(endpoint, key)\\r\\n```\\r\\n```\\r\\n# Connect to the database and container\\r\\ndatabase_name = \'your_database_name\'\\r\\ncontainer_name = \'your_container_name\'\\r\\ndatabase = client.get_database_client(database_name)\\r\\ncontainer = database.get_container_client(container_name)\\r\\n```\\r\\n```\\r\\n# Query these items using the SQL query syntax\\r\\nquery = \\"SELECT * FROM c where ITEM_DESC like \'%JACKET\'\\"\\r\\nitems = list(container.query_items(query=query, enable_cross_partition_query=True))\\r\\n```\\r\\n```\\r\\n# Convert the query result to a DataFrame\\r\\npricing_data = pd.DataFrame(items)\\r\\n```\\r\\n\\r\\n#### Preprocess Data and Split into Training and Testing\\r\\n\\r\\nBefore feeding the data into an ML model, preprocess it and split it into training and testing sets using the code below:\\r\\n\\r\\n```\\r\\nfrom sklearn.model_selection import train_test_split\\r\\n```\\r\\n```\\r\\n# Assume the DataFrame `pricing_data` has columns: \'quote_date\', \'price\', \'price_relative\', \'item_id\', etc.\\r\\n```\\r\\n```\\r\\n# Convert \'quote_date\' from string to datetime for proper chronological splitting\\r\\npricing_data[\'QUOTE_DATE\'] = pd.to_datetime(pricing_data[\'QUOTE_DATE\'], format=\'%Y%m\')\\r\\n```\\r\\n```\\r\\n# Selecting the features and target for the model\\r\\nX = pricing_data[[\'QUOTE_DATE\', \'ITEM_ID\', \'PRICE_RELATIVE\',\'STRATUM_WEIGHT\', \'SHOP_WEIGHT\']]\\r\\ny = pricing_data[\'price\']\\r\\n```\\r\\n```\\r\\n# Split the data into training and testing sets\\r\\n# We\'ll use a chronological split rather than a random split to maintain the time series integrity\\r\\nsplit_date = pd.Timestamp(\'YYYY-MM-DD\')  # replace with the appropriate date\\r\\ntrain = pricing_data.loc[pricing_data[\'QUOTE_DATE\'] <= split_date]\\r\\ntest = pricing_data.loc[pricing_data[\'QUOTE_DATE\'] > split_date]\\r\\n```\\r\\n```\\r\\nX_train, y_train = train[[\'ITEM_ID\', \'PRICE_RELATIVE\', \'STRATUM_WEIGHT\', \'SHOP_WIGHT\']], train[\'PRICE\']\\r\\nX_test, y_test = test[[\'ITEM_ID\', \'PRICE_RELATIVE\', \'STRATUM_WEIGHT\', \'SHOP_WEIGHT\']], test[\'PRICE\']\\r\\n```\\r\\n\\r\\n#### Train a Forecasting Model Using Azure Machine Learning\\r\\n\\r\\nNext, you\u2019ll build and train the forecasting model using Azure Machine Learning. Note that in the code below, you\u2019re using a local compute target, which works on simple datasets like the one used for this tutorial. However, Azure Machine Learning offers more powerful compute targets for more complex models.\\r\\n\\r\\n```\\r\\nfrom azureml.core import Workspace, Experiment, Environment\\r\\nfrom azureml.train.automl import AutoMLConfig\\r\\n```\\r\\n```\\r\\n# Connect to your Azure ML workspace\\r\\nws = Workspace.from_config()\\r\\n```\\r\\n```\\r\\n# Define your experiment\\r\\nexperiment_name = \'price_forecasting_experiment\'\\r\\nexperiment = Experiment(ws, experiment_name)\\r\\n```\\r\\n```\\r\\n# Configure the automated ML job \\r\\n\\r\\nautoml_config = AutoMLConfig(\\r\\n    task=\'forecasting\',\\r\\n    primary_metric=\'normalized_root_mean_squared_error\',\\r\\n    experiment_timeout_minutes=30,\\r\\n    training_data=train,\\r\\n    label_column_name=\'PRICE\',\\r\\n    n_cross_validations=5,\\r\\n    enable_early_stopping=True,\\r\\n    verbosity=logging.INFO,\\r\\n    compute_target=\'local\'\\r\\n) \\r\\n```\\r\\n```\\r\\n# Submit the experiment\\r\\nrun = experiment.submit(automl_config, show_output=True)\\r\\n```\\r\\n\\r\\n#### Evaluate and Integrate the Model\\r\\n\\r\\nNext, check the results of the model by running the following:\\r\\n\\r\\n```\\r\\nfrom azureml.widgets import RunDetails\\r\\n```\\r\\n```\\r\\n# Show run details while running\\r\\nRunDetails(run).show()\\r\\n```\\r\\n```\\r\\n# Wait for the run to complete\\r\\nrun.wait_for_completion()\\r\\n```\\r\\n```\\r\\n# Retrieve the best model from the AutoML run\\r\\nbest_run, fitted_model = run.get_output()\\r\\nprint(best_run)\\r\\nprint(fitted_model)\\r\\n```\\r\\n```\\r\\n# Evaluate the best model\'s accuracy using the test data\\r\\n# Assuming test data is a Pandas DataFrame with the same structure as the training data\\r\\nX_test = test_data.drop(\'PRICE\', axis=1)  # Features (drop the target column)\\r\\ny_test = test_data[\'PRICE\']  # True values of the target column\\r\\n```\\r\\n```\\r\\n# Predict using the fitted model\\r\\ny_pred = fitted_model.predict(X_test)\\r\\n```\\r\\n```\\r\\n# Calculate the accuracy or any other performance metrics\\r\\nfrom sklearn.metrics import mean_squared_error, r2_score\\r\\nmse = mean_squared_error(y_test, y_pred)\\r\\nr2 = r2_score(y_test, y_pred)\\r\\n```\\r\\n```\\r\\nprint(f\\"Mean Squared Error: {mse}\\")\\r\\nprint(f\\"R-squared: {r2}\\")\\r\\n```\\r\\n\\r\\nWith the performance metrics calculated, you can now determine whether the model\u2019s predictions are accurate enough for your needs. If they are, you can integrate the model with a hypothetical e-commerce platform. The easiest way to integrate a model is to deploy it using an Azure Machine Learning endpoint:\\r\\n\\r\\n```\\r\\nws = Workspace.from_config() \\r\\n```\\r\\n```\\r\\n# Register the model from the best run\\r\\nmodel = best_run.register_model(model_name=\'price_forecast_model\', model_path=\'outputs/model.pkl\') \\r\\n```\\r\\n```\\r\\n# Download the scoring file produced by AutoML\\r\\nbest_run.download_file(\'outputs/scoring_file_v_1_0_0.py\', \'score.py\')\\r\\n```\\r\\n```\\r\\n# Download the environment file produced by AutoML\\r\\nbest_run.download_file(constants.CONDA_ENV_FILE_PATH, \'environment.yml\')\\r\\n```\\r\\n```\\r\\n# Create the environment\\r\\nenv = Environment.from_conda_specification(name=\'forecasting_environment\', file_path=\'environment.yml\')\\r\\n```\\r\\n```\\r\\n# Create the inference configuration\\r\\ninference_config = InferenceConfig(entry_script=\'score.py\', environment=env)\\r\\n```\\r\\n```\\r\\n# Create the deployment configuration\\r\\ndeployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\\r\\n```\\r\\n```\\r\\n# Deploy the model as a web service\\r\\nservice_name = \'price-forecast-service\'\\r\\nservice = Model.deploy(ws, service_name, [model], inference_config, deployment_config) \\r\\nservice.wait_for_deployment(show_output=True)\\r\\n```\\r\\n```\\r\\n# The web service endpoint URL\\r\\nprint(service.scoring_uri)\\r\\n```\\r\\n\\r\\nAnd with that, you\u2019ve deployed your Azure ML endpoint and are ready for Part 3!\\r\\n\\r\\n### Conclusion\\r\\n\\r\\nIn this tutorial, you extracted data from Cosmos DB, preprocessed it, performed a train/test split, initiated a model training pipeline using Azure Machine Learning, and, finally, tested and deployed the model. These are crucial steps to building a system that can intelligently forecast product prices.\\r\\n\\r\\nIn the third and final article of this series, you\u2019ll build a web interface that displays the generated price forecasts using approachable, simple graphs that help businesses easily make data-informed decisions.\\r\\n\\r\\nTo challenge yourself, learn more about Azure\u2019s AI and ML tooling, and put the skills you\u2019ve learned in this tutorial to work, participate in the [Data Cloud Skill Challenge](https://azure.github.io/Cloud-Native/Build-IA/CloudSkills). You can also register for **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days** at the premier conference for cloud-native technologies, *KubeCon EU 2024*."},{"id":"dynamic-repricing-of-products-using-intelligent-apps-part-3","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-3","source":"@site/blog-60daysofIA/2024-03-12/dynamic-repricing-of-products-using-intelligent-apps-part-3.md","title":"3.3 Dynamic Repricing of Products Using Intelligent Apps Part 3: Graphing and Displaying Price Forecasts in a Web Interface","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","date":"2024-03-08T09:05:00.000Z","formattedDate":"March 8, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":8.03,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-08T09:05","slug":"dynamic-repricing-of-products-using-intelligent-apps-part-3","title":"3.3 Dynamic Repricing of Products Using Intelligent Apps Part 3: Graphing and Displaying Price Forecasts in a Web Interface","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"3.2 Dynamic Repricing of Products Using Intelligent Apps Part 2","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-2"},"nextItem":{"title":"4. Fuel Your Intelligent Apps with Azure AI","permalink":"/Cloud-Native/60DaysOfIA/fuel-your-intelligent-apps-with-azure-ai"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/dynamic-repricing-of-products-using-intelligent-apps-part-3\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-3\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-3\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Dynamic Repricing of Products Using Intelligent Apps: Graphing and Displaying Price Forecasts in a Web Interface](../../static/img/60-days-of-ia/blogs/2024-03-12/3-3-1.png)\\r\\n\\r\\n*This three-part series demonstrates how to use Azure Cosmos DB to build an Intelligent App that uses historical pricing and product data to forecast future price fluctuations for specific products. In the final article of the series, you\u2019ll build a web interface to graph and display the Intelligent App\u2019s price forecasts.*\\r\\n\\r\\n## Dynamic Repricing of Products Using Intelligent Apps Part 3: Graphing and Displaying Price Forecasts in a Web Interface\\r\\n\\r\\nIn [\u200b\u200bPart 1](https://azure.github.io/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1) of this series, you set up an Azure Cosmos DB database and populated the database with pricing data. Then, in \u200b[\u200bPart 2](https://azure.github.io/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-2), you successfully set up an Azure Machine Learning model and deployed it as a web service.\\r\\n\\r\\nIn this final article of the series, you\u2019ll create a web application using Flask that interacts with the Azure Machine Learning endpoint to retrieve predictions and display them using a simple graph.\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nBefore proceeding, ensure you have the following:\u202f \\r\\n\\r\\n- [Python](https://www.python.org/downloads/) version 3.10 or greater\\r\\n- Flask (`pip install flask`)\\r\\n- Requests (`pip install requests`)\\r\\n- Matplotlib (`pip install matplotlib`)\\r\\n- Access to the Azure Machine Learning endpoint created in Part 2\\r\\n- [Docker](https://docs.docker.com/get-docker/), including the [Docker command-line interface](https://docs.docker.com/engine/reference/commandline/cli/) (CLI), installed. You\u2019ll use this to build a container image to run the web app on Azure Kubernetes Service (AKS).\\r\\n- The [Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli?ocid=buildia24_60days_blogs) installed. You\u2019ll use this for deployment to AKS.\\r\\n\\r\\nFor a preview of the completed Intelligent App, take a look at the [\u200bproject code](https://aka.ms/intelligent-apps/60daysofIA/3.3projectcode).\\r\\n\\r\\n:::info\\r\\nComplete the **[Data Skills Challenge](https://aka.ms/intelligent-apps/data-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n### Building the Web Interface\\r\\n\\r\\nIt only takes a few steps to create a simple web app that queries the Azure Machine Learning endpoint, retrieves predictions, and displays the resulting prediction in a graph. Let\u2019s dive in!\\r\\n\\r\\nStart by creating a new folder for your web application. Then, create these files and folders in it:\\r\\n\\r\\n```\\r\\n/your-flask-app\\r\\n\u202f\u202f\u202f /templates\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f index.html\\r\\n\u202f\u202f\u202f app.py\\r\\n```\\r\\n\\r\\nThe `app.py` file is the backbone of the Flask application. So, add the following code to it:\\r\\n\\r\\n```\\r\\nfrom flask import Flask, render_template, request\\r\\nimport requests\\r\\nimport json\\r\\nimport matplotlib.pyplot as plt\\r\\nimport io\\r\\nimport base64\\r\\nfrom datetime import datetime, timedelta\\r\\n```\\r\\n\\r\\n```\\r\\napp = Flask(__name__)\\r\\n```\\r\\n\\r\\n```\\r\\n# Replace with your actual Azure ML endpoint and key\\r\\nscoring_uri = \'<your_azure_ml_endpoint>\'\\r\\napi_key = \'<your_api_key>\'\u202f # Replace with your actual key if needed\\r\\n```\\r\\n```\\r\\ndef generate_future_dates(start_date, periods=3, freq=\'M\'):\\r\\n\u202f\u202f\u202f # Generate future dates for the next \'periods\' months\\r\\n\u202f\u202f\u202f future_dates = [(start_date + timedelta(days=30 * i)).strftime(\'%Y%m\') for i in range(1, periods + 1)]\\r\\n\u202f\u202f\u202f return future_dates\\r\\n```\\r\\n\\r\\n```\\r\\ndef get_predictions(dates):\\r\\n\u202f\u202f\u202f # Prepare the data in JSON format\\r\\n\u202f\u202f\u202f data = {\\"data\\": [[date] for date in dates]}\\r\\n\u202f\u202f\u202f headers = {\'Content-Type\': \'application/json\'}\\r\\n\u202f\u202f\u202f if api_key:\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f headers[\'Authorization\'] = f\'Bearer {api_key}\'\\r\\n\\r\\n\u202f\u202f\u202f # Send the request to the Azure ML endpoint\\r\\n\u202f\u202f\u202f response = requests.post(scoring_uri, json=data, headers=headers)\\r\\n\u202f\u202f\u202f if response.status_code == 200:\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f return response.json()\\r\\n\u202f\u202f\u202f else:\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f raise Exception(f\\"Failed to fetch prediction: {response.text}\\")\\r\\n```\\r\\n\\r\\n```\\r\\n@app.route(\'/\', methods=[\'GET\', \'POST\'])\\r\\ndef index():\\r\\n\u202f\u202f\u202f graph_url = None\\r\\n\u202f\u202f\u202f if request.method == \'POST\':\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f start_date = datetime.utcnow()\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f future_dates = generate_future_dates(start_date)\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f predictions = get_predictions(future_dates)\\r\\n```\\r\\n\\r\\n```\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f # Plotting the predictions\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.figure(figsize=(10, 5))\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.plot(future_dates, predictions, marker=\'o\', linestyle=\'-\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.title(\'Future Price Predictions for Jackets\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.xlabel(\'Date (YYYYMM)\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.ylabel(\'Predicted Price\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.grid(True)\\r\\n\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f # Save plot to a BytesIO buffer\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f img = io.BytesIO()\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.savefig(img, format=\'png\', bbox_inches=\'tight\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f img.seek(0)\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f graph_url = base64.b64encode(img.getvalue()).decode(\'utf8\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.close()\\r\\n```\\r\\n```\\r\\n\u202f\u202f\u202f return render_template(\'index.html\', graph_url=graph_url)\\r\\n```\\r\\n```\\r\\nif __name__ == \'__main__\':\\r\\n\u202f\u202f\u202f app.run(debug=True)\\r\\n```\\r\\n\\r\\nThis simple Flask app accepts incoming requests and queries the Azure Machine Learning endpoint for the next few months of price forecasts for jackets. When it receives the predictions, it generates a graph using `matplotlib`, encoding it with base64 so it can display it in the HTML template. In a larger app, you could save the image to disk and then load it in the web page instead of base64 encoding it\u2014but we\u2019ve skipped that here to keep things simple.\\r\\n\\r\\nNext, create an `index.html` file in the templates directory. Add the following code for the user interface:\\r\\n\\r\\n```\\r\\n<!DOCTYPE html>\\r\\n<html lang=\\"en\\">\\r\\n<head>\\r\\n\u202f\u202f\u202f <meta charset=\\"UTF-8\\">\\r\\n\u202f\u202f\u202f <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1.0\\">\\r\\n\u202f\u202f\u202f <title>Price Forecast Visualization</title>\\r\\n\u202f\u202f\u202f \x3c!-- Load Tailwind CSS from CDN --\x3e\\r\\n\u202f\u202f\u202f <link href=\\"https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css\\" rel=\\"stylesheet\\">\\r\\n</head>\\r\\n<body class=\\"bg-gray-100 flex flex-col justify-center items-center min-h-screen\\">\\r\\n\u202f\u202f\u202f <div class=\\"w-full bg-blue-800 text-white\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f <div class=\\"container mx-auto py-4\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f <h1 class=\\"text-center text-xl md:text-3xl font-bold\\">Price Forecast for Jackets</h1>\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f </div>\\r\\n\u202f\u202f\u202f </div>\\r\\n\\r\\n\u202f\u202f\u202f <div class=\\"mt-8 mb-4\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f <form method=\\"post\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f <button type=\\"submit\\" class=\\"bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f Get Future Price Predictions\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f </button>\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f </form>\\r\\n\u202f\u202f\u202f </div>\\r\\n\\r\\n\u202f\u202f\u202f {% if graph_url %}\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f <div class=\\"shadow-xl bg-white rounded-lg p-8\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f <h2 class=\\"text-lg md:text-xl font-semibold mb-4 text-center\\">Price Prediction Graph</h2>\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f <div class=\\"flex justify-center\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f <img src=\\"data:image/png;base64,{{ graph_url }}\\" alt=\\"Price Prediction Graph\\" class=\\"max-w-full h-auto rounded-lg\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f </div>\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f </div>\\r\\n\u202f\u202f\u202f {% endif %}\\r\\n</body>\\r\\n</html>\\r\\n```\\r\\n\\r\\nTo run your Flask app, navigate to the directory containing your `app.py` file and execute the following command:\\r\\n\\r\\n```\\r\\nflask run\\r\\n```\\r\\n\\r\\nYour web application should now be accessible at `http://127.0.0.1:5000`. Users can input feature data, submit it, and see both the predicted price and a simple graph comparing the current and predicted prices.\\r\\n\\r\\n:::info\\r\\nCheck out the **[Azure Cosmos DB Ask The Expert](https://aka.ms/intelligent-apps/ate-cosmos?ocid=buildia24_60days_blogs)** session to learn how to build RAG solutions, manage chat history by seamlessly connecting with *Azure OpenAI*, as well as explore the power of *Azure Cosmos DB\'s copilot*.\\r\\n\\r\\nThe experts also cover how to seamlessly integrate your operational and transactional data with AI frameworks and sdks like Semantic Kernel, Langchain, and LlamaIndex.\\r\\n:::\\r\\n\\r\\n### Deploying to Azure Kubernetes Service (AKS)\\r\\n\\r\\nRunning locally is great, but in production, you\u2019ll probably want to deploy to the cloud. Fortunately, Azure makes this easy. Let\u2019s review how to deploy your Flask app using AKS.\\r\\n\\r\\nFirst, you need to containerize the Flask app and push it to an Azure Container Registry. Then, you\u2019ll create an AKS cluster and deploy the container image to it.\\r\\n\\r\\n#### Create a Dockerfile\\r\\n\\r\\nStart by creating a file named `Dockerfile` in the Flask app\u2019s root folder. Add the following contents:\\r\\n\\r\\n```\\r\\nFROM python:3.11-slim\\r\\n```\\r\\n```\\r\\nWORKDIR /usr/src/app\\r\\n```\\r\\n```\\r\\nRUN pip install --no-cache-dir Flask\\r\\n```\\r\\n```\\r\\nCOPY . .\\r\\n```\\r\\n```\\r\\nCMD [\\"flask\\", \\"run\\"]\\r\\n```\\r\\n\\r\\n#### Create a Container Registry\\r\\n\\r\\nNext, create a container registry to store the container image. Use the Azure CLI to create a new resource group if you don\u2019t already have one you\u2019d like to use:\\r\\n\\r\\n```\\r\\naz group create --name my-container-resources --location eastus\\r\\n```\\r\\n\\r\\nThen, create a container registry in the resource group:\\r\\n\\r\\n```\\r\\naz acr create --resource-group my-container-resources --name my-registry --sku Basic\\r\\n```\\r\\n\\r\\nYou\u2019re now ready to build the container and push it to the registry.\\r\\n\\r\\n#### Build and Push the Container Image\\r\\n\\r\\nBuild the container image using the following command:\\r\\n\\r\\n```\\r\\ndocker build -t my-app-image .\\r\\n```\\r\\n\\r\\nThen, push the image to your container registry:\\r\\n\\r\\n```\\r\\ndocker push my-registry.azurecr.io/my-app-image\\r\\n```\\r\\n\\r\\n#### Create an AKS Cluster\\r\\n\\r\\nNow, it\u2019s time to create an AKS cluster. Run the following:\\r\\n\\r\\n```\\r\\naz aks create --name my-aks-cluster --resource-group my-resource-group --node-count 3 --node-vm-size Standard_B2s --location eastus\\r\\n```\\r\\n\\r\\nIt may take a few minutes for Azure to spin up your cluster. Once it\u2019s ready, you can deploy the Flask app.\\r\\n\\r\\n#### Deploy the Application to AKS\\r\\n\\r\\nCreate a Kubernetes deployment file named `deployment.yaml` in the project\u2019s root folder with the following contents. Update the image field to match the name of your registry and container image.\\r\\n\\r\\n```\\r\\napiVersion: apps/v1\\r\\nkind: Deployment\\r\\nmetadata:\\r\\n\u202f name: my-app-deployment\\r\\nspec:\\r\\n\u202f replicas: 1\\r\\n\u202f selector:\\r\\n\u202f\u202f\u202f matchLabels:\\r\\n\u202f\u202f\u202f\u202f\u202f app: my-app\\r\\n\u202f template:\\r\\n\u202f\u202f\u202f metadata:\\r\\n\u202f\u202f\u202f\u202f\u202f labels:\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f app: my-app\\r\\n\u202f\u202f\u202f spec:\\r\\n\u202f\u202f\u202f\u202f\u202f containers:\\r\\n\u202f\u202f\u202f\u202f\u202f - name: my-app\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f image: my-registry.azurecr.io/my-app-image\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f ports:\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f - containerPort: 5000\\r\\n```\\r\\n\\r\\nFinally, deploy the application to the AKS cluster using the following command:\\r\\n\\r\\n```\\r\\nkubectl apply -f deployment.yaml\\r\\n```\\r\\n#### Verify the Deployment\\r\\n\\r\\nOnce deployed, verify that the application is running using the following command:\\r\\n\\r\\n```\\r\\nkubectl get pods\\r\\n```\\r\\n\\r\\nYou should see a pod named `my-app` in the `Running` state.\\r\\n\\r\\nTo access the application, port-forward the service using the following command:\u202f\\r\\n\\r\\n```\\r\\nkubectl port-forward svc/my-app-service 5000:5000\\r\\n```\\r\\n\\r\\nFinally, navigate to `http://localhost:5000` in a web browser to verify the application is running.\\r\\n\\r\\n### Conclusion\\r\\n\\r\\nIn the final part of this series, you learned how to create a simple Flask web app that interacts with the Azure Machine Learning endpoint to provide real-time price predictions and visualize them. By integrating cloud-based artificial intelligence (AI) models with a web interface like this, businesses can dynamically adjust their pricing\u2014helping them remain competitive and stand out from the rest.\\r\\n\\r\\nIf you like what you\u2019ve seen in this series, try the **[Intelligent Apps Cloud Skill Challenge](https://aka.ms/intelligent-apps/csc)**. You can also register for **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days** at the premier conference for cloud-native technologies, *KubeCon EU 2024*."},{"id":"fuel-your-intelligent-apps-with-azure-ai","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/fuel-your-intelligent-apps-with-azure-ai","source":"@site/blog-60daysofIA/2024-03-11/fuel-your-intelligent-apps-with-azure-ai.md","title":"4. Fuel Your Intelligent Apps with Azure AI","description":"In this kickoff post, we\'ll set the stage for the week of posts by describing the application scenario (motivation) and introducing core terminology (LLM Ops), developer tools (Azure AI Studio, frameworks) and design patterns (RAG) to help you jumpstart your journey building and deploying generative AI solutions in the enterprise. By the end of this week, you should have a good understanding of how to build a copilot app end-to-end on the Azure AI platform, how to deploy it for integration with real-world applications, and how to incorporate responsible AI principles into your development workflow.","date":"2024-03-11T09:00:00.000Z","formattedDate":"March 11, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":9.09,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-11T09:00","slug":"fuel-your-intelligent-apps-with-azure-ai","title":"4. Fuel Your Intelligent Apps with Azure AI","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["promptflow","azure","aistudio","generativeai","e2e","llmops"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this kickoff post, we\'ll set the stage for the week of posts by describing the application scenario (motivation) and introducing core terminology (LLM Ops), developer tools (Azure AI Studio, frameworks) and design patterns (RAG) to help you jumpstart your journey building and deploying generative AI solutions in the enterprise. By the end of this week, you should have a good understanding of how to build a copilot app end-to-end on the Azure AI platform, how to deploy it for integration with real-world applications, and how to incorporate responsible AI principles into your development workflow.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"3.3 Dynamic Repricing of Products Using Intelligent Apps Part 3: Graphing and Displaying Price Forecasts in a Web Interface","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-3"},"nextItem":{"title":"4.1 Build Contoso Chat End-to-End","permalink":"/Cloud-Native/60DaysOfIA/build-contoso-chat-end-to-end"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/fuel-your-intelligent-apps-with-azure-ai\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this kickoff post, we\'ll set the stage for the week of posts by describing the application scenario (motivation) and introducing core terminology (LLM Ops), developer tools (Azure AI Studio, frameworks) and design patterns (RAG) to help you jumpstart your journey building and deploying generative AI solutions in the enterprise. By the end of this week, you should have a good understanding of how to build a copilot app end-to-end on the Azure AI platform, how to deploy it for integration with real-world applications, and how to incorporate responsible AI principles into your development workflow.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/fuel-your-intelligent-apps-with-azure-ai\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this kickoff post, we\'ll set the stage for the week of posts by describing the application scenario (motivation) and introducing core terminology (LLM Ops), developer tools (Azure AI Studio, frameworks) and design patterns (RAG) to help you jumpstart your journey building and deploying generative AI solutions in the enterprise. By the end of this week, you should have a good understanding of how to build a copilot app end-to-end on the Azure AI platform, how to deploy it for integration with real-world applications, and how to incorporate responsible AI principles into your development workflow.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/fuel-your-intelligent-apps-with-azure-ai\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n# Kicking Off Azure AI Week!\\r\\n\\r\\nWelcome to the `Azure AI` week on **#60Days Of IA**. Over the next 5 days, we\'ll share a series of blog posts that give you a comprehensive look at the tools and end-to-end development workflow reequired to build intelligent applications [code-first on the Azure AI platform](https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/a-code-first-experience-for-building-a-copilot-with-azure-ai/ba-p/4058659?ocid=buildia24_60days_blogs). \\r\\n\\r\\nIn this kickoff post, we\'ll set the stage for the week of posts by describing the application scenario (motivation) and introducing core terminology (LLM Ops), developer tools (Azure AI Studio, frameworks) and design patterns (RAG) to help you jumpstart your journey building and deploying generative AI solutions in the enterprise. By the end of this week, you should have a good understanding of how to build a copilot app end-to-end on the Azure AI platform, how to deploy it for integration with real-world applications, and how to incorporate responsible AI principles into your development workflow.\\r\\n\\r\\nReady? Let\'s get started!\\r\\n\\r\\n## What We\'ll Cover Today\\r\\n * **Application Scenario |** What is Contoso Chat?\\r\\n * **Paradigm Shift |** What is LLM Ops?\\r\\n * **Unified Platform |** What is Azure AI Studio?\\r\\n * **Copilot Experience |** What is the development workflow?\\r\\n * **The Week Ahead |** What will we cover?\\r\\n * **Resources:** [Explore the Code-First Azure AI Collection](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs)\\r\\n\\r\\n---\\r\\n\\r\\n![Roadmap](../../static/img/60-days-of-ia/blogs/2024-03-11/banner.png)\\r\\n\\r\\n<br/>\\r\\n\\r\\nGenerative AI applications are transforming the user experience and accelerating adoption of AI tools and solutions in the enterprise. But as developers, we face new challenges in building solutions **end-to-end** - from prompt engineering to LLM Ops. We need new tools, frameworks, and guidance to help us navigate and streamline a fast-growing ecosystem. \\r\\n\\r\\nIn [a recent blog post](https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/a-code-first-experience-for-building-a-copilot-with-azure-ai/ba-p/4058659?ocid=buildia24_60days_blogs) we described how the Azure AI platform is addressing these challanges with a _code-first experience for building a copilot application end-to-end_ with your data and APIs. This week, we unpack that post in more detail - walking you through a end-to-end application sample, and several _quickstart_ options, to get you started on your own generative AI solutions.\\r\\n\\r\\nTo kick things off, let\'s set the stage by describing a common generative AI application scenario (\\"Contoso Chat\\") and introduce core terminology, tools and processes that we will be using throughout the week, on our development journey.\\r\\n\\r\\n## 1 | The Application Scenario\\r\\n\\r\\nSay hello to _Contoso Outdoor Company_ - an online retailer of outdoor adventuring equipment with a loyal and growing customer base. Your website has a rich catalog of items organized into categories like _tents_, _backpacks_, _hiking boots_ and more. Customers visit the site looking to find the best gear for their next adventure, and often have questions about the products, or how they might fit with their previous purchases.\\r\\n\\r\\n![Contoso Outdoors site](../../static/img/60-days-of-ia/blogs/2024-03-11/app-contoso-outdoors.png)\\r\\n\\r\\nThe company has a customer support line, but it is getting overwhelmed with calls and you don\'t have the resources to meet the demand. You hear about generative AI applications and decide to build a _customer support chat AI_ agent that knows your catalog and customers. You can then integrate it into the site as shown, to improve customer satisfaction and drive follow-up actions.\\r\\n\\r\\n![Contoso Chat concept](../../static/img/60-days-of-ia/blogs/2024-03-11/app-contoso-chat-concept.png)\\r\\n\\r\\nYou identify three requirements for your chat AI application:\\r\\n - **Custom Data**. The application responses must prioritize your catalog data.\\r\\n - **Responsible AI**. The application must follow responsible AI principles.\\r\\n - **LLM Ops**. The end-to-end development workflow must be operationalizable.\\r\\n\\r\\n## 2 | The Paradigm Shift\\r\\n\\r\\nBuilding generative AI applications requires a different mindset from traditional ML applications. The latter are trained on finite custom data, deploying an endpoint that makes _predictions_. By contrast, generative AI applications are trained on massive amounts of data, using large language models (LLM) and natural language processing (NLP) to _generate_ new content.\\r\\n\\r\\nThe focus now moves from **MLOps** (workflow for building ML apps) to **LLMOps** (workflow for building generative AI apps) - starting with _prompt engineering_, a process where we refine the inputs to the LLM (\\"prompts\\") through a process of trial-and-error (build-run-evaluate) till the responses meet our quality, cost and performance requirements. The generative AI application lifecycle now looks more like this:\\r\\n\\r\\n![LLM App Lifecyle](../../static/img/60-days-of-ia/blogs/2024-03-11/llm-app-lifecycle.png)\\r\\n\\r\\n1. **Ideation Phase**: Start by building the basic AI application (copilot) for your scenario. At this stage, you define the architectural elements (AI resources, design patterns) and language models (chat completion, chat evaluation, text embeddings) that you will need to build-run-evaluate the basic experience. And have sample data to test against.\\r\\n2. **Augmentation Phase**: Iteratively refine the quality and performance of your application by _engineering_ the prompts, _tuning_ the models, and _evaluating_ the responses with sample data (smal) and batch runs (large). Use relevant metrics (groundedness, coherence, relevance, fluency) to guide decisions on what to change, and when to stop iterating.\\r\\n3. **Operationalization Phase:** Now, you\'re ready to deploy the application to a production environment so that the endpoint can be accessed by others, for integrating into user-facing experiences. This is also a good time to review the entire workflow for responsible AI practices, and explore automation and monitoring solutions for efficiency and performance.\\r\\n\\r\\n## 3 | The Azure AI Platform\\r\\n\\r\\nImplementing this end-to-end workflow and managing the various phases of the application lifecycle can be challenging for developers. Azure AI Studio addresses these challenges with a [**unified platform**](https://ai.azure.com?ocid=buildia24_60days_blogs) for building generative AI applications and custom copilot experiences. \\r\\n\\r\\nUse the platform to **explore** language models from Microsoft and the broader community, and experiment with them in a built-in playground. Then **build** your \\r\\nAI project by seamlessly integrating with deployed models and built-in AI services - and **manage** your AI resources (for compute, access, billing and more) from the unified UI. \\r\\n\\r\\n![Azure AI Studio](../../static/img/60-days-of-ia/blogs/2024-03-11/azure-ai.png)\\r\\n\\r\\nAs a developer, you have both low-code and code-first options for engaging with the platform. Use the [Azure AI Studio UI](https://ai.azure.com?ocid=buildia24_60days_blogs) for a browser-based low-code experience, and the [Azure AI SDK](https://learn.microsoft.com/azure/ai-studio/how-to/sdk-generative-overview?ocid=buildia24_60days_blogs) for a Python-based code-first experience. In our posts this week, we\'ll focus on the code-first experience, and show you how to build a copilot app on Azure AI using the Python SDK and popular frameworks.\\r\\n\\r\\n\\r\\n## 4 | The Copilot Experience\\r\\n\\r\\nSo how do we get started on the end-to-end development journey using the Azure AI platform? Let\'s start by defining what we mean by a _copilot_ experience for enterprise-grade generative AI applications. A copilot is:\\r\\n - a generative AI application that uses large language models (LLM) and natural language processing (NLP) \\r\\n - to assist customers in completing complex cognitive tasks **using your data** \\r\\n - typically using conversational \u201cchat\u201d interactions (request-reponse)\\r\\n\\r\\nThe copilot (generative AI application) is deployed in the cloud to expose an interaction endpoint (API) that can be integrated into customer-facing experiences (e.g,, web or mobile apps) for real-world use. For our specific application scenario, the implementation will involve two components:\\r\\n - Contoso Chat (copilot API) as the backend component with the chat AI\\r\\n - Contoso Outdoors (web App) as the frontend component with the chat UI\\r\\n\\r\\n![Azure Copilot](../../static/img/60-days-of-ia/blogs/2024-03-11/copilot-architecture.png) \\r\\n\\r\\nThe figure shows the high-level application architecture for [building generative AI applications using custom code with Azure AI](https://www.youtube.com/watch?v=UbJg7RNLi7E), where the **App** represents the front-end component and the blue box encloses the components of the **Copilot** implementation exposed through the managed online endpoint (API). The copilot experience now involves the following steps:\\r\\n - The user (customer) asks a question from the chat UI (web app)\\r\\n - The web app sends the question to the chat API (copilot endpoint)\\r\\n - The chat API invokes our custom Python code (chat function) which:\\r\\n    - converts the user question (prompt) into a machine-friendly format (vector)\\r\\n    - uses the vectorized prompt to find matches in our custom data (search index)\\r\\n    - combines the user question with custom results for an enhanced prompt\\r\\n    - sends this prompt to the chat model to get the completion (answer)\\r\\n - The chat API now returns the answer as a response to the chat UI request\\r\\n\\r\\nTo build this workflow requires us to complete the following steps:\\r\\n 1. Provision the necessary resources on Azure\\r\\n 1. Create the search index using our custom data\\r\\n 1. Deploy chat and embedding models for use by the chat function\\r\\n 1. Configure connections between chat function and models, resources\\r\\n 1. Write the code to _orchestrate_ the steps for the chat function\\r\\n 1. Deploy our chat function to expose the API endpoint online\\r\\n 1. Integrate the API endpoint with our front-end application for usage\\r\\n\\r\\nFrom an LLM Ops perspective, we also need to consider two additional steps:\\r\\n 1. Evaluation of the chat function using sample data - to assess quality\\r\\n 1. Automation of the workflow steps - for iteration and operationalization\\r\\n\\r\\nThis is a non-trivial set of requirements for building, running, evaluating, and deploying a generative AI application. Thankfully, the Azure AI platform and related ecosystem of tools and services, helps streamline the process for developers - allowing us to focus on our chat function logic and user experience.\\r\\n\\r\\n## 5 | The Week Ahead!\\r\\n\\r\\nIn the upcoming week, we\'ll dive into the implementation details of these processes in the context of a signature reference sample (Contoso Chat) and as quickstart templates that showcase usage with popular frameworks. Here\'s what we\'ll cover:\\r\\n- [**Day 1:**](https://azure.github.io/Cloud-Native/60DaysOfIA/build-contoso-chat-end-to-end) Build the Contoso Chat app on Azure AI (end-to-end reference sample)\\r\\n- [**Day 2:**](https://azure.github.io/Cloud-Native/60DaysOfIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk) Build a Copilot app on Azure AI with the Python SDK (quickstart)\\r\\n- [**Day 3:**](https://azure.github.io/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-prompt-flow) Build a Copilot app on Azure AI with promptflow (framework)\\r\\n- [**Day 4:**](https://azure.github.io/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-langchain) Build a Copilot app on Azure AI with LangChain (framework)\\r\\n- [**Day 5:**](https://azure.github.io/Cloud-Native/60DaysOfIA/deploying-your-copilot-on-azure) Deploy your Copilot app responsibly on Azure AI (advanced topics)"},{"id":"build-contoso-chat-end-to-end","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/build-contoso-chat-end-to-end","source":"@site/blog-60daysofIA/2024-03-11/build-contoso-chat-end-to-end.md","title":"4.1 Build Contoso Chat End-to-End","description":"Building generative AI applications poses new challenges for streamlining end-to-end application development - from prompt engineering, to LLM Ops. In this post we introduce Contoso Chat, a sample application for building a copilot with your data - using the Azure AI platform with prompt flow.","date":"2024-03-11T09:01:00.000Z","formattedDate":"March 11, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":9.93,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-11T09:01","slug":"build-contoso-chat-end-to-end","title":"4.1 Build Contoso Chat End-to-End","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["promptflow","azure","aistudio","generativeai","e2e","llmops"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"Building generative AI applications poses new challenges for streamlining end-to-end application development - from prompt engineering, to LLM Ops. In this post we introduce Contoso Chat, a sample application for building a copilot with your data - using the Azure AI platform with prompt flow.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4. Fuel Your Intelligent Apps with Azure AI","permalink":"/Cloud-Native/60DaysOfIA/fuel-your-intelligent-apps-with-azure-ai"},"nextItem":{"title":"4.2 Build A Copilot Code-First with the Azure AI Python SDK","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/build-contoso-chat-end-to-end\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Building generative AI applications poses new challenges for streamlining end-to-end application development - from prompt engineering, to LLM Ops. In this post we introduce Contoso Chat, a sample application for building a copilot with your data - using the Azure AI platform with prompt flow.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-contoso-chat-end-to-end\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"Building generative AI applications poses new challenges for streamlining end-to-end application development - from prompt engineering, to LLM Ops. In this post we introduce Contoso Chat, a sample application for building a copilot with your data - using the Azure AI platform with prompt flow.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-contoso-chat-end-to-end\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n**Welcome to Day 1\ufe0f\u20e3 of Azure AI week on ##60Days Of IA** \\r\\n\\r\\nIn today\'s post, we\'ll introduce you to the [Contoso Chat](https://aka.ms/aitour/contoso-chat) sample - a comprehensive end-to-end reference sample that walks you through the journey of building the customer support AI application we talked about in our kickoff post yesterday. By the end of this tutorial, you will be able to:\\r\\n - explain how to build a copilot app end-to-end on Azure AI\\r\\n - explain what Retrieval Augmented Generation does for copilot apps\\r\\n - explain what prompt flow is and how it streamlines your workflow\\r\\n - describe the Azure AI platform and Azure AI SDK capabilities\\r\\n\\r\\n_Ready? Let\'s go!_\\r\\n\\r\\n## What You\'ll Learn Today\\r\\n * **Contoso Chat Sample**: Building a copilot with Azure AI and Prompt flow\\r\\n * **Retrieval Augmented Generation**: Design pattern for using custom data\\r\\n * **Prompt flow**: Open-source tooling for orchestrating end-to-end workflow\\r\\n * **Azure resources**: Provisioning Azure for the Contoso Chat AI project\\r\\n * **Hands-on lab**: Step-by-step tutorial to build & deploy Contoso Chat\\r\\n * **Exercise**: [_Fork the sample_](https://aka.ms/aitour/contoso-chat) then work through the hands-on tutorial.\\r\\n * **Resources**: [_Explore this collection_](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs) for samples, docs and training resources.\\r\\n\\r\\n<br/>\\r\\n\\r\\n![Build Contoso Chat - from prompt-engineering to LLM Ops](../../static/img/60-days-of-ia/blogs/2024-03-11/banner.png)\\r\\n\\r\\n---\\r\\n\\r\\n## Contoso Chat Sample\\r\\n\\r\\nThe [Contoso Chat](https://aka.ms/aitour/contoso-chat) sample provides a comprehensive end-to-end reference example for using Azure AI Studio and Prompt flow, to build a copilot application end-to-end. The sample implements a _customer support chat AI_ experience - allowing customers on the Contoso Outdoors website to ask questions about related products and receive relevant responses based on their query and purchase history. The illustrated guide below gives you a high-level overview of the steps involved in building the application - from provisioning Azure resources to deploying and using the chat AI endpoint. To learn more about the application scenario, refer to our [kickoff post](https://azure.github.io/Cloud-Native/60DaysOfIA/fuel-your-intelligent-apps-with-azure-ai) for this week.\\r\\n\\r\\n![Sketchnote](../../static/img/60-days-of-ia/blogs/2024-03-11/contoso-chat-sketchnote.png)\\r\\n\\r\\n## RAG Design Pattern\\r\\n\\r\\nOur first step is to define the application architecture for Contoso Chat. We know we want to have our copilot _grounded in our data_ so that customer queries return responses that reflect the product catalog or customer purchase history.\\r\\n\\r\\nThe challenge is that Large Language Models (LLM) are trained on massive datasets so the default responses may not be _relevant_ or _accurate_ with respect to your data. This is where prompt engineering and design patterns like Retrieval Augmented Generation (RAG) come in. RAG is a design pattern that uses an information _retrieval_ component to get data relevant to the user prompt, then _augments_ the prompt with that context before sending it to the LLM, as illustrated below.\\r\\n\\r\\n![RAG](../../static/img/60-days-of-ia/blogs/2024-03-11/rag.png)\\r\\n\\r\\nWe can break down the workflow into the following steps:\\r\\n 1. User asks a question (\\"User prompt\\")\\r\\n 1. The question is sent to an information retrieval component (\\"AI Search\\")\\r\\n 1. This vectorizes the query (\\"Embedding Model\\")\\r\\n 1. And uses the vector to retrieve relevant results (\\"Product Index\\")\\r\\n 1. Results are used to augment User prompt (\\"Model prompt\\")\\r\\n 1. The enhanced prompt is sent to the LLM (\\"Chat completion\\")\\r\\n\\r\\nThe answer is then returned to the user, who now sees a response that is more relevant to the products in your catalog, and personalized to their purchase history. Note that this basic copilot workflow requires us to deploy two large language models:\\r\\n 1. Text-Embedding model (e.g., `text-embedding-ada-002`) that vectories the user query \\r\\n 1. Text-Generation model (e.g., `gpt-35-turbo`) that generates the final response\\r\\n\\r\\n## Prompt flow Orchestration\\r\\n\\r\\nImplementing the RAG pattern requires a number of interactions between the language model deployments and the data sources used (e.g., search index for products, cusomer database for purchase history), and _coordination_ of intermediate steps before the final response can be delivered. This is where frameworks like Prompt flow, LangChain and Semantic kernel come in.\\r\\n\\r\\n\\r\\nThe Contoso Chat sample makes extensive use of Prompt flow - an [open-source project](https://github.com/microsoft/promptflow) on GitHub, with its own SDK and VS Code extension. Prompt flow provides a comprehensive solution that simplifies the process of prototyping, experimenting, iterating, and deploying your AI applications. It is [recommended for use as a feature within Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/prompt-flow?ocid=buildia24_60days_blogs), making it a natural first choice for building our Contoso Chat application. The figure shows a high-level architecture diagram showcasing the Azure components used with Prompt flow as the orchestration layer.\\r\\n\\r\\n![Prompt Flow Architecture](../../static/img/60-days-of-ia/blogs/2024-03-11/contoso-chat-flow.png)\\r\\n\\r\\nWith Prompt flow, your application is defined as a a directed acyclic graph of _nodes_ (`flow.dag.yaml`) that connect _input_ (prompt) and final _output_ (response) - with intermediate nodes implemented as Python _functions_ (tools) that process or transform the data flowing through them. The Prompt flow extension in VS Code provides a rich _visual editor_ capability as shown below, making it easy to define, debug, run, and test, your application in a local development environment. _This view also helps us see how the RAG pattern is implemented in practice, in our copilot_.\\r\\n\\r\\n![Contoso Chat Flow](../../static/img/60-days-of-ia/blogs/2024-03-11//promptflow-visual.png)\\r\\n\\r\\n## Azure Provisioning\\r\\n\\r\\nThe Contoso Chat sample comes with a [`provision.sh`](https://github.com/Azure-Samples/contoso-chat/blob/main/provision.sh) script that will pre-provision many of the Azure resources for you, for use in the development workflow. To get started with the implementation, follow the instructions in the [README](https://github.com/Azure-Samples/contoso-chat/blob/main/README.md) file in the repo by doing the following:\\r\\n 1. [Fork the sample](https://github.com/Azure-Samples/contoso-chat/fork) to your own GitHub account\\r\\n 2. [Setup development environment](https://github.com/Azure-Samples/contoso-chat/blob/main/README.md#3-development-environment) using GitHub Codespaces\\r\\n 3. [Authenticate](https://github.com/Azure-Samples/contoso-chat/tree/main#41-authenticate-with-azure) with your Azure subscription\\r\\n 4. [Run the Provisioning script](https://github.com/Azure-Samples/contoso-chat/tree/main#42-run-provisioning-script) and verify your setup is complete\\r\\n\\r\\nAt this point, you should have an Azure resource group created for your project with the following resources created for your application. Note that in order to complete this step, you must have a valid Azure subscription that has been given access to the relevant Azure OpenAI services. You must also have available quota for model deployments in the specific regions that we use in the provisioning script.\\r\\n\\r\\n![Provisioning Azure](../../static/img/60-days-of-ia/blogs/2024-03-11//provision-azure.png)\\r\\n\\r\\n## Hands-on Lab\\r\\n\\r\\nYou can now complete the step-by-step tutorial in the [README](https://github.com/Azure-Samples/contoso-chat/blob/main/README.md) to build, evaluate and deploy the application. Let\'s quickly review the main steps involved in the end-to-end workflow.\\r\\n\\r\\n| Stage | Description |\\r\\n|:---|:---|\\r\\n| 1. Build a Copilot. | Get familiar with the application codebase. Check out the `data/` folder to see the data we will be using for customer order (history) and product catalog (index). |\\r\\n| 2. Provision Azure. | Run the `./provision.sh` script or manually provision the required resources. This should setup an Azure AI hub (manage), an Azure AI project (build), an Azure Cosmos DB resource  (customer data) and an Azure AI Search resource (product index). Verify you have a `config.json` created (for local Azure configuration) and an `.env` file (for relevant keys and endpoints for access). |\\r\\n| 3. Add Models & Data. | The provisioning script does the model deployments - but review them now. Make sure you have a chat completion model (gpt-35-turbo), a chat evaluation model (gpt-4) and a text-embeddings model (text-embedding-ada-02). Use the provided notebooks to populate the data in Azure Cosmos DB and Azure AI Search. |\\r\\n| 4. Add Connections | The `devcontainer` configuration ensures you have the Prompt flow extension installed in VS Code, and the `pf` too for command-line, by default. Use the provided notebooks to setup _connection configurations_ from prompt flow to key services (Azure OpenAI, Azure AI Search, Azure Cosmos DB) for use in related notes of the prompt flow graph. Use the `pf` tool to validate these were setup correctly (on VS Code). The provision script may have setup some of these for you in the cloud (Azure) for use in later stages (deploy) - take a minute to verify and correct these as described in README. |\\r\\n| 5. Build Prompt Flow| You are all set to run the prompt flow with your data in Azure.  Explore the components of the prompt flow. Click the _stylized P_ icon in the sidebar to see the Prompt Flow extension activity menu. Open the `contoso-chat/flow.dag.yaml` file in VS Code, then click the _Visual Editor_ option to see the view shown in the earlier screeshot above. Run it to validate it works - then explore the nodes, outputs and code.|\\r\\n| 6. Evaluate Prompt Flow| You can complete a local evaluation by opening the relevant notebook and running it _cell-by-cell_. Review the code in each cell of the notebook, then analyze the output to understand what the relevant metrics are telling you about the quality of the basic flow. The _batch run_ step takes a while and requires Azure connection setup so consider that an optional step. Switch periodically to the _Azure AI Studio_ website view to see how the relevant Azure AI project pages are updated to show the status of various activities or configurations. |\\r\\n| 7. Deploy Prompt Flow| Deploying the prompt flow is a 2-step process. First, we need to upload the flow (code, assets) to Azure AI Studio. Do this using the provided notebook, or you can try to do this manually using the _import_ option in Azure AI Studio under the _Prompt Flow_ section. Once uploaded, you need to select a runtime (\\"automatic\\") and start it to get a compute instance provisioned to execute your flow. Use that to _test_ that your flow was imported successfully. Then click the _Deploy_ option to deploy the flow. This will take a while - refresh the _Deployments_ page to get updates. Once deployment is successful, use the built-in testing feature to try a simple question against the hosted API endpoint. **Congratulations** Your chat AI endpoint is ready for use! |\\r\\n| 8. Summary & Clean up | This was a lot. Note that almost every step of this process can be achieved using code (SDK), command-line (CLI) or UI (Studio website) so explore the documentation. _Note that Azure AI Studio is in preview_ so the features are constantly evolving and things may break unexpectedly - send feedback if so! Finally, don\'t forget to **delete your codespaces and your Azure resources for this lab** to avoid unnecessary charges. And watch the sample repo for updates on workshop content and exercises to extend this further. |\\r\\n| | |\\r\\n\\r\\nCompleting this workshop can take 60-90 minutes based on your level of familiarity with the tools. In the _next_ blog post, we\'ll dive a bit deeper into the process with specific focus on the **Azure AI SDK** to understand _how_ you can implement core steps of the workflow from your Python application. And, in the _final_ post of this week, we\'ll return to the Contoso Chat sample to explore deployment and evaluation in more detail - with additional guidance for ensuring responsible AI usage in your generative AI applications.\\r\\n\\r\\n\\r\\n## Exercise\\r\\n\\r\\nCongratulations! You made it to the end of this whirlwind tour of the Contoso Chat sample. Now it\'s time for you to do the hard work of building this yoursel!! Start by [_forking the sample_](https://aka.ms/aitour/contoso-chat) - then follow the step-by-step instructions in the README.\\r\\n\\r\\n\\r\\n## Resources\\r\\n\\r\\nWe\'ve referenced a number of links and samples in this post. Bookmark the [_Azure AI Studio: Code-First Collection_](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs) and revisit it regularly for an updated list of resources for code-first development of generative AI applications on Azure."},{"id":"build-a-copilot-code-first-with-the-azure-ai-python-sdk","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk","source":"@site/blog-60daysofIA/2024-03-12/build-a-copilot-code-first-with-the-azure-ai-python-sdk.md","title":"4.2 Build A Copilot Code-First with the Azure AI Python SDK","description":"Have a generative AI application you want to build, but don\'t know where to start? In this blog post, we introduce the Azure AI Studio Python Quickstart Sample, explain the end-to-end development workflow, then show you how you can get started customizing it, to explore your own application requirements.","date":"2024-03-12T09:00:00.000Z","formattedDate":"March 12, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":11.26,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-12T09:00","slug":"build-a-copilot-code-first-with-the-azure-ai-python-sdk","title":"4.2 Build A Copilot Code-First with the Azure AI Python SDK","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["azure","azureai","copilot","aisdk"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"Have a generative AI application you want to build, but don\'t know where to start? In this blog post, we introduce the Azure AI Studio Python Quickstart Sample, explain the end-to-end development workflow, then show you how you can get started customizing it, to explore your own application requirements.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4.1 Build Contoso Chat End-to-End","permalink":"/Cloud-Native/60DaysOfIA/build-contoso-chat-end-to-end"},"nextItem":{"title":"4.3 Build a Copilot on Azure Code-First with Prompt Flow","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-prompt-flow"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/build-a-copilot-code-first-with-the-azure-ai-python-sdk\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Have a generative AI application you want to build, but don\'t know where to start? In this blog post, we introduce the Azure AI Studio Python Quickstart Sample, explain the end-to-end development workflow, then show you how you can get started customizing it, to explore your own application requirements.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"Have a generative AI application you want to build, but don\'t know where to start? In this blog post, we introduce the Azure AI Studio Python Quickstart Sample, explain the end-to-end development workflow, then show you how you can get started customizing it, to explore your own application requirements.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n**Welcome to Day 2\ufe0f\u20e3 of the Azure AI week on #60Days Of IA** \\r\\n\\r\\nLet\'s recap what we learned so far. In our _kickoff_ post we set the stage by describing our application scenario (Contoso Chat), the paradigm shift for generative AI apps (LLM Ops) and the unified platform for streamlining development (Azure AI Studio). In the next post we walked through the signature [Contoso Chat](https://aka.ms/aitour/contoso-chat) application sample to understand how we can implement that scenario using Azure AI Studio and Prompt flow - from building the chat function, to evaluating it, deploying it to a hosted endpoint, then testing that API in a chat client.\\r\\n\\r\\nBut what if you want to get started building your own application scenario? Over the next three posts, we\'ll look at _starter samples_ that will get you from ideation (define chat function) to operationalization (deploy chat API) using different tools and frameworks to simplify orchestration.\\r\\n\\r\\nReady? Let\'s go!\\r\\n\\r\\n## What You\'ll Learn Today\\r\\n * What is the copilot architecture?\\r\\n * What is the Azure AI SDK?\\r\\n * What is the Quickstart sample?\\r\\n * How can I customize and extend this for my scenario?\\r\\n * **Challenge:** Fork [this quickstart](https://github.com/Azure-Samples/aistudio-python-quickstart-sample) and build it, then extend it with your data.\\r\\n * **Resources:** Bookmark [this collection](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs) for training & documentation.\\r\\n\\r\\n<br/>\\r\\n\\r\\n\\r\\n![Build a Copilot on Azure Code-First with Azure AI SDK](../../static/img/60-days-of-ia/blogs/2024-03-12/banner.png)\\r\\n\\r\\n---\\r\\n\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\x3c!--  AUTHORS: WRITE BLOG POST CONTENT HERE --\x3e\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\\r\\n\\r\\n## 1 | Learning Objectives\\r\\n\\r\\nThe [copilot ai-sdk quickstart](https://github.com/Azure-Samples/aistudio-python-quickstart-sample) is a Python-based starter sample for a code-first approach to building a copilot experience on the Azure AI platform. Since this is the foundational sample, we\'ll use it to explore some of the details of the implementation and set the stage for you to explore customizing it further for your application requirements.\\r\\n\\r\\nBy the end of this tutorial you should be able to:\\r\\n\\r\\n1. Explain the functional components of the copilot architecture\\r\\n1. Explain the Azure resources required to implement a copilot\\r\\n1. Explain the core functionality provided by the Azure AI SDK\\r\\n1. Build, run, evaluate, and deploy, a basic copilot with Azure AI Studio.\\r\\n1. Explore the Azure AI curated VS Code environment to customize the sample\\r\\n\\r\\nKeep in mind that this is a _quickstart sample_ and is **not meant for production use**. We encourage you to extend and customize the sample to understand the platform capabilities and end-to-end development workflow. Make sure to validate the responses yourself and evaluate its suitability for your application needs in context.\\r\\n\\r\\n## 2| Copilot Architecture\\r\\n\\r\\nLet\'s first revisit the high-level application architecture for our copilot and familiarize ourselves with the core functional components. Our goal is to **build the chat function** component and deploy it to get a hosted **Copilot API** endpoint that we can integrate into front-end applications to provide a conversational chatbot capability grounded in our data.\\r\\n![Copilot architecture](../../static/img/60-days-of-ia/blogs/2024-03-12/copilot-architecture.png)\\r\\n\\r\\nLet\'s review what we will need to implement this architecture:\\r\\n\\r\\n1. **Model Deployments** - we need deployed models for chat and embeddings.\\r\\n1. **Search Index** - we need a search index populated with our product data.\\r\\n1. **Azure Resources** - we need to setup and configure our Azure AI project.\\r\\n1. **App Evaluation** - we need to evaluate copilot quality for responsible AI.\\r\\n1. **App Deployment** - we need to deploy the copilot for a hosted API endpoint.\\r\\n\\r\\nThe [copilot ai-sdk quickstart](https://github.com/Azure-Samples/aistudio-python-quickstart-sample) provides a starter codebase that implements this chat function using the Retrieval Augmented Generation (RAG) pattern with custom data. The implementation makes use of Azure AI Studio and the [Azure AI SDK (Python)](https://aka.ms/aistudio/docs/sdk?ocid=buildia24_60days_blogs) for a code-first approach. Since these technologies are currently in preview, we expect the sample to keep evolving quickly and **recommend following the README-based tutorial there** for the latest instructions.\\r\\n\\r\\n## 3 | Azure AI SDK\\r\\n\\r\\nBefore we dive into the sample, let\'s take a moment to learn about the [Azure AI SDK for Python (preview)](https://learn.microsoft.com/python/api/overview/azure/ai?view=azure-python-preview?ocid=buildia24_60days_blogs). The SDK consists of two packages:\\r\\n - [azure-ai-generative](https://pypi.org/project/azure-ai-generative/) - which provides the functionality needed for building, evaluating and deploying Generative AI applications. This has extra packages (index, evaluate, promptflow) you can use for enhanced local development capabilities - or optionally, remove if unused.\\r\\n - [azure-ai-resources](https://pypi.org/project/azure-ai-resources/) - which provides the functionality for connecting to, and managing, your Azure AI projects and resources. Use this for control plane operations to create and manage data, indexes, models and deployments.\\r\\n\\r\\nThe generative package makes use of the resources package to [create an `AIClient` instance](https://learn.microsoft.com/azure/ai-studio/how-to/sdk-generative-overview#connecting-to-projects?ocid=buildia24_60days_blogs) that can be used for connecting to the Azure AI project resources.\\r\\n\\r\\n```python\\r\\nfrom azure.ai.resources.client import AIClient\\r\\nfrom azure.identity import DefaultAzureCredential\\r\\n\\r\\nai_client = AIClient(\\r\\n    credential=DefaultAzureCredential(),\\r\\n    subscription_id=\'subscription_id\',\\r\\n    resource_group_name=\'resource_group\',\\r\\n    project_name=\'project_name\'\\r\\n)\\r\\n```\\r\\n\\r\\nOnce connected, you can use the generative package to build an index, run a local evaluation, or deploy chat functions and prompt flows, using the imports shown:\\r\\n\\r\\n```python\\r\\nfrom azure.ai.generative.index import build_index\\r\\nfrom azure.ai.generative.evaluate import evaluate\\r\\nfrom azure.ai.resources.entities.deployment import Deployment\\r\\n```\\r\\n\\r\\nTo get started, you will need to [install the SDK](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/sdk-install?ocid=buildia24_60days_blogs) in your local development environment. When you use the quickstart sample with GitHub Codespaces or the Azure AI curated VS Code environment, the SDK comes pre-installed and ready to use. \\r\\n\\r\\n## 4 | Using the Quickstart Sample\\r\\n\\r\\nThe [copilot ai-sdk quickstart](https://github.com/Azure-Samples/aistudio-python-quickstart-sample) provides a comprehensive **README.md** document that describes the step-by-step process for building, running, evaluating, and deploying, a starter copilot sample.\\r\\n\\r\\n### 4.1 | Pre-Requisites\\r\\n\\r\\nTo get started, you will need an active Azure subscription and have access to the Azure OpenAI service to create and deploy the required models for chat completion, chat evaluation and embedddings. You will also need a GitHub account. \\r\\n\\r\\n### 4.2 | Setup Dev Environment\\r\\n\\r\\nThe fastest way to get started exploring the sample is to fork the repo to your personal profile, then launch GitHub Codespaces by navigating to the \\"Codespaces\\" tab under the \\"Code\\" dropdown and creating a new codespace. Active codespaces are listed as shown below. \\r\\n\\r\\n![Launch](../../static/img/60-days-of-ia/blogs/2024-03-12/01-launch-codespaces.png)\\r\\n\\r\\nOnce the Codespace is ready, you will see the Visual Studio Code editor view in your browser tab. Open the **README.md** in the editor, then follow the instructions to complete the tutorial.\\r\\n\\r\\n![Run](../../static/img/60-days-of-ia/blogs/2024-03-12/03-running-codespaces.png)\\r\\n\\r\\n### 4.3 | Initialize Azure AI Resources\\r\\n\\r\\nTo build the copilot, we need to provision the Azure resources listed below. \\r\\n - An [Azure AI hub resource](https://learn.microsoft.com/azure/ai-studio/concepts/ai-resources?ocid=buildia24_60days_blogs) to provide a working _team_ environment and manage resource access, billing and more.\\r\\n - An [Azure AI project resource](https://learn.microsoft.com/azure/ai-studio/how-to/create-projects?ocid=buildia24_60days_blogs) to organize the data, models, and deployments for _an application_, and save its state for future use.\\r\\n - An [Azure AI Search resource](https://learn.microsoft.com/en-us/azure/search/?ocid=buildia24_60days_blogs) to host the search index for our product data.\\r\\n - An [Azure OpenAI resource](https://learn.microsoft.com/azure/openai?ocid=buildia24_60days_blogs) to deploy the models for chat completion, chat evaluation and embeddings.\\r\\n\\r\\nFor now, we will be creating these resources from the [Azure AI Studio UI](https://ai.azure.com?ocid=buildia24_60days_blogs) and [Azure Portal](https://portal.azure.com?ocid=buildia24_60days_blogs) UI in the browser. However, we expect future support for a command-line (CLI) based approach for efficiency and automation. Refer to the sample README for the step-by-step guidance.\\r\\n\\r\\n### 4.4 | Initialize Azure Configuration\\r\\n\\r\\nOnce we\'ve created the Azure resources, we need to configure our Visual Studio Code environment to connect to the cloud.  The repo comes with a `config.sample.json` that shows you the properties that need to be configured. The easiest way to set these is to download the `config.json` file from your Azure AI project resource and place it in the root folder. This information is then used to initialize the`AIClient` in the code, to support interactions with those resources, as explained earlier.\\r\\n\\r\\n```json\\r\\n{\\r\\n    \\"subscription_id\\": \\"your_subscription_id\\",\\r\\n    \\"resource_group\\": \\"your_resource_group\\",\\r\\n    \\"project_name\\": \\"your_project_name\\"\\r\\n}\\r\\n```\\r\\n\\r\\n### 4.5 | Configure Environment Variables\\r\\n\\r\\nThe codebase comes with a sample `.env.sample` file that shows the environment variables you will need to configure, to run the sample. Copy this to `.env` then replace the placeholder strings with the values from the respective Azure resources you provisioned earlier. These environment variables will be used by the Azure AI SDK, to connect to relevant services (by endpoint) with required authentication (key) when implementing the chat function.\\r\\n\\r\\n```bash\\r\\nAZURE_SUBSCRIPTION_ID=replace_with_azure_subscription_id\\r\\nOPENAI_API_TYPE=azure\\r\\nOPENAI_API_KEY=replace_with_openai_key\\r\\nOPENAI_API_BASE=replace_with_openai_base\\r\\nOPENAI_API_VERSION=replace_with_openai_version\\r\\nAZURE_AI_SEARCH_ENDPOINT=replace_with_aisearch_target\\r\\nAZURE_AI_SEARCH_KEY=replace_with_aisearch_key\\r\\nAZURE_AI_SEARCH_INDEX_NAME=replace_with_aisearch_index_name\\r\\nAZURE_OPENAI_CHAT_MODEL=gpt-35-turbo-16k\\r\\nAZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo-16k-0613\\r\\nAZURE_OPENAI_EVALUATION_MODEL=gpt-35-turbo-16k\\r\\nAZURE_OPENAI_EVALUATION_DEPLOYMENT=\\"gpt-35-turbo-16k-0613\\"\\r\\nAZURE_OPENAI_EMBEDDING_MODEL=text-embedding-ada-002\\r\\nAZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-ada-embedding-002-2\\r\\n```\\r\\n\\r\\n### 4.6 | Explore Custom Data\\r\\n\\r\\nAt this point, the base system configuration is done and we just need to populate the data (for the search index) and then run, evaluate, and iterate, the chat function till the response quality is acceptable. Let\'s take a minute to explore the codebase `data/` folder to see the sample data we provide in the starter. We only use the product catalog data (to build the index) in _this_ sample but you can explore usage of the other data types for advanced features or integrations later.\\r\\n\\r\\n| Data Folder | Data Description |\\r\\n| --- | --- |\\r\\n| `data/0-misc` | General information - e.g., customer policies for org. |\\r\\n| `data/1-customer-info`| Customer purchase records  - for 13 fictional customers |\\r\\n| `data/2-chat-history`| Customer conversation history - for a subset of customers |\\r\\n| `data/3-product-info` | Product catalog data - for 20 items in 7 categories |\\r\\n| `data/4-scores` | Test data - for use in evaluations  |\\r\\n| `data/5-prompt-templates` | Example templates - for different contexts |\\r\\n\\r\\n### 4.7 | Explore The Codebase\\r\\n\\r\\nHere are the main files you need to be aware of:\\r\\n\\r\\n| File | Description |\\r\\n| --- | --- |\\r\\n| `src/run.py` | The main entry point for executing core operations |\\r\\n| `src/streaming_utils.py` | Functions for use in interactive conversation |\\r\\n| `src/copilot_aisdk/chat.py` | The chat function implementation. |\\r\\n| `src/system-message.jinja2` | The prompt template with system context (assistant) |\\r\\n\\r\\nYou can now execute the various steps of the end-to-end workflow as follows:\\r\\n- `python src/run.py --build-index` - to build the search index\\r\\n- `python src/run.py --question \\"which tent is the most waterproof?\\"` - to test the chat function\\r\\n- `python src/run.py --evaluate` - to evaluate the chat function\\r\\n- `python src/run.py --deploy` - to deploy the chat function\\r\\n- `python src/run.py --invoke` - to test the deployed chat API endpoint\\r\\n\\r\\nNote that the exact syntax and parameters used in these commands may evolve over time - so check the README in the sample for the latest instructions.\\r\\n\\r\\n### 4.8 | Explore The Chat Function\\r\\n\\r\\nLet\'s briefly talk about the custom code for the copilot, found in the `src/chat.py` file. \\r\\n- The main entry point is the `chat_completion` function that takes a list of messages representing the conversation history.\\r\\n- The `get_documents` function extracts the last message (\\"user question\\") and uses it to retrieve relevant search results using the OpenAI embeddings model and the Azure AI Search client respectively, in a _retrieval augmented generation_ (RAG) pattern.\\r\\n- The `chat_completion` function then takes the returned response and crafts an enhanced prompt (with the system context template, initial user message, and returned search results) and sends the request to the OpenAI chat model for completion.\\r\\n- The returned response is then returned to the user either interactively, or by adding it to the conversation thread (in stream mode).\\r\\n\\r\\n## 5 | Operationalization\\r\\n\\r\\nThe starter sample provides a simple sequence of command-line operations to build, run, evaluate, deploy, and test, the chat function. However, in a real-world scenario, you would integrate the deployed app with a front-end chat UI (like the Contoso Outdoors website) - and use the Azure AI Studio platform to further evaluate the chat function (batch runs), configure content filters (content safety), and monitor usage (performance) for iterative improvement. We\'ll discuss some of these tools and practices in the final post of this series.\\r\\n\\r\\n\\r\\n## 6 | Customizing the Sample\\r\\n\\r\\nThe quickstart sample is a great starting point for exploring your own application scenarios using your own data. Note that the sample is not designed for production use - you will need to do your own validation and evaluation of responses to determine if the chat function is suitable for your application needs. \\r\\n\\r\\nHowever, this is a great time to introduce you to the _cloud development environment_ provided by the [Azure AI curated Visual Studio Code environment](https://learn.microsoft.com/azure/ai-studio/how-to/develop-in-vscode?ocid=buildia24_60days_blogs).This allows you to open your fork of the sample directly from Azure AI Studio, creating a compute instance with a development environment that has the Azure AI SDK and other dependencies pre-installed. Watch this video from the Azure AI Studio team to see how that works - then replicate the process to jumpstart your application exploration journey. \\r\\n\\r\\n<iframe width=\\"600\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/UbJg7RNLi7E\\" title=\\"Build generative AI applications using custom code with Azure AI\\" frameborder=\\"0\\" allowfullscreen></iframe>\\r\\n\\r\\n## Resources\\r\\n\\r\\nWe\'ve referenced a number of links and samples in this post. Bookmark the [_Azure AI Studio: Code-First Collection_](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs) and revisit it regularly for an updated list of resources for code-first development of generative AI applications on Azure."},{"id":"build-a-copilot-on-azure-code-first-with-prompt-flow","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-prompt-flow","source":"@site/blog-60daysofIA/2024-03-13/build-a-copilot-on-azure-code-first-with-prompt-flow.md","title":"4.3 Build a Copilot on Azure Code-First with Prompt Flow","description":"This blog walks you through how you can build, evaluate, and test a custom copilot implementation using Prompt Flow and Azure AI SDK.","date":"2024-03-13T09:00:00.000Z","formattedDate":"March 13, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.105,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-13T09:00","slug":"build-a-copilot-on-azure-code-first-with-prompt-flow","title":"4.3 Build a Copilot on Azure Code-First with Prompt Flow","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["promptflow","azure ai studio","ai cli","RAG"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"This blog walks you through how you can build, evaluate, and test a custom copilot implementation using Prompt Flow and Azure AI SDK.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4.2 Build A Copilot Code-First with the Azure AI Python SDK","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk"},"nextItem":{"title":"4.4 Build a Copilot on Azure Code-First with Langchain","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-langchain"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/build-a-copilot-on-azure-code-first-with-prompt-flow\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"This blog walks you through how you can build, evaluate, and test a custom copilot implementation using Prompt Flow and Azure AI SDK.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-on-azure-code-first-with-prompt-flow\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"This blog walks you through how you can build, evaluate, and test a custom copilot implementation using Prompt Flow and Azure AI SDK.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-on-azure-code-first-with-prompt-flow\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n\\r\\n**Welcome to Day 3\ufe0f\u20e3 of the Azure AI week on #60Days Of IA**\\r\\nIn the previous post, we learned about how to get started with the Azure AI SDK and using it to build a Copilot. In today\'s post we\'ll be covering `building a copilot with custom code and data using PromptFlow`.\\r\\n\\r\\n\\r\\n## What You\'ll Learn Today\\r\\n * Quickstart Sample: Using PromptFlow to build a copilot.\\r\\n * What is \\"Prompt Flow\\" ? \\r\\n * Build the Copilot\\r\\n * Evaluate and Test your flow\\r\\n * Deploy the Copilot\\r\\n * Challenge: [Try this Quickstart sample](https://github.com/Azure-Samples/aistudio-python-promptflow-sample)\\r\\n * Resources: [To learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/prompt-flow?ocid=buildia24_60days_blog)\\r\\n\\r\\n<br/>\\r\\n\\r\\n\x3c!-- FIXME: banner image --\x3e\\r\\n![Build a Copilot on Azure Code-First with Promptflow](../../static/img/60-days-of-ia/blogs/2024-03-13/BIA-3.png)\\r\\n\\r\\n---\\r\\n\\r\\n---\\r\\n\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\x3c!--  AUTHORS: WRITE BLOG POST CONTENT HERE --\x3e\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\\r\\n## 1 | Learning Objectives\\r\\n\\r\\nThis [quickstart tutorial](https://github.com/Azure-Samples/aistudio-python-promptflow-sample) walks you through the steps of creating a copilot app for the enterprise using custom Python code and Prompt Flow to ground the copilot responses in your company data and APIs. The sample is meant to provide a starting point that you can further customize to add additional intelligence or capabilities. By the end of this tutorial, you should be able to\\r\\n1. Describe Prompt Flow and its components\\r\\n1. Build a copilot code-first, using Python and Prompt Flow\\r\\n1. Run the copilot locally, and test it with a question\\r\\n1. Evaluate the copilot locally, and understand metrics\\r\\n1. Deploy the copilot to Azure, and get an endpoint for integrations\\r\\n\\r\\nOnce you\'ve completed the tutorial, try to customize it further for your application requirements, or to explore other platform capabilities. **This is not a production sample** so make sure you validate responses and evaluate the suitability of this sample for use in your application context.\\r\\n\\r\\n## What is Prompt Flow? \\r\\n\\r\\nPrompt Flow is a tool that simplifies the process of building a fully-fledged AI Solution. It helps you prototype, experiment, iterate, test and deploy your AI Applications. Some of the tasks you can achieve with promptflow include:\\r\\n\\r\\n* Create executable flows linking LLM prompts and Python tools through a graph\\r\\n* Debug, share, and iterate through your flows with ease\\r\\n* Create prompt variants and evaluate their performance through testing\\r\\n* Deploy a real-time endpoint that unlocks the power of LLMs for your application.\\r\\n\\r\\n## 2 | Pre-Requisites\\r\\n\\r\\nCompleting the [tutorial](https://github.com/Azure-Samples/aistudio-python-promptflow-sample) requires the following:\\r\\n\\r\\n1. An Azure subscription - [Create one for free](https://azure.microsoft.com/free/cognitive-services?ocid=buildia24_60days_blog)\\r\\n2. Access to Azure OpenAI in the Azure Subscription - [Request access here](https://aka.ms/oai/access?ocid=buildia24_60days_blog)\\r\\n3. Custom data to ground the copilot - [Sample product-info data is provided](https://github.com/Azure-Samples/aistudio-python-promptflow-sample/tree/main/data/3-product-info)\\r\\n4. A GitHub account - [Create one for free](https://github.com/signup)\\r\\n5. Access to GitHub Codespaces - [Free quota should be sufficient](https://docs.github.com/en/billing/managing-billing-for-github-codespaces/about-billing-for-github-codespaces#monthly-included-storage-and-core-hours-for-personal-accounts)\\r\\n\\r\\nThe tutorial uses Azure AI Studio which is currently in public preview.\\r\\n\\r\\n - Read [the documentation](https://learn.microsoft.com/azure/ai-studio/reference/region-support#azure-public-regions?ocid=buildia24_60days_blog) to learn about regional availability of Azure AI Studio (preview)\\r\\n - Read the [Azure AI Studio FAQ](https://learn.microsoft.com/azure/ai-studio/faq?ocid=buildia24_60days_blog?ocid=buildia24_60days_blog) for answers to some commonly-asked questions.\\r\\n\\r\\n## Components of our Prompt Flow\\r\\n\\r\\n* **Flows:** LLM apps essentially involve a series of calls to external services. For instance, our application connects to AI Search, Embeddings Model, and GPT-35-turbo LLM. A flow in PromptFlow are merely...... There are two types of flows:\\r\\n    * **Standard flow:** This is a flow for you to develop you LLM application.\\r\\n    * **Chat flow:** This is similiar to standard flow but the difference is you can define the `chat_history`, `chat_input` and `chat_output` for our flow, enhancing the flow for conversations. \\r\\n    * **Evaluation Flow:** this flow allows you to test and evaluate the quality of your LLM application. It runs on the output of yout flow and computes metrics that can be used to determine whether the flow performs well.\\r\\n\\r\\n![example of our chat flow](../../static/img/60-days-of-ia/blogs/2024-03-13/flow.png)\\r\\n\\r\\n> The flow is defined in `src/copilot_proptflow/flow.dag.yaml`, where you will find all the inputs, nodes and outputs. \\r\\n\\r\\n* **Tools:** these are the fundamental building blocks (nodes) of a flow. The three basic tools are:\\r\\n    * **LLM:** allows you to customize your prompts and leverage LLMs to achieve specific goals\\r\\n    * **Python:** enables you to write custom Python functions to perform various tasks\\r\\n    * **Prompt:** allows you to prepare a prompt as a string for more complex use cases.\\r\\n\\r\\n![LLM Response .jinja2](../../static/img/60-days-of-ia/blogs/2024-03-13/llm_reponse_jinja2.png)\\r\\n\\r\\n*LLM Response .jinja2 file*\\r\\n\\r\\n![customer lookup .py](../../static/img/60-days-of-ia/blogs/2024-03-13/customer_lookup_py.png)\\r\\n\\r\\n*Customer lookup .py file*\\r\\n\\r\\nThe `source code` in `.py` or `.jinja2` defines tools used by the flow.\\r\\n\\r\\n* **Connections** these are for storing information about how you can access external services such as LLM endpoints, API keys, databases, and custom connections e.g. Azure Cosomos DB. You can add your connection as follows using `Prompt flow: Create connection` command:\\r\\n![Prompt flow: Create connection command](../../static/img/60-days-of-ia/blogs/2024-03-13/create_connection.png)\\r\\n\\r\\nOnce created, you can then update your new connection in you flow:\\r\\n\\r\\n![adding new connection to your flow](../../static/img/60-days-of-ia/blogs/2024-03-13/connections_editor.png)\\r\\n\\r\\n* **Variants:** they are used to tune your prompts, for instance utilizing different variants for prompts to evaluate how your model responds to different inputs to get the most suitable combination for your application.\\r\\n* **Running our code:** You can run by clicking run on the visual editor.\\r\\n\\r\\n![screenshot showing the flow output](../../static/img/60-days-of-ia/blogs/2024-03-13/code_output.png)\\r\\n\\r\\n## Test and evaluate our PromptFlow\\r\\n\\r\\nOnce you have built your flow, you need to evaluate the quality of your LLM app response to see if it is performing up to expectations. Some of the metrics you can include in your evaluation are:\\r\\n\\r\\n* Groundedness: how well does the generated responses align with the source data?\\r\\n* Relevance: to what extent is the model\'s generated responses directly related to the questions/input?\\r\\n* Coherence: to what extent does the generated response sound natural, fluent and human like?\\r\\n* Fluency: how grammatically proficient is the output generated by the AI?\\r\\n\\r\\nDuring local evaluation you can explore one metric e.g. groundedness or multiple metrics to evaluate your application. We will evaluate Groundedness by using the evaluation flow as shown below:\\r\\n\\r\\n![Screenshot showing evaluation of the groundeness of our flow](../../static/img/60-days-of-ia/blogs/2024-03-13/groundedness_flow.png)\\r\\n\\r\\n## Exercise\\r\\n\\r\\nWe have covered the building blocks of PromptFlow and how you can ground your data and build your AI Application. Next, once you are satisfied with the performance of your model, you can go ahead and deploy your application. You can do this using either *Azure AI Studio* or *Azure AI Python SDK.*\\r\\n\\r\\n> \ud83d\ude80 **EXERCISE**\\r\\n>\\r\\n> Deploy the PromptFlow either using the Azure AI Studio UI or using the Azure AI SDK\\r\\n\\r\\n## Resources\\r\\n\\r\\n* [AI Studio Prompt Flow Quickstart Sample:](https://github.com/Azure-Samples/aistudio-python-promptflow-sample) Code-first approach to building, running, evaluating, and deploying, a **prompt flow based** Copilot application _using your own data_.\\r\\n* [AI Tour Workshop 4:](https://aka.ms/aitour/contoso-chat/workshop) Comprehensive step-by-step instructions for building the Contoso Chat production RAG app with Prompt Flow and Azure AI Studio\\r\\n* [Azure AI Studio - Documentation:](https://learn.microsoft.com/en-us/azure/ai-studio/?ocid=buildia24_60days_blog) Build cutting-edge, market-ready, responsible applications for your organization with AI"},{"id":"build-a-copilot-on-azure-code-first-with-langchain","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-langchain","source":"@site/blog-60daysofIA/2024-03-14/build-a-copilot-on-azure-code-first-with-prompt-flow.md","title":"4.4 Build a Copilot on Azure Code-First with Langchain","description":"This project uses the AI Search service to create a vector store for a custom department store data.  To enable the user to ask questions our data in a conversational format, we\'ll using Langchain to connect our prompt template with our Azure Open AI LLM.","date":"2024-03-14T09:00:00.000Z","formattedDate":"March 14, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.445,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-14T09:00","slug":"build-a-copilot-on-azure-code-first-with-langchain","title":"4.4 Build a Copilot on Azure Code-First with Langchain","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["AzureAlStudio","copilot","RAG","Langchain"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"This project uses the AI Search service to create a vector store for a custom department store data.  To enable the user to ask questions our data in a conversational format, we\'ll using Langchain to connect our prompt template with our Azure Open AI LLM.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4.3 Build a Copilot on Azure Code-First with Prompt Flow","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-prompt-flow"},"nextItem":{"title":"4.5 Deploying Your Copilot On Azure","permalink":"/Cloud-Native/60DaysOfIA/deploying-your-copilot-on-azure"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/build-a-copilot-on-azure-code-first-with-langchain\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"This project uses the AI Search service to create a vector store for a custom department store data.  To enable the user to ask questions our data in a conversational format, we\'ll using Langchain to connect our prompt template with our Azure Open AI LLM.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-on-azure-code-first-with-langchain\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"This project uses the AI Search service to create a vector store for a custom department store data.  To enable the user to ask questions our data in a conversational format, we\'ll using Langchain to connect our prompt template with our Azure Open AI LLM.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-on-azure-code-first-with-langchain\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n\\r\\n**Welcome to Day 4\ufe0f\u20e3 of the Azure AI week on #60Days Of IA**\\r\\nIn the previous post, we learned about how to get started with the Azure AI SDK and Prompt Flow to build a Copilot. In today\'s post we\'ll be covering `building a copilot with custom code and data using Langchain`.\\r\\n\\r\\n\\r\\n\\r\\n## What You\'ll Learn Today\\r\\n * Quickstart Sample: Using Langchain to build a copilot.\\r\\n * What is \\"Langchain\\" ? \\r\\n * Build the Copilot\\r\\n * Evaluate the Copilot\\r\\n * Deploy the Copilot\\r\\n * **Challenge**: [Try this quickstart sample](https://github.com/Azure-Samples/aistudio-python-langchain-sample/tree/main)\\r\\n * **Resources**: To learn more\\r\\n    - [Azure AI Studio](https://aka.ms/azureaistudio?ocid=buildia24_60days_blogs) - UI to explore, build & manage AI solutions.\\r\\n    - [Azure AI Studio Docs](https://learn.microsoft.com/azure/ai-studio?ocid=buildia24_60days_blogs) - Azure AI Studio documentation.\\r\\n    - [Azure AI Services](https://learn.microsoft.com/azure/ai-services/what-are-ai-services?ocid=buildia24_60days_blogs) - Azure AI Services documentation.\\r\\n    - [Training: Using vector search in Azure Cognitive Search](https://learn.microsoft.com/training/modules/improve-search-results-vector-search?ocid=buildia24_60days_blogs) \\r\\n    - [Tutorial: Deploy a web app for chat on your data](https://learn.microsoft.com/azure/ai-studio/tutorials/deploy-chat-web-app?ocid=buildia24_60days_blogs) \\r\\n\\r\\n<br/>\\r\\n\\r\\n\x3c!-- FIXME: banner image --\x3e\\r\\n![Build a Copilot on Azure Code-First with Langchain](../../static/img/60-days-of-ia/blogs/2024-03-14/BIA-4.png)\\r\\n\\r\\n---\\r\\n\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\x3c!--  AUTHORS: WRITE BLOG POST CONTENT HERE --\x3e\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\\r\\n## 1 | Introduction \\r\\n\\r\\nThis project use the AI Search service to create a vector store for a custom department store data.  We will be using Azure Open AI\'s text-embedding-ada-002 deployment for embedding the data in vectors. The vector representation of your data is stored in [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search?ocid=buildia24_60days_blogs) (formerly known as \\"Azure Cognitive Search\\").  \\r\\n\\r\\nTo enable the user to ask questions our data in a conversational format, we\'ll using Langchain to connect our prompt template with our Azure Open AI LLM.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/rag-pattern.png)\\r\\n\\r\\nWe\'ll use Retrieval Augmented Generation (RAG), a pattern used in AI which uses an LLM to generate answers with your own data. In addition, we\'ll  construct prompt template to provide the scope of our dataset, as well as the context to the submit questions. Lastly, we\'ll maintain the state of the conversation by store the chat history in the prompt.\\r\\n\\r\\n**Custom Data:** The sample data that we\'ll be using in this project is a department store dataset.  The dataset contains a list of customers, orders, products and their descriptions, and their prices.  We\'ll be using this dataset to create a copilot that can answer questions about the products in the dataset.\\r\\n\\r\\n## What is Langchain?\\r\\n\\r\\nLangchain is a framework for developing applications powered by language models. It enables you to connect a language model such as Azure OpenAI to a prompt template including: prompt instructions, chat history, context of the chat conversation, few shot examples, content to ground its response in, etc.).  This helps facilitate end-users to interact with the application to ask questions and language models to generate responses in a conversational format.\\r\\n\\r\\nIn this exercise, we\'ll be using ConversationalRetrievalChain, which is a subclass of langchain that handles chats that are based on retrieving data from documents or vector datasources. We will use it to connect the Azure OpenAI model, retriever, prompt template and chat memory in order to search the AI Search database to retrieve the most relevant response. To activate the instance you need an LLM model (ex. gpt-35-turbo) to retrieve response, the prompt template rules, and chat history. \\r\\n\\r\\n## 2 | Pre-Requisites\\r\\n\\r\\nCompleting the [tutorial](https://github.com/Azure-Samples/aistudio-python-langchain-sample/tree/main) requires the following:\\r\\n\\r\\n1. An Azure subscription - [Create one for free](https://azure.microsoft.com/free/cognitive-services?ocid=buildia24_60days_blogs)\\r\\n2. Access to Azure OpenAI in the Azure Subscription - [Request access here](https://aka.ms/oai/access?ocid=buildia24_60days_blogs)\\r\\n3. Custom data to ground the copilot - [Sample product-info data is provided](https://github.com/Azure-Samples/aistudio-python-langchain-sample/tree/main/data/3-product-info)\\r\\n4. A GitHub account - [Create one for free](https://github.com/signup)\\r\\n5. Access to GitHub Codespaces - [Free quota should be sufficient](https://docs.github.com/en/billing/managing-billing-for-github-codespaces/about-billing-for-github-codespaces#monthly-included-storage-and-core-hours-for-personal-accounts)\\r\\n\\r\\nThe tutorial uses Azure AI Studio which is currently in public preview.\\r\\n\\r\\n - Read [the documentation](https://learn.microsoft.com/azure/ai-studio/reference/region-support#azure-public-regions?ocid=buildia24_60days_blogs) to learn about regional availability of Azure AI Studio (preview)\\r\\n - Read the [Azure AI Studio FAQ](https://learn.microsoft.com/azure/ai-studio/faq?ocid=buildia24_60days_blogs) for answers to some commonly-asked questions.\\r\\n\\r\\n\\r\\n## Open the copilot with Jupiter notebook\\r\\n\\r\\nWe\'ll be using Python SDK to create our copilot for the Contoso outdoor/camping gear AI Chat application.\\r\\n\\r\\nLet\'s begin by opening the `copilot_langchain.ipynb` notebook in the visual studio code (VS code) editor.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/3-select-kernel.png)\\r\\n\\r\\nIn VS code, click on **Select Kernel**. Then under Python Environments, select the **Python 3.10.13** environment you just created\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/3-python-env.png)\\r\\n\\r\\n\\r\\n## Connect to azure resources\\r\\n\\r\\nIn order to access the resources you created in your project in AI studio, we\'ll use the python SDK to authenticate and connect to them.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/conn-to-azure.png)\\r\\n\\r\\n\\r\\nTo find the most relevant results from the vector database, we\'ll be using Langchain\'s retriever to search content from Azure AI Search. \\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/langchain-retriever.png)\\r\\n\\r\\n\\r\\n## Create Prompt Template\\r\\n\\r\\nPrompt engineering is an integral part of providing good user experience and relevant answers.  To achieve that you\'ll need to define a prompt template that includes system prompt rules, restrictions, chat history, input questions and context of conversation.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/prompt-template.png)\\r\\n\\r\\n\\r\\n## Add Langchain to connect Azure OpenAI and Prompt template\\r\\n\\r\\nTo process the search results and apply the system rules, you need it initialize the LLM.  In our case, we\'ll using AzureChatOpenAI class to specify the GPT-35-Turbo model deployment and settings we need for the chat.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/init-azure-openai.png)\\r\\n\\r\\nAll the dialogue that the end-user has with the chat needs retained to maintain the context of the conversation.  That\'s why we are using the ConversationMemoryBuffer class to store the chat history.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/store-convo.png)\\r\\n\\r\\nTo search the AI search database, we\'ll use a subclass of langchain to connect the Azure OpenAI, datasource retriever, prompt template and memory together.  When an instance of the langchain is invoke with an user input prompt, the retriever is used to search your data in AI Search.  The Azure OpenAI uses the prompt rules to process the response back to the user.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/config-langchain.png)\\r\\n\\r\\n\\r\\n## Run the copilot with Jupiter notebook\\r\\n\\r\\nTo run a single question & answer through the sample copilot:\\r\\n\\r\\nClick on **Run All** to run the notebook.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/4-run-all.png)\\r\\n\\r\\n\\r\\n## Validate your copilot by asking a question about your custom data.\\r\\n\\r\\nEnter a question about the outdoor/camping gear and clothing products. For example:\\r\\n\\r\\n```shell\\r\\nWhich of your sleeping bags are polyester?\\r\\n```\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/5-question.png)\\r\\n\\r\\n\\r\\n`The CozyNights Sleeping Bag (item_number: 7) and the MountainDream Sleeping Bag (item_number: 14) are both made of polyester.`\\r\\n\\r\\nTry asking another question. For example:\\r\\n\\r\\n```shell\\r\\nwhich tent is the most waterproof?\\r\\n```\\r\\n\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nIn this exercise you learned how to use Azure AI Search to create and load your data into a vector store.  Next, you learned how to use prompt engineering by constructing *System Prompt* with instructions on how to engage with the user, the scope of the subject area to enforce grounding which prevents the LLM from providing responses that are not relevent to your data.  You should now be able to build AI applications using Lanchain to connect Azure OpenAI, your prompts, chat history, context and retriever of your data source.  You\'ve now gained the knowledge on how to use the Retrieval Augmented Generation (RAG) pattern in AI which uses LLMs to generate answers with your own data."},{"id":"deploying-your-copilot-on-azure","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/deploying-your-copilot-on-azure","source":"@site/blog-60daysofIA/2024-03-15/build-a-copilot-on-azure-code-first-with-prompt-flow.md","title":"4.5 Deploying Your Copilot On Azure","description":"You\'ve build a RAG-based copilot application on Azure AI with Prompt flow. Now it\'s time to deploy it, test it, and integrated it into your chat UI experience. Let\'s dive in!","date":"2024-03-15T09:00:00.000Z","formattedDate":"March 15, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":11.62,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-15T09:00","slug":"deploying-your-copilot-on-azure","title":"4.5 Deploying Your Copilot On Azure","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["azure","ai-studio","automation","accelerator"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"You\'ve build a RAG-based copilot application on Azure AI with Prompt flow. Now it\'s time to deploy it, test it, and integrated it into your chat UI experience. Let\'s dive in!","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4.4 Build a Copilot on Azure Code-First with Langchain","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-langchain"},"nextItem":{"title":"5 The Role of Platform Engineering in Developing Intelligent Apps","permalink":"/Cloud-Native/60DaysOfIA/the-role-of-platform-engineering-in-developing-intelligent-apps"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/deploying-your-copilot-on-azure\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"You\'ve build a RAG-based copilot application on Azure AI with Prompt flow. Now it\'s time to deploy it, test it, and integrated it into your chat UI experience. Let\'s dive in!\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/deploying-your-copilot-on-azure\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"You\'ve build a RAG-based copilot application on Azure AI with Prompt flow. Now it\'s time to deploy it, test it, and integrated it into your chat UI experience. Let\'s dive in!\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/deploying-your-copilot-on-azure\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n\\r\\nWelcome to `Day 5\ufe0f\u20e3` of our journey **Building An AI App End-to-End On Azure!**. It\'s time to wrap-up the week with a look at two key topics - _deployment_ and _responsible AI_! Ready? Let\'s go!\\r\\n\\r\\n## What You\'ll Learn In This Post\\r\\n * Deploying the chat AI (Contoso Chat)\\r\\n * Deploying the chat UI (Contoso Web)\\r\\n * Automate Deployments (CI/CD)\\r\\n * Accelerate Solutions (Enterprise)\\r\\n * Evaluate & Mitigate Harms (Responsible AI)\\r\\n * Exercise: Explore training resources.   \\r\\n * Resources: [**Azure AI Studio Code-First Collection**](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs) \\r\\n\\r\\n<br/>\\r\\n\\r\\n![Deploy with Responsible AI](../../static/img/60-days-of-ia/blogs/2024-03-15/banner.png)\\r\\n\\r\\n---\\r\\n## 1. Revisiting Contoso Chat\\r\\n\\r\\nWe started the week by talking about LLM Ops, and identifying the three core phases of the end-to-end lifecycle for a generative AI application. In the previous posts, we\'ve mostly focused on the first two phases: **ideating** (building & validating a starter app) and **augmenting** (evaluating & iterating app for quality). In this post, we\'ll focus on phase 3: **operationalizing** the application to get it ready for real-world usage.\\r\\n\\r\\n![LLM Ops](../../static/img/60-days-of-ia/blogs/2024-03-15/llm-app-lifecycle.png)\\r\\n\\r\\nFirst, let\'s remind ourselves of the high-level architecture for a copilot application. Our solution has two components:\\r\\n - **Backend**: The _chat AI_ app that is deployed to provide a hosted API endpoint.\\r\\n - **Frontend**: The _chat UI_ app that is deployed to support user interactions with API.\\r\\n\\r\\nLet\'s look at what deployment means in each case:\\r\\n\\r\\n![Copilot Arch](../../static/img/60-days-of-ia/blogs/2024-03-15/copilot-architecture.png)\\r\\n\\r\\n## 2. Deploy your chat AI app\\r\\n\\r\\nIn our example, the chat AI is implemented by the [Contoso Chat](https://aka.ms/aitour/contoso-chat?ocid=buildia24_60days_blogs) sample. Deploying this chat AI solution involves [**three steps**](https://learn.microsoft.com/azure/ai-studio/concepts/deployments-overview?ocid=buildia24_60days_blogs).\\r\\n 1. Deploy the Models\\r\\n 1. Deploy the Flows\\r\\n 1. Deploy the Web App\\r\\n\\r\\nLet\'s look at the first two in this section, starting with **[model deployment](https://learn.microsoft.com/azure/ai-studio/concepts/deployments-overview?ocid=buildia24_60days_blogs#deploying-models)**. Azure AI Studio has a rich model catalog from providers including OpenAI, HuggingFace, Meta and Microsoft Research. Some models can be deployed _as a service_ (with a pay-as-you-go subscription) while others require _hosted, managed infra_ (with a standard Azure subscription). Our chat AI uses three models, all of which used the hosted, managed option.\\r\\n  - `gpt-35-turbo` - for chat completion (core function)\\r\\n  - `text-embedding-ada-002` - for embeddings (query vectorization)\\r\\n  - `gpt-4` - for chat evaluation (responsible AI)\\r\\n\\r\\nNext, let\'s talk about **[deploying flows](https://learn.microsoft.com/azure/ai-studio/how-to/flow-deploy?tabs=azure-studio?ocid=buildia24_60days_blogs)**. There are two kinds of flows we\'ll use in our chat AI - _completion flows_ (that we\'ll use for real-time inference) and _evaluation flows_ (that we\'ll use for quality assessment). Azure AI Studio provides [low-code deployment](https://learn.microsoft.com/azure/ai-studio/how-to/flow-deploy?tabs=azure-studio?ocid=buildia24_60days_blogs#create-an-online-deployment) via the UI and [code-first deployment](https://learn.microsoft.com/azure/ai-studio/how-to/flow-deploy?tabs=python?ocid=buildia24_60days_blogs#create-an-online-deployment) using the Azure AI SDK. In our Contoso Chat sample, we use the SDK to [upload the flow](https://github.com/Azure-Samples/contoso-chat/blob/main/deployment/push_and_deploy_pf.ipynb) to Azure, then deploy it using the UI as shown.\\r\\n  ![Deploy Contoso Chat](../../static/img/60-days-of-ia/blogs/2024-03-15/contoso-chat-deploy.png)\\r\\n\\r\\nFinally, let\'s talk about **[deploying web apps](https://learn.microsoft.com/azure/ai-studio/concepts/deployments-overview?ocid=buildia24_60days_blogs#deploying-web-apps)**. Here, the web app is a _chat UI_ that can invoke requests on the deployed chat AI and validate the functionality in production. There are three options to consider:\\r\\n1. **Built-in Testing UI**. When you deploy your flow via Azure AI Studio, you can visit the deployment details page and navigate to the _Test_ tab, to get a built-in testing sandbox as shown. This provides a quick way to test prompts with each new iteration, in a manual (interactive) way.\\r\\n  ![Deployment Testing](../../static/img/60-days-of-ia/blogs/2024-03-15/contoso-chat-test.png)\\r\\n1. **Deploy as Web App**. Azure AI Studio also provides a _Playground_ where you can deploy models directly (for chat completion) and _add your data (preview)_ (for grounding responses) using Azure AI Search and Blob Storage resources, to customize that chat experience. Then _deploy a new web app_ directly from that interface, to an Azure App Service resource.\\r\\n  ![Deploy as web app](https://learn.microsoft.com/azure/ai-studio/media/tutorials/chat-web-app/deploy-web-app.png?ocid=buildia24_60days_blogs)\\r\\n3. **Dedicated Web App**. This is the option we\'ll explore in the next section.\\r\\n\\r\\n## 3. Deploy your chat UI app\\r\\n\\r\\nThe Contoso Chat sample comes with a dedicated [Contoso Web](https://github.com/Azure-Samples/contoso-web) application that is implemented using the Next.js framework with support for static site generation. This provides a rich \\"Contoso Outdoors\\" website experience for users as shown below.\\r\\n\\r\\n![Contoso Web](../../static/img/60-days-of-ia/blogs/2024-03-15/app-contoso-chat-concept.png)\\r\\n\\r\\nTo use that application, simply [setup the endpoint variables](https://github.com/Azure-Samples/contoso-web?tab=readme-ov-file#setting-up-endpoints) for Contoso Chat and deploy the app to Azure App Service. Alternatively, you can use [this fork of the application](https://github.com/nitya/contoso-web/tree/main-codespaces-swa?tab=readme-ov-file) to explore a version that can be run in GitHub Codespaces (for development) and deployed to Azure Static Web Apps (for production) using GitHub Actions for automated deploys. Once deployed, you can click the _chat_ icon onscreen *bottom right) to see the chat dialog as shown in the screenshot above, and interact with the deployed Contoso chat AI.\\r\\n\\r\\n\\r\\n## 4. Automate your chat AI deployment\\r\\n\\r\\nThe [Contoso Chat sample](https://github.com/Azure-Samples/contoso-chat) is a **constantly-evolving** application sample that is updated regularly to reflect both the changes to Azure AI Studio (preview) and showcase new capabilities for end-to-end development workflows. You can currently explore two additional capabilities implemented in the codebase, to streamline your deployment process further.\\r\\n 1. **Using GitHub Actions**. The sample has instructions to [Deploy with GitHub Actions](https://github.com/Azure-Samples/contoso-chat?tab=readme-ov-file#9-deploy-with-github-actions) instead of the manual Azure AI Studio based deployment step we showed earlier. By setting up the actions workflow, you can automated deployments on every commit or PR, and get a baseline CI/CD pipeline for your chat AI, to build on later.\\r\\n 1. **Using Azure Developer CLI**. The sample was [just azd-enabled](https://github.com/Azure-Samples/contoso-chat/pull/74) recently, making it possible to use the [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/azd-templates?tabs=csharp?ocid=buildia24_60days_blogs) as a unified tool to accelerate the end-to-end process from _provisioning_ the resources to _deploying_ the solution. The [azd template](https://github.com/Azure-Samples/contoso-chat/blob/main/azure.yaml) adds support for _infrastructure-as-code_, allowing your application to have a consistent and repeatable deployment blueprint for all users. You can also [browse the azd template gallery](https://azure.github.io/awesome-azd/?tags=ai&tags=chatgpt) for other _ChatGPT_ style application examples.\\r\\n\\r\\nNote that the Contoso Chat sample is a _demo application_ sample that is designed to showcase the capabilities of Azure AI Studio and Azure AI services. It is not a production-ready application, and should be used primarily as a learning tool and starting point for your own development. \\r\\n\\r\\n---\\r\\n\\r\\n## 5. Enterprise Architecture Options\\r\\n\\r\\nThe objective of this series was to familiarize you with the Azure AI Studio (preview) platform and the capabilities it provides for building generative AI applications. And to give you a sense of how to build, run, test and deploy, your chat AI application for real-world use. But the platform is still in preview (and evolving rapidly). So what are your options if you want to build and deploy generative AI solutions at enterprise scale **today**? How can you design it using a well-architected cloud framework with **cloud-native technologies** like [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?ocid=buildia24_60days_blogs) or [Azure Kubernetes Service](https://learn.microsoft.com/azure/aks/intro-kubernetes?ocid=buildia24_60days_blogs)? \\r\\n\\r\\nHere are some open-source samples and guidance you can explore to start with:\\r\\n1. [ChatGPT + Enterprise data with Azure Open AI and AI Search (Python)](https://github.com/Azure-Samples/azure-search-openai-demo/) - open-source sample that uses Azure App Service, Azure Open AI, Azure AI Search and Azure Blob Storage, for an enterprise-grade solution grounded in your (documents) data.\\r\\n1. [ChatGPT + Enterprise data with Azure Open AI and AI Search (.NET)](https://github.com/Azure-Samples/azure-search-openai-demo-csharp) - open-source sample chat AI for a fictitious company called \\"Contoso Electronics\\" using the application architecture shown below. [This blog post](https://devblogs.microsoft.com/dotnet/transform-business-smart-dotnet-apps-azure-chatgpt/?ocid=buildia24_60days_blogs) provides more details.\\r\\n  ![Chat GPT Enterprise](../../static/img/60-days-of-ia/blogs/2024-03-15/chatgpt-enterprise.png)\\r\\n1. [Chat with your data Solution Accelerator](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator) - uses Azure App Service, Azure Open AI, Azure AI Search and Azure Blob Storage, for an end-to-end baseline RAG sample that goes beyond the [Azure OpenAI Service On Your Data](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/on-your-data-is-now-generally-available-in-azure-openai-service/ba-p/4059514?ocid=buildia24_60days_blogs) feature (GA in Feb 2024).\'\'\\r\\n1. [Built a private ChatGPT style app with enterprise-ready architecture](https://techcommunity.microsoft.com/t5/microsoft-mechanics-blog/build-your-own-private-chatgpt-style-app-with-enterprise-ready/ba-p/4069529?ocid=buildia24_60days_blogs) - a blog post from the Microsoft Mechanics team that uses an [open-source chat UI sample](https://aka.ms/GitHubChatBotUI?ocid=buildia24_60days_blogs) and discusses how to [enhance the chat experience with Azure AI Studio](https://www.youtube.com/watch?v=IKcuod-JFYU&t=252s) and streamline setup by [using Azure Landing Zones](https://www.youtube.com/watch?v=IKcuod-JFYU&t=404s).\\r\\n\\r\\nWe covered a lot today - but there\'s one last thing we should talk about before we wrap up. **Responsible AI**.\\r\\n\\r\\n---\\r\\n\\r\\n## 6. Responsible AI In Practice\\r\\n\\r\\n### 6.1 Principles of Responsible AI\\r\\n\\r\\nBy [one definition](https://learn.microsoft.com/azure/machine-learning/concept-responsible-ai?view=azureml-api-2?ocid=buildia24_60days_blogs), Responsible AI is _approach to developing, assessing, and deploying AI systems in a safe, trustworthy, and ethical way_. The [Responsible AI standard](https://www.microsoft.com/ai/principles-and-approach?ocid=buildia24_60days_blogs) was developed by Microsoft as a framework for building AI systems, using 6 principles to guide our design thinking.\\r\\n\\r\\n![Responsible AI Standard](https://learn.microsoft.com/en-us/azure/machine-learning/media/concept-responsible-ai/concept-responsible-ml.png?view=azureml-api-2?ocid=buildia24_60days_blogs)\\r\\n\\r\\n|Principle|Description|\\r\\n|---|---|\\r\\n|Fairness|How might an AI system allocate opportunities, resources, or information in ways that are fair to the humans who use it?|\\r\\n|Reliability & Safety|How might the system function well for people across different use conditions and contexts, including ones it was not originally intended for?|\\r\\n|Privacy & Security|How might the system be designed to support privacy and security?.|\\r\\n|Inclusiveness|How might the system be designed to be inclusive of people of all abilities?|\\r\\n|Transparency|How might people misunderstand, misuse, or incorrectly estimate the capabilities of the system?|\\r\\n|Accountability|How can we create oversight so that humans can be accountable and in control?|\\r\\n\\r\\n### 6.2 Implications for Generative AI\\r\\n\\r\\nThe [Fundamentals of Responsible Generative AI](https://learn.microsoft.com/en-us/training/modules/responsible-generative-ai/?ocid=buildia24_60days_blogs) describes core guidelines for building  generative AI solutions _responsibly_ as a 4-step process:\\r\\n1. **Identify** potential harms relevant to your solution.\\r\\n1. **Measure** presence of these harms in outputs generated by your solution.\\r\\n1. **Mitigate** harms at multiple layers to minimize impact, and ensure transparent communication about potential risks to users.\\r\\n1. **Operate** your solution responsibly by defining and following a deployment and operational readiness plan.\\r\\n\\r\\n### 6.3 Identify Potential Harms\\r\\n\\r\\nThe first step of the process is to identify potential harms in your application domain using a 4-step process:\\r\\n 1. Identify potential harms (offensive, unethical, fabrication) that may occur in generated content.\\r\\n 1. Assess likelihood of each occurrence, and severity of impact.\\r\\n 1. Test and verify if harms occur, and under what conditions.\\r\\n 1. Document and communicate potential harms to stakeholders.\\r\\n\\r\\n![4 steps](https://learn.microsoft.com/en-us/training/wwl-data-ai/responsible-generative-ai/media/identify-harms.png)\\r\\n\\r\\n\\r\\n### 6.4 Measure Presence of Harms\\r\\n\\r\\n[Evaluation of generative AI applications](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-approach-gen-ai?ocid=buildia24_60days_blogs) is the process of measuring the presence of identified harms in the generated output. Think of it as a 3-step process:\\r\\n\\r\\n 1. Prepare a diverse selection of input prompts that may result in the potential harms documented.\\r\\n 1. Submit prompts to your AI application and retrieve generated output\\r\\n 1. **Evaluate** those responses using pre-defined criteria.\\r\\n\\r\\nAzure AI Studio provides many features and pathways to support evaluation. Start with _manual evaluation_ (small set of inputs, interactive) to ensure coverage and consistency. Then scale to _automated evaluation_ (larger set of inputs, flows) for increased coverage and operationalization.\\r\\n\\r\\n![Evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/media/evaluations/evaluation-monitor-flow.png)\\r\\n\\r\\nBut what _metrics_ can we use to quantify the quality of generated output? Quantifying accuracy is now complicated _because we don\'t have access to a ground truth or deterministic answer_ that can serve as a baseline. Instead, we can use [AI-assisted metrics](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in?ocid=buildia24_60days_blogs) - where we _instruct_ another LLM to score your generated output **for quality and safety** using the guidelines and criteria you provide.\\r\\n- **Quality** is measured using metrics like _relevance, coherence and fluency_.\\r\\n- **Safety** is measured using metrics like _groundedness and content harms_.\\r\\n\\r\\nIn our _Contoso Chat_ app sample, we [show examples](https://github.com/Azure-Samples/contoso-chat/blob/main/eval/evaluate-chat-prompt-flow.ipynb) of local evaluation (with single and multiple metrics) and batch runs (for automated evaluation in the cloud). Here\'s an exmaple of what the output from the local evaluation looks like:\\r\\n\\r\\n![Local Eval](../../static/img/60-days-of-ia/blogs/2024-03-15/eval-local.png)\\r\\n\\r\\n\\r\\n### 6.5 Content Safety for Mitigation\\r\\n\\r\\nOne of the most effective ways to mitigate harmful responses from generative AI models in Azure OpenAI is to use [Content Filtering](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/content-filtering?ocid=buildia24_60days_blogs) powered by the [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview?ocid=buildia24_60days_blogs) service. The service works by running the user input (prompt) and the generated output (completion) through _an ensemble of classification models_ that are trained to detect, and act on, identified caegories of harmful content. \\r\\n\\r\\nAzure AI Studio provides a default content safety filter, and allows you to create custom content filters with more tailored configurations if you opt-in to that capability first. These filters can then be _applied_ to a model or app deployment to ensure that inputs and outputs are gated to meet your content safety requirements.\\r\\n\\r\\n![Create Filter](https://learn.microsoft.com/en-us/azure/ai-studio/media/content-safety/content-filter/configure-threshold.png#lightbox)\\r\\n\\r\\nThe screenshot shows the [different content filtering categories](https://learn.microsoft.com/azure/ai-studio/concepts/content-filtering?ocid=buildia24_60days_blogs#content-filtering-categories-and-configurability) and the level of configurability each provides. This allows us to identify and mitigate different categories of issues (Violence, Hate, Sexual and Self-harm) by **automatically detecting** these in both user prompts (input) and model completions (output). An additional filter (optional) lets you enable filters for more advanced usage scenarios including _jailbreaks, protected content or code_ as [described here](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/content-filtering?ocid=buildia24_60days_blogs#more-filters-for-generative-ai-scenarios).\\r\\n\\r\\n![Content Filter](https://learn.microsoft.com/en-us/azure/ai-studio/media/content-safety/content-filter/additional-models.png)\\r\\n\\r\\nOnce the filters are applied, the deployment can be opened up in the Playground, or using an integrated web app, to validate that the filters work. Check out [this #MSIgnite session](https://ignite.microsoft.com/en-US/sessions/5db0e51a-d8b1-4234-b149-31671a633ffc?source=sessions?ocid=buildia24_60days_blogs) from the Responsible AI team for strategies and examples for responsible AI practices with prompt engineering and retrieval augmented generation patterns in context.\\r\\n\\r\\n## 7. Exercise: \\r\\n\\r\\nWe covered a lot today - and that also brings us to the end of our journey into Azure AI in this series. Want to get hands-on experience with some of these concepts? Here are some suggestions:\\r\\n\\r\\n1. Walk through the [Contoso Chat](https://aka.ms/aitour/contoso-chat?ocid=buildia24_60days_blogs) sample end-to-end, and get familiar with the Azure AI Studio platform and the LLM Ops workflow for generative AI solutions.\\r\\n1. Explore the [Responsible AI Developer Hub](https://aka.ms/rai-hub/website?ocid=buildia24_60days_blogs) and try out the Content Safety and Prompt flow Evaluation workshops to get familiar with the Responsible AI principles and practices for generative AI.\\r\\n\\r\\n## 8. Resources\\r\\n\\r\\nWe covered a lot this week!! But your learning journey with Generative AI development and Azure AI is just beginning. Want to keep going? Here are three resources to help you:\\r\\n\\r\\n1. [Azure AI Studio for Developers](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs)\\r\\n1. [Responsible AI For Developers](https://aka.ms/rai-hub/collection?ocid=buildia24_60days_blogs)\\r\\n1. [Contoso Chat Sample](https://aka.ms/aitour/contoso-chat?ocid=buildia24_60days_blogs)"},{"id":"the-role-of-platform-engineering-in-developing-intelligent-apps","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/the-role-of-platform-engineering-in-developing-intelligent-apps","source":"@site/blog-60daysofIA/2024-03-22/the-role-of-platform-engineering-in-developing-intelligent-apps.md","title":"5 The Role of Platform Engineering in Developing Intelligent Apps","description":"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.","date":"2024-03-22T09:00:00.000Z","formattedDate":"March 22, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":8.935,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-22T09:00","slug":"the-role-of-platform-engineering-in-developing-intelligent-apps","title":"5 The Role of Platform Engineering in Developing Intelligent Apps","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4.5 Deploying Your Copilot On Azure","permalink":"/Cloud-Native/60DaysOfIA/deploying-your-copilot-on-azure"},"nextItem":{"title":"6.1 Creating a Virtual Stylist Chatbot \u2014 Part 1: Analyzing Images with AI","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-1"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/the-role-of-platform-engineering-in-developing-intelligent-apps\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/the-role-of-platform-engineering-in-developing-intelligent-apps\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/the-role-of-platform-engineering-in-developing-intelligent-apps\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![The Role of Platform Engineering in Developing Intelligent Apps](../../static/img/60-days-of-ia/blogs/2024-03-22/5-1.png)\\r\\n\\r\\n## The Role of Platform Engineering in Developing Intelligent Apps\\r\\n\\r\\nIntelligent Apps leverage advanced technologies like machine learning (ML), data analytics, and artificial intelligence (AI) to enhance decision-making, content generation, and user experiences. These apps incorporate AI and ML components to process data, derive insights, and adapt to user behavior to boost efficiency and personalization.\\r\\n\\r\\n[Platform engineering](https://learn.microsoft.com/platform-engineering/what-is-platform-engineering?ocid=buildia24_60days_blogs) is integral to building robust Intelligent Apps. It\u2019s the DevOps-inspired practice of designing, building, and maintaining the infrastructure and systems that underpin software applications. This includes strengthening security, maintaining compliance, controlling costs, and enhancing business value within a governed framework and via an internal developer platform.\\r\\n\\r\\nIntelligent Apps require careful planning and execution as they necessitate complex processes like data management, model optimization and training, algorithm selection, and development. Fortunately, platform engineering helps with these tasks.\\r\\n\\r\\nCoupled with Azure services, platform engineering creates a robust foundation for developing, deploying, and maintaining Intelligent Apps. With the help of platform engineering, you\u2019re free to focus on what you do best as a developer. Instead of worrying about the details of infrastructure, ensuring compliance, or navigating the maze of underlying technologies that support modern apps, you can put your efforts toward designing, implementing, and iterating on their Intelligent Apps.\\r\\n\\r\\nWith platform engineering practices, Azure\u2019s scalable infrastructure streamlines the development lifecycle, enabling rapid prototyping and iteration. Azure services\u2014including Azure\u2019s [AI portfolio](https://azure.microsoft.com/solutions/ai?ocid=buildia24_60days_blogs) and [Azure OpenAI Service](https://azure.microsoft.com/products/ai-services/openai-service?ocid=buildia24_60days_blogs)\u2014enhance app intelligence. Furthermore, [Azure\u2019s DevOps-supporting tools](https://azure.microsoft.com/products?ocid=buildia24_60days_blogs#devops) automate deployment and maintenance tasks, ensuring seamless updates and operational efficiency.\\r\\n\\r\\nLet\u2019s explore how Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps.\\r\\n\\r\\n### How Platform Engineering Paves the Way for Intelligent Apps\\r\\n\\r\\nThe rise of cloud computing and microservices has laid the groundwork for platform engineering techniques.\\r\\n\\r\\nAs applications have become more advanced in their functionality, so too has the underlying ecosystem necessary to deploy, manage, and maintain them. This shift necessitated creating specialized platforms to manage deployment complexities using platform engineering processes. Cloud resources and Infrastructure as Code (IaC) further revolutionized platform engineering by automating infrastructure provisioning and management via code. This streamlined resource deployment, improved scalability, and boosted reliability across environments.\\r\\n\\r\\nWhile these technologies provide substantial benefits to developers, they require a careful strategy to be truly effective. This is where platform engineering truly shines. A well-engineered platform supports developer efforts through the concept of self-service with guardrails. It facilitates the autonomy you need in your workflows while simultaneously offering a set of organizational constraints that free you from unnecessary context switching, helping dissolve silos between teams.\\r\\n\\r\\nOften embracing an \u201ceverything as code\u201d philosophy, platform engineering ensures that everything from infrastructure to deployment is easily managed. The \u201cas code\u201d concept allows for infrastructure, policy, security, and all cloud configurations to be maintained like any other form of code using familiar tools and repositories. This approach offers a powerful method for achieving self-service autonomy, enabling you to make changes in a format you\u2019re already familiar with.\\r\\n\\r\\nPlatform engineering supports the entire lifecycle of Intelligent Apps, from conceptualization to deployment and scaling:\\r\\n\\r\\n* **Conceptualization and design**\u2014Platform engineering teams collaborate with app developers, data scientists, and stakeholders to grasp Intelligent Apps\u2019 requirements. They offer tech insights, aiding in tech selection and architecture design for scalability, reliability, and performance. Platform engineers help design data architecture, encompassing pipelines, storage, and processing frameworks. They also provide start-right templates that ensure you can start the development process correctly, adhering to policy, following best practices, and utilizing the most relevant technologies.\\r\\n* **Development and testing**\u2014Platform engineers configure development environments, giving you tools for efficient coding. They also establish continuous integration and continuous delivery (CI/CD) pipelines for automated processes and offer testing infrastructures. Tools like [Azure Pipelines](https://azure.microsoft.com/products/devops/pipelines/?ocid=buildia24_60days_blogs) and [Azure Test Plans](https://azure.microsoft.com/products/devops/test-plans/?ocid=buildia24_60days_blogs) help. By putting the right infrastructure in place\u2014and empowering you with enough self-service autonomy to utilize this infrastructure\u2014platform engineering allows for more focused and consistent development and testing cycles.\\r\\n* **Model training and optimization**\u2014Using technologies like distributed computing and GPU acceleration, platform engineers work with data scientists to establish scalable infrastructure for model training. They enhance training efficiency by adjusting hardware setups, refining data pipelines, and employing parallel processing methods. Additionally, they integrate model monitoring tools for performance tracking and retraining.\\r\\n* **Deployment and scaling**\u2014Platform engineers create automated deployment pipelines and infrastructure templates for deploying Intelligent Apps across various environments. They guarantee reliable, scalable processes, monitor performance, and use tools like Kubernetes for containerized workloads, ensuring scalability, resilience, and portability.\\r\\n* **Monitoring and maintenance**\u2014Platform engineers deploy monitoring and observability tools, like [Azure Monitor](https://azure.microsoft.com/products/monitor/?ocid=buildia24_60days_blogs), for real-time tracking of Intelligent Apps\u2019 health, performance, and usage. They set up alert systems and automated responses, ensuring proactive issue detection and minimizing downtime. Regular performance tuning, capacity planning, and security audits optimize infrastructure efficiency.\\r\\n\\r\\nPlatform engineering ensures scalability, improves reliability through optimized performance, and fosters efficiency by automating workflows\u2014aspects that make for more powerful, performant Intelligent Apps.\\r\\n\\r\\n:::info\\r\\nExplore the [Platform Engineering Guide](https://learn.microsoft.com/platform-engineering/?ocid=buildia24_60days_blogs) to learn how platform engineering teams can use building blocks from Microsoft and other vendors to create deeply personalized, optimized, and secure developer experiences.\\r\\n:::\\r\\n\\r\\n#### Platform Engineering in Action\\r\\n\\r\\nTo illustrate the impact of platform engineering on developing Intelligent Apps, let\u2019s compare the journey of developing one such app using a traditional software development methodology versus using an Azure-supported platform engineering approach.\\r\\n\\r\\n##### The Initial Phase\\r\\n\\r\\nIn the traditional software development approach, the initial phase typically involves requirements gathering followed by siloed development stages. Comparatively, a platform engineering approach involves embracing integrated planning and development stages. Teams collaborate from the beginning, leveraging Azure\u2019s cloud capabilities for streamlined workflows. Azure services, such as [Azure DevOps](https://azure.microsoft.com/products/devops/?ocid=buildia24_60days_blogs), facilitate seamless coordination between development, operations, and product teams, ensuring alignment with business objectives.\\r\\n\\r\\nFrom the outset, platform engineering supports your development efforts with a templated approach to creating new apps or services. Platform engineers often create start-right templates based on best practices, industry standards, or organizational guidelines to ensure consistency, efficiency, and quality from the project\u2019s conception. These templates may encompass IaC templates, service catalogs, or repositories containing pre-built components and libraries you can use to accelerate development.\\r\\n\\r\\nPlatform engineers help to define the core policies that govern resource provisioning and configuration. These policies might include restrictions on resource types, sizes, or regions, as well as rules for security, compliance, and cost management. With these guardrails in place, you can work without constantly looking over your shoulder to ensure you\u2019re adhering to policy and best practices.\\r\\n\\r\\n##### Development and Deployment Phases\\r\\n\\r\\nTraditionally, development progresses linearly with limited flexibility for adaptation. Deployment may encounter challenges due to disparate environments, leading to inconsistencies. Moreover, in traditional methods, siloing commonly occurs across design, development, testing, and deployment stages, hampering communication, slowing progress, creating inefficiencies, and hindering collaboration\u2014ultimately resulting in disjointed outcomes.\\r\\n\\r\\nBy empowering you with self-service platforms and powerful automation, a well-engineered platform expedites your development efforts. Self-service interfaces simplify provisioning infrastructure resources such as virtual machines, containers, databases, storage, and networking for use in these phases. You can request and provision the resources you need on-demand without waiting for manual intervention from infrastructure teams.\\r\\n\\r\\nLeveraging Azure services like [Azure Kubernetes Service](https://azure.microsoft.com/products/kubernetes-service?ocid=buildia24_60days_blogs) (AKS) and [Azure App Service](https://azure.microsoft.com/products/app-service?ocid=buildia24_60days_blogs) makes deployment automated and scalable, ensuring consistent performance across environments.\\r\\n\\r\\nAdditionally, Azure\u2019s AI-specific tools\u2014including [Azure Machine Learning](https://azure.microsoft.com/products/machine-learning/?ocid=buildia24_60days_blogs), [Data Science Virtual Machines](https://azure.microsoft.com/products/virtual-machines/data-science-virtual-machines/?ocid=buildia24_60days_blogs#overview) (DSVMs), and [Azure AI Language](https://azure.microsoft.com/products/ai-services/ai-language/?ocid=buildia24_60days_blogs)\u2014make developing and deploying robust Intelligent Apps straightforward.\\r\\n\\r\\n##### The Maintenance Phase\\r\\n\\r\\nDuring the maintenance phase, the benefits of platform engineering shine even brighter. Traditional methods often struggle with managing ongoing updates and addressing user feedback promptly. Moreover, post-deployment maintenance using traditional methodology requires dedicated resources for ongoing support and updates.\\r\\n\\r\\nPlatform engineering selects, deploys, and configures infrastructure monitoring tools that provide visibility into the underlying infrastructure components\u2019 health and performance. By letting you access these metrics directly, platform engineering arms you with the information you need to make informed decisions for maintenance and optimization.\\r\\n\\r\\nPlatform engineering enables teams to iterate rapidly based on real-time insights, leveraging Azure\u2019s analytics and monitoring tools to gather actionable data. For instance, [Azure Monitor](https://learn.microsoft.com/azure/azure-monitor/overview?ocid=buildia24_60days_blogs) and [Application Insights](https://learn.microsoft.com/azure/azure-monitor/app/app-insights-overview?ocid=buildia24_60days_blogs) enable proactive monitoring and efficient troubleshooting, minimizing downtime and optimizing performance. Additionally, Azure DevOps facilitates iterative improvements through feature flags and A/B testing, so teams can gather feedback and iterate quickly.\\r\\n\\r\\nFurthermore, Azure\u2019s AI-powered tools\u2014including [AI Anomaly Detector](https://azure.microsoft.com/products/ai-services/ai-anomaly-detector/?ocid=buildia24_60days_blogs) and [Azure AI Metrics Advisor](https://azure.microsoft.com/products/ai-services/ai-metrics-advisor/?ocid=buildia24_60days_blogs)\u2014support analytics and anomaly detection, allowing teams to address issues before they impact users.\\r\\n\\r\\n#### The Benefits of Platform Engineering\\r\\n\\r\\nLet\u2019s review some of the benefits of using platform engineering over traditional development methods for your Intelligent Apps:\\r\\n\\r\\n* **Reduced time to market**\u2014Platform engineering accelerates software development through reusable infrastructure components, automation tools, and standardized workflows. This contrasts with traditional methods, which prolong development cycles due to manual processes and lack of automation.\\r\\n* **Improved app quality**\u2014Platform engineering ensures consistency, repeatability, and reliability with standardized configurations, security policies, and automated testing. With traditional approaches, quality may suffer due to manual testing, ad-hoc configurations, and inconsistent environments.\\r\\n* **Scalability and resilience**\u2014Intelligent Apps designed with platform engineering in mind have resilient infrastructure that supports seamless scalability thanks to automated scaling and fault-tolerant architecture. Traditional, manual development methods can\u2019t compete.\\r\\n* **Enhanced ability to iterate based on user feedback**\u2014Traditional methods face the constraints of manual processes and lengthy deployment cycles. Comparatively, platform engineering facilitates rapid iteration and experimentation with flexible platform infrastructure.\\r\\n* **Operational efficiency**\u2014Platform engineering improves efficiency through automation, standardized processes, and centralized management. This contrasts with traditional methods, where operational tasks are more manual, leading to inefficiencies and increased costs.\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs)**\u202fto compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n### Conclusion\\r\\n\\r\\nAzure services and platform engineering have revolutionized the landscape of Intelligent Apps development, offering organizations unprecedented scalability, flexibility, and efficiency. And with Azure\u2019s robust suite of tools, you can deliver intelligent solutions that drive growth and enhance customer experiences.\\r\\n\\r\\nAs we look to the future, the potential of Intelligent Apps to provide substantial business value only continues to grow, promising even greater insights, automation, and competitive advantages. To learn more about the transformative power of Azure, join us at [Microsoft Build](https://build.microsoft.com/en-US/home?ocid=buildia24_60days_blogs)."},{"id":"creating-a-virtual-stylist-chatbot-part-1","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-1","source":"@site/blog-60daysofIA/2024-03-26/creating-a-virtual-stylist-chatbot-part-1.md","title":"6.1 Creating a Virtual Stylist Chatbot \u2014 Part 1: Analyzing Images with AI","description":"In this three-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this first part, you\u2019ll analyze clothing images using AI to generate a text description of the piece, focusing on the clothing\u2019s characteristics. ","date":"2024-03-26T09:00:00.000Z","formattedDate":"March 26, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":13.335,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-26T09:00","slug":"creating-a-virtual-stylist-chatbot-part-1","title":"6.1 Creating a Virtual Stylist Chatbot \u2014 Part 1: Analyzing Images with AI","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this three-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this first part, you\u2019ll analyze clothing images using AI to generate a text description of the piece, focusing on the clothing\u2019s characteristics. ","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"5 The Role of Platform Engineering in Developing Intelligent Apps","permalink":"/Cloud-Native/60DaysOfIA/the-role-of-platform-engineering-in-developing-intelligent-apps"},"nextItem":{"title":"6.2 Creating a Virtual Stylist Chatbot \u2014 Part 2: Adding a Chatbot Interface","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-2"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/creating-a-virtual-stylist-chatbot-part-1\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this three-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this first part, you\u2019ll analyze clothing images using AI to generate a text description of the piece, focusing on the clothing\u2019s characteristics. \\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-1\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this three-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this first part, you\u2019ll analyze clothing images using AI to generate a text description of the piece, focusing on the clothing\u2019s characteristics. \\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-1\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![virtual stylist chatbot that uses AI to analyze images and suggest clothing items](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-1.jpeg)\\r\\n\\r\\n## Creating a Virtual Stylist Chatbot \u2014 Part 1: Analyzing Images with AI\\r\\n\\r\\nEver wished you had a personal fashion consultant who could help you find the ideal outfit for any occasion? What if you could use artificial intelligence (AI) to create a virtual stylist chatbot that could analyze clothing in images and suggest the perfect match from a database of clothing options.\\r\\n\\r\\nThis assistant is an example of an intelligent app\u2014an application that leverages AI to enhance and personalize its user experience.\\r\\n\\r\\nIn this three-part series, you\u2019ll learn how to build your own AI stylist app. When you\u2019re done, you\u2019ll have an app that can understand the contents of user-uploaded images and recommends similar items from a fashion image dataset.\\r\\n\\r\\nThe first article of this series demonstrates how to create the app\u2019s core logic. It analyzes the clothing styles in the image and finds the closest match in the dataset using Azure AI Search, Azure OpenAI Service, and Azure Functions. In the later parts of the series, you\u2019ll add a chatbot interface to the app.\\r\\n\\r\\nLet\u2019s get started!\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nBefore you start, ensure you have:\\r\\n\\r\\n* Python 3.10 or later\\r\\n* An Azure subscription with access to [Azure OpenAI Service](https://azure.microsoft.com/products/ai-services/openai-service?ocid=buildia24_60days_blogs)\\r\\n* [Azure command-line interface (CLI)](https://learn.microsoft.com/cli/azure/?ocid=buildia24_60days_blogs) installed\\r\\n* [Azure Functions Core Tools](https://github.com/Azure/azure-functions-core-tools) installed\\r\\n* An Azure OpenAI Service resource with a GPT-4 Vision model deployed. Read the [resource deployment guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal&ocid=buildia24_60days_blogs) if you haven\u2019t yet deployed a model. Note that [GPT-4 Vision](https://learn.microsoft.com/azure/ai-services/openai/concepts/models?ocid=buildia24_60days_blogs#model-summary-table-and-region-availability) is only available in the Sweden Central and West US regions, so be sure to select either of those two.\\r\\n* The deployment name, endpoint, and API key for your OpenAI Service. See the \u201cRetrieve key and endpoint\u201d section in the [Azure OpenAI Service docs](https://learn.microsoft.com/azure/ai-services/openai/dall-e-quickstart?pivots=programming-language-python?ocid=buildia24_60days_blogs#retrieve-key-and-endpoint) for details on finding your model\u2019s endpoint URL and API key.\\r\\n* The [Fashion Product Images dataset](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small) from Kaggle. Download and unzip the dataset. You\u2019ll only need the CSV file in part 1, but keep all the images because you\u2019ll use them later in the series.\\r\\n* Familiarity with Python\\r\\n* [Flask installed](https://flask.palletsprojects.com/en/3.0.x/installation/)\\r\\n* [Visual Studio Code](https://code.visualstudio.com/Download) or another code editor of your choice\\r\\n\\r\\nFor a preview, refer to the complete code for [part 1 available on GitHub](https://github.com/rogerwinter/Microsoft-Creating-a-Virtual-Stylist-Chatbot/tree/main/stylist-backend).\\r\\n\\r\\n### Analyzing Clothing Styles with AI\\r\\n\\r\\nWith the prerequisites in place, it\u2019s time to create an app from scratch. It will use Azure AI Search, Azure Functions (in Python), and Azure OpenAI Service to do the following:\\r\\n\\r\\n* Accept an image uploaded from a web interface. It should be an image of a clothing item or a person wearing one or more pieces of clothing.\\r\\n* Analyze that image using Azure OpenAI GPT-4 Turbo with Vision to generate a text description of the piece. Focus on describing the characteristics of the clothing.\\r\\n* Use the text description of the clothing\u2019s characteristics to find its closest matches in the clothing dataset.\\r\\n* Return a suggestion from the dataset of which clothing items are the best matches.\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n#### Create the Search Index and Upload the Dataset\\r\\n\\r\\nFirst, you must create a search index and upload the dataset that contains the clothing options. You\u2019ll use Azure AI Search, which can automatically ingest and parse the CSV data supplied with the fashion image dataset.\\r\\n\\r\\nBegin by uploading the CSV data included in the [fashion dataset](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small) into Azure Blob Storage. Navigate to the Storage Accounts page to get started. To find it quickly, enter its name in the Azure Portal\u2019s search bar:\\r\\n\\r\\n![image of storage accounts search in Azure](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-2.png)\\r\\n\\r\\nWhen the page loads, choose an existing storage account if you already have one. If not, create a new one. Click the storage account\u2019s name to load its dashboard page. Then, click **Upload** to upload a new file:\\r\\n\\r\\n![image of upload storage accounts search in Azure](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-3.jpeg)\\r\\n\\r\\nNext, select the `styles.csv` file from the fashion dataset downloaded from Kaggle.\\r\\n\\r\\n![image of file upload](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-4.png)\\r\\n\\r\\nIf you have an existing storage container you\u2019d like to use, select it from the dropdown menu. Otherwise, click the link to create a new one. Either way, ensure the container is empty before proceeding. The `styles.csv` file you upload should be the only file in the container.\\r\\n\\r\\nNow, you\u2019re ready to create the AI Search service. Look it up using the Azure Portal search box:\\r\\n\\r\\n![image of searching for AI Search service in Azure Portal](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-5.png)\\r\\n\\r\\nWhen the AI Search page loads, click **+ Create** to create a new AI Search instance.\\r\\n\\r\\n![image of create in AI services](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-6.png)\\r\\n\\r\\nSelect the subscription and resource group you\u2019d like to use to create the search service. Then, enter a unique name of your choice \u2014 this demonstration uses \u201cstylist-search-service.\u201d\\r\\n\\r\\n![image of fields for creating a new search service in AI services](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-7.png)\\r\\n\\r\\nUse the defaults for all remaining settings and click **Create** to create the search service. This may take a few minutes. The Azure Portal will let you know when the service is ready.\\r\\n\\r\\nNow, it\u2019s time to index the data in the `styles.csv` file you uploaded to Blob Storage earlier. From the main page of your new search index, click **Import data**.\\r\\n\\r\\n![image of import data option for indexing data](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-8.jpeg)\\r\\n\\r\\nIn the first data import screen, select **Azure Blob Storage** as the data source and enter \u201cfashion-images\u201d as the data source name. Choose **Delimited text** as the parsing mode, and enter a comma as the delimiter character. For the connection string, click **Choose an existing connection** and select the storage container where you uploaded `styles.csv`. Delete the forward slash in the Blob folder input box. Azure will auto-populate the connection string.\\r\\n\\r\\n![image of fields for importing data](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-9.png)\\r\\n\\r\\nClick **Next** until Azure prompts you to customize the target index, and then update the field settings as follows:\\r\\n\\r\\n![image of field settings for importing data](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-10.png)\\r\\n\\r\\nClick **Next**. On the final screen, enter a name for the indexer and click **Submit**.\\r\\n\\r\\n![image of final screen when importing data](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-11.png)\\r\\n\\r\\nAzure will create a search index and then run the ingester to import the data. It should finish in under two minutes. When it does, you\u2019re done with search index creation.\\r\\n\\r\\n:::info\\r\\nRegister for the new learning series on **[Intelligent Apps with Serverless on Azure](https://aka.ms/serverless-learn-live?ocid=buildia24_60days_blogs)**. Join the community along with MVPs, and the Azure Product Group on how to leverage AI with Serverless on Azure technologies \u2013Azure Functions and Azure Container Apps \u2013 to build intelligent applications.\\r\\n:::\\r\\n\\r\\n#### Create the Azure Function\\r\\n\\r\\nThe next step is to create the Azure Function that will perform image analysis, matching logic, and recommendation generation. You\u2019ll use Python as the programming language and Flask as the web framework.\\r\\n\\r\\nTo create and deploy the Azure Functions app, use the Azure Functions CLI. Open a terminal and create a new directory to store your app. Then, run:\\r\\n\\r\\n```\\r\\nfunc init --python\\r\\n```\\r\\n\\r\\nThe app generator will run. Open the directory in Visual Studio Code or your text editor of choice. You should see several files:\\r\\n\\r\\n![the directory in Visual Studio Code](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-12.png)\\r\\n\\r\\nOpen `requirements.txt` and add the following:\\r\\n\\r\\n```\\r\\nazure-functions\\r\\nrequests\\r\\nazure-search-documents\\r\\n```\\r\\n\\r\\nThis change ensures Azure will install all the dependencies the function needs before trying to run it.\\r\\n\\r\\nNext, open `function_app.py` and replace its contents with the following:\\r\\n\\r\\n```\\r\\nimport base64\\r\\nimport os\\r\\nimport json\\r\\nimport requests\\r\\nimport azure.functions as func\\r\\nfrom azure.search.documents import SearchClient\\r\\nfrom azure.core.credentials import AzureKeyCredential\\r\\n\\r\\napp = func.FunctionApp()\\r\\n\\r\\n# Get the environment variables\\r\\nOPENAI_API_KEY = os.environ[\'OPENAI_API_KEY\']\\r\\nOPENAI_ENDPOINT = os.environ[\'OPENAI_ENDPOINT\']\\r\\nOPENAI_DEPLOYMENT_NAME = os.environ[\'OPENAI_DEPLOYMENT_NAME\']\\r\\nSEARCH_API_KEY = os.environ[\'SEARCH_API_KEY\']\\r\\nSEARCH_ENDPOINT = os.environ[\'SEARCH_ENDPOINT\']\\r\\nSEARCH_INDEX_NAME = os.environ[\'SEARCH_INDEX_NAME\']\\r\\n\\r\\n# Initialize the Azure OpenAI headers\\r\\nopenai_headers = {\\r\\n    \'Authorization\': \'Bearer {}\'.format(OPENAI_API_KEY),\\r\\n    \'Content-Type\': \'application/json\'\\r\\n}\\r\\n\\r\\n# Initialize the Azure Search client\\r\\nsearch_credentials = AzureKeyCredential(SEARCH_API_KEY)\\r\\nsearch_client = SearchClient(SEARCH_ENDPOINT, SEARCH_INDEX_NAME, search_credentials)\\r\\n\\r\\n@app.route(route=\\"stylist\\", methods=[\\"post\\"], auth_level=func.AuthLevel.FUNCTION)\\r\\ndef stylist(req: func.HttpRequest) -> func.HttpResponse:\\r\\n    # get image from request and convert to a base64 string\\r\\n    image = req.files[\\"image\\"]\\r\\n    image_bytes = image.read()\\r\\n    image_base64 = base64.b64encode(image_bytes).decode(\\"utf-8\\")\\r\\n\\r\\n    # Generate a text description from the image using Azure OpenAI\\r\\n    base_url = f\\"{OPENAI_ENDPOINT}openai/deployments/{OPENAI_DEPLOYMENT_NAME}\\"\\r\\n    endpoint = f\\"{base_url}/chat/completions?api-version=2023-12-01-preview\\"\\r\\n    data = {\\r\\n        \\"messages\\": [\\r\\n            { \\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\" },\\r\\n            { \\"role\\": \\"user\\", \\"content\\": [\\r\\n                {\\r\\n                    \\"type\\": \\"text\\",\\r\\n                    \\"text\\": \\"Describe the main fashion item in this picture. Make sure you include the type of item (e.g., Shirt, T-Shirt, Shorts, Pants, Dress, Purse, Clutch), the color of the item, and \'Men\' or \'Women\' if the fashion item appears to be specific to either of those genders.\\"\\r\\n                },\\r\\n                {\\r\\n                    \\"type\\": \\"image_url\\",\\r\\n                    \\"image_url\\": {\\r\\n                        \\"url\\": image_base64\\r\\n                    }\\r\\n                }\\r\\n            ] }\\r\\n        ],\\r\\n        \\"max_tokens\\": 2000\\r\\n    }\\r\\n\\r\\n    response = requests.post(endpoint, headers=openai_headers, data=json.dumps(data))\\r\\n    result = response.json()\\r\\n    image_description = result[\'text\']\\r\\n\\r\\n    # Find the closest match from the search index using Azure OpenAI\\r\\n    search_result = search_client.search(\\r\\n        search_text=image_description,\\r\\n        select=[\\"id\\", \\"productDisplayName\\"],\\r\\n        top=1\\r\\n    )\\r\\n    match_id = search_result[\\"id\\"]\\r\\n    match_name = search_result[\\"productDisplayName\\"]\\r\\n\\r\\n    # Generate a natural language recommendation based on the match result using Azure OpenAI\\r\\n    data = {\\r\\n        \\"messages\\": [\\r\\n            { \\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\" },\\r\\n            { \\"role\\": \\"user\\", \\"content\\": [\\r\\n                {\\r\\n                    \\"type\\": \\"text\\",\\r\\n                    \\"text\\": f\\"Please generate a natural language recommendation based on the matching item: {match_id}, {match_name}. For example: The best match for your clothing item is: Peter England Men Party Blue Jeans. This is a pair of jeans for men in blue color, suitable for casual occasions. You can pair it with a shirt or a t-shirt of your choice.\\"\\r\\n                }\\r\\n            ] }\\r\\n        ],\\r\\n        \\"max_tokens\\": 2000\\r\\n    }\\r\\n    response = requests.post(endpoint, headers=openai_headers, data=json.dumps(data))\\r\\n    result = response.json()\\r\\n    recommendation = result[\'text\']\\r\\n\\r\\n    # Return the recommendation as a JSON response\\r\\n    return func.HttpResponse(json.dumps({\\r\\n        \'image_id\': match_id,\\r\\n        \'recommendation\': recommendation\\r\\n    })) \\r\\n```\\r\\n\\r\\nLet\u2019s break down what\u2019s happening step by step.\\r\\n\\r\\nFirst, you set up the function app and Azure clients. This code:\\r\\n\\r\\n* Initializes an Azure Function with an HTTP trigger\\r\\n* Retrieves necessary API keys and endpoints from environment variables for OpenAI and Azure Search services\\r\\n* Sets up headers for interacting with Azure OpenAI Service and a client for using Azure Search\\r\\n\\r\\nThen, you define the `process_image` function. This function:\\r\\n\\r\\n* Is the application\u2019s code and executes on a specific request to the Flask app\\r\\n* Receives an image as part of the request\\r\\n\\r\\nNext, you generate text descriptions with Azure OpenAI. This code:\\r\\n\\r\\n* Constructs a request to Azure OpenAI Service\u2019s chat API to generate a description of the main fashion item in the image. The request includes the image and a prompt to describe the fashion item, including its type, color, and gender specificity.\\r\\n* Sends the request and extracts the generated description from the response\\r\\n\\r\\nAfter, you search for a matching product. This code:\\r\\n\\r\\n* Uses the Azure Search client to search for a product that matches the description generated by OpenAI. The search query uses the textual description and selects specific fields (`id`, `productDisplayName`) from the search index.\\r\\n* Extracts the ID and display name of the closest matching product from the search results\\r\\n\\r\\nThen, you generate a natural language recommendation. This code:\\r\\n\\r\\n* Constructs another request to Azure OpenAI Service\u2019s chat API to generate a natural language recommendation based on the matching product. The request includes the matching product\u2019s details and asks for a natural language recommendation.\\r\\n* Sends the request and extracts the recommendation from the response\\r\\n\\r\\nNext, the code returns the recommendation:\\r\\n\\r\\n* The function ends by returning a JSON response containing the matching product\u2019s ID and the natural language recommendation.\\r\\n* The ID matches the file name of an image in the dataset, so you can use it to load and display images in the web UI in parts 2 and 3 of this series.\\r\\n\\r\\nFinally, you define a main function:\\r\\n\\r\\n* This is the function Azure runs to boot the function app and prepare it to receive HTTP requests.\\r\\n\\r\\nThis app combines image processing, text generation, and search capabilities to provide fashion item recommendations. It demonstrates how to implement the entire back end of an intelligent application.\\r\\n\\r\\n#### Deploy the Azure Function\\r\\n\\r\\nThe final step is to deploy the Azure Function to the cloud so the web interface can access it. Start by using the Azure CLI to create a new function app:\\r\\n\\r\\n```\\r\\naz functionapp create --resource-group <RESOURCE_GROUP_NAME> --consumption-plan-location westus --runtime python --runtime-version 3.9 --functions-version 4 --name <APP_NAME> --os-type linux --storage-account <STORAGE_NAME> \\r\\n```\\r\\n\\r\\nYou can use the same resource group and storage account you used for the search service.\\r\\n\\r\\n**Note**: The app name must be unique, so you might need to try a few options to find one available.\\r\\n\\r\\nNext, set all the environment variables the app will need by running:\\r\\n\\r\\n```\\r\\naz functionapp config appsettings set \u2013name <APP_NAME> --resource-group <RESOURCE_GROUP> --settings\\r\\nOPENAI_API_KEY=<your Azure OpenAI key>\\r\\nOPENAI_ENDPOINT=<your Azure OpenAI endpoint>\\r\\nOPENAI_DEPLOYMENT_NAME=<your Azure OpenAI deployment name>\\r\\nSEARCH_API_KEY=<your Search API key>\\r\\nSEARCH_ENDPOINT=<your Search service endpoint>\\r\\nSEARCH_INDEX_NAME=<your Search index name>\\r\\n```\\r\\n\\r\\nIf you\u2019re unsure where to find any of these values, here\u2019s how to locate them:\\r\\n\\r\\n* For the OpenAI values, find and load the Azure OpenAI page by entering OpenAI in the Azure Portal search bar. Click the name of your Azure OpenAI service, and you\u2019ll see two menu options:\\r\\n\\r\\n  ![the directory in Visual Studio Code](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-13.png)\\r\\n\\r\\n  Click **Keys and Endpoint** to locate the required information, or click **Model deployments** to navigate to Azure OpenAI Studio and find the names of your model deployments.\\r\\n\\r\\n* For the search service, load your stylist search service\u2019s page in Azure Portal:\\r\\n\\r\\n  * On the **Overview** page, the **Url** value is your search endpoint.\\r\\n  * Click **Keys** in the menu to access your search service\u2019s keys.\\r\\n  * Click **Indexes** in the menu to see the name of your search service\u2019s index.\\r\\n\\r\\nNote that you\u2019re saving these as app settings for simplicity. In a product app, you should keep secrets like API keys safe by using [Azure Key Vault](https://azure.microsoft.com/products/key-vault?ocid=buildia24_60days_blogs).\\r\\n\\r\\nOnce you\u2019ve created the app and saved its settings, you can deploy your function by running the following command from the Azure Functions CLI:\\r\\n\\r\\n```\\r\\nfunc azure functionapp publish <APP_NAME>\\r\\n```\\r\\n\\r\\nThe CLI will begin deploying your app. When the deployment is complete, the interface will provide a URL to send HTTPS requests to the function app.\\r\\n\\r\\nNow, the back end of the stylist chatbot app is complete! You\u2019re ready to move on to creating the web interface for the app.\\r\\n\\r\\n:::info\\r\\nJoin the Azure Functions product group for an **[Ask The Expert](https://aka.ms/intelligent-apps/ate-functions?ocid=buildia24_60days_blogs)** session on how to focus on the pieces of code that matter most to you in AI application development, while Azure Functions handles the rest for you.\\r\\n:::\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nIn this article, you learned how to create a virtual stylist chatbot that can analyze clothing styles in an image and identify the best match from a dataset of clothing options \u2014 leveraging Azure Functions and Azure OpenAI Service to do so. You also learned how to use Azure AI Search feature to index, store, and retrieve entries from a search index. Next, you discovered how to use Azure OpenAI Service to generate natural language descriptions and recommendations based on the user\u2019s input image.\\r\\n\\r\\nIn the next part of this series, you\u2019ll learn how to add a chatbot interface to the app using React and an [Azure Static Web App](https://azure.microsoft.com/products/app-service/static?ocid=buildia24_60days_blogs)."},{"id":"creating-a-virtual-stylist-chatbot-part-2","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-2","source":"@site/blog-60daysofIA/2024-03-27/creating-a-virtual-stylist-chatbot-part-2.md","title":"6.2 Creating a Virtual Stylist Chatbot \u2014 Part 2: Adding a Chatbot Interface","description":"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this second installment, you\u2019ll design the chatbot\u2019s interface.","date":"2024-03-27T09:00:00.000Z","formattedDate":"March 27, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":13.865,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-27T09:00","slug":"creating-a-virtual-stylist-chatbot-part-2","title":"6.2 Creating a Virtual Stylist Chatbot \u2014 Part 2: Adding a Chatbot Interface","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this second installment, you\u2019ll design the chatbot\u2019s interface.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"6.1 Creating a Virtual Stylist Chatbot \u2014 Part 1: Analyzing Images with AI","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-1"},"nextItem":{"title":"6.3 Creating a Virtual Stylist Chatbot \u2014 Part 3: Deploying the App","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-3"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/creating-a-virtual-stylist-chatbot-part-2\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this second installment, you\u2019ll design the chatbot\u2019s interface.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-2\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this second installment, you\u2019ll design the chatbot\u2019s interface.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-2\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Graphical representation of a chatbot. The human user\'s chat bubble contains a t-shirt with a question mark, while the bot\'s chat bubble contains three dots to indicate it is responding.](../../static/img/60-days-of-ia/blogs/2024-03-27/6-2-1.jpeg)\\r\\n\\r\\n## Creating a Virtual Stylist Chatbot \u2014 Part 2: Adding a Chatbot Interface\\r\\n\\r\\nWelcome to part 2 of this tutorial series on creating a virtual stylist chatbot using Azure OpenAI Service.\u202f \\r\\n\\r\\nIn [part 1](https://azure.github.io/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-1), you built the chatbot app\u2019s back end using Azure Functions, Azure AI Services, and GPT-4 Vision with Azure OpenAI Service. That tutorial covered using these services to analyze an image of a fashion item or outfit and generate natural language responses and recommendations based on it.\\r\\n\\r\\nIn this second installment, you\u2019ll create a chatbot interface for your virtual stylist app using Vite, Vue, TypeScript, and vue-advanced-chat. You\u2019ll learn how to use these tools to build a web application that allows you to interact with your stylist bot conversationally.\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nBefore you begin, ensure you have:\\r\\n\\r\\n* An Azure subscription with access to [Azure OpenAI Service](https://azure.microsoft.com/products/ai-services/openai-service?ocid=buildia24_60days_blogs)\\r\\n* [Azure command-line interface (CLI)](https://learn.microsoft.com/cli/azure/?ocid=buildia24_60days_blogs) installed\\r\\n* [Azure Functions Core Tools](https://github.com/Azure/azure-functions-core-tools) installed\\r\\n* An Azure OpenAI Service resource with a GPT-4 Vision model deployed\\r\\n* The deployment name, endpoint, and API key for your OpenAI Service\\r\\n* The [Fashion Product Images dataset](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small) from Kaggle\\r\\n* [Node.js 20](https://nodejs.org/en/download/) or later installed on your local machine \\r\\n* A text editor that supports Vue and TypeScript. If you use Visual Studio Code, consider installing the [TypeScript Vue Plugin](https://marketplace.visualstudio.com/items?itemName=Vue.vscode-typescript-vue-plugin) to ensure the editor understands all the files you\u2019re about to create. \\r\\n\\r\\nFor a preview of this tutorial, check out the [project code available on GitHub](https://github.com/rogerwinter/Microsoft-Creating-a-Virtual-Stylist-Chatbot/tree/main/stylist-backend).\\r\\n\\r\\n\\r\\n### Creating a Chatbot Interface for Your Virtual Stylist\\r\\n\\r\\nIn this section, you\u2019ll create a chatbot interface for the virtual stylist app using Vue and vue-advanced-chat. You\u2019ll use Vue to create the main components of the app, including the header, the footer, the chat window, and the image upload button. You\u2019ll also use the vue-advanced-chat library to create the chat messages, chat input, and other chat options, using Tailwind CSS to style the app.\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n#### Setting Up the Project\\r\\n\\r\\nThe first step is creating a new Vue project using Vite. Vite is a fast and lightweight build tool that provides a smooth developer experience and supports features like hot module replacement, code splitting, and tree shaking.\\r\\n\\r\\nTo create a new Vue project with Vite, run the following command in your terminal:\\r\\n\\r\\n```\\r\\nnpm init vite@latest virtual-stylist-chat -- --template vue-ts\\r\\n```\\r\\n\\r\\nThis builds a new folder, `virtual-stylist-chat`, with the following structure:\\r\\n\\r\\n```\\r\\nvirtual-stylist-chat\\r\\n\u251c\u2500\u2500 index.html\\r\\n\u251c\u2500\u2500 package.json\\r\\n\u251c\u2500\u2500 public\\r\\n\u2502   \u2514\u2500\u2500 favicon.svg\\r\\n\u251c\u2500\u2500 src\\r\\n\u2502   \u251c\u2500\u2500 App.vue\\r\\n\u2502   \u251c\u2500\u2500 assets\\r\\n\u2502   \u2502   \u2514\u2500\u2500 logo.svg\\r\\n\u2502   \u251c\u2500\u2500 components\\r\\n\u2502   \u2502   \u2514\u2500\u2500 HelloWorld.vue\\r\\n\u2502   \u251c\u2500\u2500 main.ts\\r\\n\u2502   \u2514\u2500\u2500 shims-vue.d.ts\\r\\n\u2514\u2500\u2500 tsconfig.json\\r\\n```\\r\\n\\r\\nNext, add a few dependencies:\\r\\n\\r\\n* [vue-advanced-chat](https://github.com/advanced-chat/vue-advanced-chat), a feature-rich and highly customizable Vue chat component library that provides many out-of-the-box features for chat interfaces. These include images, videos, files, voice messages, emojis, link previews, typing indicators, reactions, markdown text formatting, online presence indicators, delivery and read receipts, theming and customization options, and responsive design.\\r\\n* [Tailwind CSS](https://tailwindcss.com/), [PostCSS](https://postcss.org/), and [autoprefixer](https://www.npmjs.com/package/autoprefixer) to simplify styling the app\\r\\n* [uuid](https://www.npmjs.com/package/uuid) to generate unique IDs for each message\\r\\n\\r\\nTo install the required packages, run the following command:\\r\\n\\r\\n```\\r\\nnpm install --save vue-advanced-chat tailwindcss@latest postcss@latest autoprefixer@latest uuid @types/uuid \\r\\n```\\r\\n\\r\\nThis command adds vue-advanced-chat, Tailwind, and PostCSS as dependencies in the `package.json` file.\\r\\n\\r\\nNow that you\u2019ve set up the project and installed the dependencies, check that it builds as expected by running `npm run dev`. The app should build and provide an address to view it in a web browser. Load it, and you should see the default welcome screen:\\r\\n\\r\\n![the Vite + Vue welcome page displays both logos and provides links to create-vue and Volar.](../../static/img/60-days-of-ia/blogs/2024-03-27/6-2-2.png)\\r\\n\\r\\nNext, generate the `tailwind.config.js` and `postcss.config.js` files using the following command:\\r\\n\\r\\n```\\r\\nnpx tailwindcss init -p\\r\\n```\\r\\n\\r\\nEdit the `tailwind.config.js` file and add the paths to your template files in the `content` property:\\r\\n\\r\\n```\\r\\n// tailwind.config.js\\r\\nexport default {\\r\\n  content: [\\"./index.html\\", \\"./src/**/*. {vue,js,ts,jsx,tsx}\\"],\\r\\n  theme: {\\r\\n    extend: {},\\r\\n  },\\r\\n  plugins: [],\\r\\n};\\r\\n```\\r\\n\\r\\nThen, replace the content of `style.css` file in the `src` folder with the following code to import Tailwind CSS using the `@tailwind` directives:\\r\\n\\r\\n```\\r\\n@tailwind base;\\r\\n@tailwind components;\\r\\n@tailwind utilities;\\r\\n```\\r\\n\\r\\nThen, import the `styles.css` file in the `main.ts` file and remove the unused import:\\r\\n\\r\\n```\\r\\nimport { createApp } from \\"vue\\";\\r\\nimport App from \\"./App.vue\\";\\r\\nimport \\"./styles.css\\"; // import Tailwind CSS\\r\\n\\r\\ncreateApp(App).mount(\\"#app\\");\\r\\n```\\r\\n\\r\\nFinally, copy the images from the [dataset](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small) you downloaded in the first part of this series. Using your preferred CLI or file manager, create a new folder called `Images` inside the project\u2019s `public` folder, and then copy all the images from the dataset\u2019s `images_compressed` folder to the `Images` folder. The stylist bot will use these images to make recommendations based on the image IDs it returns.\\r\\n\\r\\nThe result should look like this:\\r\\n\\r\\n```\\r\\nvirtual-stylist-chat\\r\\n\u251c\u2500\u2500 index.html\\r\\n\u251c\u2500\u2500 package.json\\r\\n\u251c\u2500\u2500 public\\r\\n\u2502   \u251c\u2500\u2500 favicon.svg\\r\\n\u2502   \u2514\u2500\u2500 images\\r\\n\u2502       \u251c\u2500\u2500 10001.jpg\\r\\n\u2502       \u251c\u2500\u2500 10002.jpg\\r\\n\u2502       \u251c\u2500\u2500 10003.jpg\\r\\n\u2502       \u251c\u2500\u2500 ...\\r\\n\u2502       \u251c\u2500\u2500 19998.jpg\\r\\n\u2502       \u251c\u2500\u2500 19999.jpg\\r\\n\u2502       \u2514\u2500\u2500 20000.jpg\\r\\n\u251c\u2500\u2500 src\\r\\n\u2502   \u251c\u2500\u2500 App.vue\\r\\n\u2502   \u251c\u2500\u2500 assets\\r\\n\u2502   \u2502   \u2514\u2500\u2500 logo.svg\\r\\n\u2502   \u251c\u2500\u2500 components\\r\\n\u2502   \u2502   \u2514\u2500\u2500 HelloWorld.vue\\r\\n\u2502   \u251c\u2500\u2500 main.ts\\r\\n\u2502   \u251c\u2500\u2500 styles.css\\r\\n\u2502   \u251c\u2500\u2500 tailwind.config.js\\r\\n\u2502   \u251c\u2500\u2500 postcss.config.js\\r\\n\u2502   \u2514\u2500\u2500 shims-vue.d.ts\\r\\n\u2514\u2500\u2500 tsconfig.json\\r\\n```\\r\\n\\r\\nNow, it\u2019s time to start coding the chatbot interface.\\r\\n\\r\\n#### Coding the Chatbot Interface\\r\\n\\r\\nIn this section, you\u2019ll prepare your virtual stylist app\u2019s chatbot interface. You\u2019ll use Vue to create the main components, including the header, the footer, the chat window, and the image upload button. Then, you\u2019ll use the vue-advanced-chat component to create the chat messages, input, and options.\\r\\n\\r\\nTo keep things simple, we\u2019ll link to the code of non-essential components like the header and footer. Since these aren\u2019t critical to how the app functions, feel free to copy and paste them into your codebase.\\r\\n\\r\\n##### Header and Footer\\r\\n\\r\\nStart by creating two files in the `src/components` folder: `Header.vue` and `Footer.vue`. Next, copy the code from the [header](https://github.com/contentlab-io/Microsoft-Creating-a-Virtual-Stylist-Chatbot/blob/main/stylist_frontend/src/components/Header.vue) and [footer](https://github.com/contentlab-io/Microsoft-Creating-a-Virtual-Stylist-Chatbot/blob/main/stylist_frontend/src/components/Footer.vue) files in the GitHub repository into the files you just created.\\r\\n\\r\\nThese files are simple Vue components that use HTML and CSS to create a stylish header and footer for the app. If you\u2019d like to customize them, replace the logo image link in the header with a link to an image of your own.\\r\\n\\r\\nNow, it\u2019s time to dive into the chat interface that makes this app work.\\r\\n\\r\\n##### Creating the Chat Window Component\\r\\n\\r\\nThe chat window component displays the messages between the user and the stylist bot. To start, create a new file called `ChatWindow.vue` inside the project\u2019s src/components folder. Then, add the following code to it:\\r\\n\\r\\n```\\r\\n<template>\\r\\n  <div class=\\"chat-window h-screen\\">\\r\\n    <vue-advanced-chat\\r\\n      .messages=\\"messages\\"\\r\\n      .options=\\"options\\"\\r\\n      .rooms=\\"[{ roomId: \'main\', roomName: \'Stylist Chat\', avatar: \'/images/logo.svg\', users: [currentUser]}]\\"\\r\\n      :rooms-list-opened=\\"false\\"\\r\\n      :rooms-loaded=\\"true\\"\\r\\n      :messages-loaded=\\"true\\"\\r\\n      :current-user-id=\\"currentUser._id\\"\\r\\n      accepted-files=\\".png, .jpg, .jpeg\\"\\r\\n      show-audio=\\"false\\"\\r\\n      @send-message=\\"onInputSubmit\\"\\r\\n      .message-actions=\\"[{\\r\\n        label: \'Send\',\\r\\n        action: (message: Message) => {\\r\\n          console.log(\'Send message \' + message.content);\\r\\n        },\\r\\n      }]\\"\\r\\n      v-bind=\\"{\\r\\n        \'current-user-id\': currentUser?._id || \'\',\\r\\n        \'room-info-enabled\': false,\\r\\n       }\\"\\r\\n\\r\\n       />\\r\\n  </div>\\r\\n</template>\\r\\n\\r\\n<script lang=\\"ts\\">\\r\\nimport { defineComponent, ref, Ref } from \\"vue\\";\\r\\nimport { VueAdvancedChat, Message, register, RoomUser } from \\"vue-advanced-chat\\";\\r\\nregister();\\r\\nimport { v4 as uuidv4 } from \\"uuid\\";\\r\\n\\r\\nfunction toTimeString(date: Date): string {\\r\\n  let month = date.toLocaleString(\'default\', { month: \'short\' });\\r\\n  return `${date.getFullYear()}-${month}-${date.getDate()} ${date.getHours()}:${date.getMinutes()}`;\\r\\n}\\r\\n\\r\\nexport default defineComponent({\\r\\n  name: \\"ChatWindow\\",\\r\\n  components: {\\r\\n    VueAdvancedChat,\\r\\n  },\\r\\n  setup() {\\r\\n    // Define the current user, the messages, and the options for the chat component\\r\\n    const currentUser: Ref<RoomUser> = ref({\\r\\n      _id: \\"user\\",\\r\\n      username: \\"User\\",\\r\\n      avatar: \\"\\",\\r\\n      status: { state: \\"online\\", lastChanged: new Date().toDateString()},\\r\\n    });\\r\\n    const messages: Ref<Array<Message>> = ref([]);\\r\\n    const options = ref({\\r\\n      enableVoiceMessages: false,\\r\\n      enableReactions: false,\\r\\n      enableSeenBy: false,\\r\\n      enableLinkPreview: false,\\r\\n      enableUploads: true,\\r\\n      enableAttachments: false,\\r\\n      enableReply: true,\\r\\n      enableEdit: false,\\r\\n      enableDelete: false,\\r\\n      enableGroup: false,\\r\\n      enableSearch: false,\\r\\n      enableOptions: false,\\r\\n      enableScrollToBottom: true,\\r\\n      enableScrollToTop: false,\\r\\n      enableLoadMore: false,\\r\\n      enableComposer: true,\\r\\n      enableInput: true,\\r\\n      enableSendButton: true,\\r\\n      enableEmojis: false,\\r\\n      enableRecording: false,\\r\\n      enableMarkdown: true,\\r\\n      enableTypingIndicator: true,\\r\\n      enableOnlinePresence: false,\\r\\n      enableCustomTheme: true,\\r\\n      enableRooms: false,\\r\\n      customTheme: {\\r\\n        primaryColor: \\"#333333\\",\\r\\n        secondaryColor: \\"#f0f0f0\\",\\r\\n        tertiaryColor: \\"#ffffff\\",\\r\\n        quaternaryColor: \\"#e0e0e0\\",\\r\\n        quinaryColor: \\"#999999\\",\\r\\n        senaryColor: \\"#666666\\",\\r\\n        septenaryColor: \\"#333333\\",\\r\\n        octonaryColor: \\"#f0f0f0\\",\\r\\n        nonaryColor: \\"#ffffff\\",\\r\\n        denaryColor: \\"#e0e0e0\\",\\r\\n      },\\r\\n    });\\r\\n\\r\\n    // Update the image preview in the chat message after it\'s uploaded\\r\\n    const updateMessageImage = (newMessage: Message, url: string) => {\\r\\n      const existingMessage = messages.value.find(m => m._id === newMessage._id);\\r\\n      // Update the URL of the first message file\\r\\n      const message = existingMessage || newMessage;\\r\\n\\r\\n      if(message && message.files && message.files.length > 0) {\\r\\n        message.files[0].url = url;\\r\\n        const existingMessages = messages.value.filter(m => m._id !== message._id);\\r\\n        //set a new message ID to prevent file from being overwritten\\r\\n        message._id = uuidv4();\\r\\n        messages.value = [...existingMessages, message];\\r\\n      }\\r\\n    }\\r\\n\\r\\n    const onInputSubmit = async (event: CustomEvent) => {\\r\\n      // Create a new message object with the content and the current user\\r\\n      console.log(\\"called!\\")\\r\\n      let content = event.detail[0].content;\\r\\n      let files = event.detail[0].files;\\r\\n      const newMessage: Message = {\\r\\n        // generate uuid\\r\\n        _id: uuidv4(),\\r\\n        content,\\r\\n        senderId: currentUser.value._id,\\r\\n        date: new Date().toLocaleString(\'default\', { year: \'numeric\', month: \'short\', day: \'numeric\' }),\\r\\n        timestamp: toTimeString(new Date()),\\r\\n      };\\r\\n\\r\\n      if(files) {\\r\\n        newMessage.files = [...files.map((file: any) => {\\r\\n          var messageFile = {\\r\\n            name: file.name,\\r\\n            size: file.size,\\r\\n            type: file.type,\\r\\n            url: file.url || file.localUrl, \\r\\n            extension: file.extension,\\r\\n            preview: file.localUrl,\\r\\n          }\\r\\n          const reader = new FileReader();\\r\\n          reader.readAsDataURL(file.blob);\\r\\n\\r\\n          reader.onload = () => { \\r\\n            // Get the base64-encoded string from the reader result \\r\\n            messageFile.url = reader.result as string;\\r\\n            // reload messages so UI updates\\r\\n            messages.value = [...messages.value];\\r\\n            updateMessageImage(newMessage, messageFile.url!);\\r\\n            callBackendFunction(content, reader.result as string);\\r\\n          };\\r\\n          return messageFile;\\r\\n        })];\\r\\n      } else {\\r\\n\\r\\n        // Push the new message to the messages array\\r\\n        messages.value = [...messages.value, newMessage];\\r\\n        // Call the backend function to get the response from the stylist bot\\r\\n        callBackendFunction(content, \\"\\");\\r\\n      }\\r\\n    };\\r\\n\\r\\n    const callBackendFunction = async (prompt: string, image: string) => {\\r\\n      // Get the previous prompts and responses from the messages array\\r\\n      const context = messages.value\\r\\n        .filter((message) => message.content || message.replyMessage)\\r\\n        .map((message) => ({\\r\\n          prompt: message.content,\\r\\n          response: message.replyMessage,\\r\\n        }));\\r\\n      // Create a JSON object with the prompt, the image, and the context\\r\\n      const data = {\\r\\n        prompt,\\r\\n        image,\\r\\n        context,\\r\\n      };\\r\\n      // Send a POST request to the backend function URL with the data\\r\\n      const response = await fetch(\\"<backend function URL>\\", {\\r\\n        method: \'POST\',\\r\\n        headers: {\\r\\n          \'Content-Type\': \'application/json\',\\r\\n        },\\r\\n        body: JSON.stringify(data),\\r\\n      });\\r\\n      // Get the response data from the fetch response\\r\\n      const responseData = await response.json();\\r\\n      // Create a new message object with the response data and the stylist bot\\r\\n      const newMessage: Message = {\\r\\n        _id: uuidv4(),\\r\\n        content: responseData.response,\\r\\n        files: responseData.images,\\r\\n        senderId: \\"stylist-bot\\",\\r\\n        date: new Date().toLocaleString(\'default\', { year: \'numeric\', month: \'short\', day: \'numeric\' }),\\r\\n        timestamp: toTimeString(new Date()),\\r\\n      };\\r\\n      // Push the new message to the messages array\\r\\n      messages.value = [...messages.value, newMessage];\\r\\n    };\\r\\n\\r\\n    // Return the current user, the messages, the options, and the event handlers\\r\\n    return {\\r\\n      currentUser,\\r\\n      messages,\\r\\n      options,\\r\\n      onInputSubmit,\\r\\n    };\\r\\n  },\\r\\n\\r\\n  mounted() {\\r\\n    // Add a welcome message from the stylist bot when the component is mounted\\r\\n    this.messages = [...this.messages, { _id: \\"stylist-bot\\", content: \\"Hello! I\'m your virtual stylist chatbot. You can ask me for fashion advice, recommendations, and more. You can also upload images of clothing items and accessories to get personalized suggestions. How can I help you today?\\", senderId: \\"stylist-bot\\", date: new Date().toTimeString()}];\\r\\n  },\\r\\n});\\r\\n\\r\\n<\/script>\\r\\n\\r\\n<style scoped>\\r\\n.chat-window {\\r\\n  @apply h-screen flex-1 overflow-y-auto;\\r\\n}\\r\\n</style> \\r\\n```\\r\\n\\r\\nThis code defines a chat window component that uses the vue-advanced-chat component to display the messages between the user and the stylist bot. It also defines some data and methods to handle the chat logic, such as the current user, messages, options, input submit event, file upload event, and the back-end function call.\\r\\n\\r\\n`currentUser` and `messages` are reactive objects that store information about the chat participant and chat history. The `currentUser` object represents the app user while the `messages` array contains the Message objects with the following properties:\\r\\n\\r\\n* `_id`\u2014A unique identifier for the message\\r\\n* `content`\u2014The text content of the message (optional)\\r\\n* `files`\u2014Contains any files attached to the image (optional)\\r\\n* `senderId`\u2014The ID of the message sender\\r\\n* `date`\u2014The date of the message\\r\\n* `timestamp`\u2014The time and date that appear with every message\\r\\n\\r\\nThe `options` object contains the configuration options for the vue-advanced-chat component. It allows you to enable or disable various features of the chat interface, including:\\r\\n\\r\\n* Voice messages\\r\\n* Reactions\\r\\n* Seen by\\r\\n* Link preview\\r\\n* Uploads and attachments\\r\\n* Reply and send button\\r\\n* Edit and delete\\r\\n* Group and search\\r\\n* Options\\r\\n* Scroll to bottom and scroll to top\\r\\n* Load more\\r\\n* Composer\\r\\n* Input\\r\\n* Emojis\\r\\n* Recording\\r\\n* Markdown\\r\\n* Typing indicator\\r\\n* Online presence/status\\r\\n* Custom theme\\r\\n\\r\\nYou can learn more about the options and their meanings in the [documentation](https://github.com/advanced-chat/vue-advanced-chat).\\r\\n\\r\\nThe `onInputSubmit` method is the event handler for the input submit event. It\u2019s triggered when the user types a text message and presses the **Enter** key or clicks the **Send** button. This method creates a new message object with the text content and the current user, then pushes it to the messages array.\\r\\n\\r\\nIf the message contains an attached image file, the function loads it into a base64-encoded string, which is what the back-end Azure function expects to receive. Finally, it calls the back-end function to prompt a response from the stylist bot. \\r\\n\\r\\nThe `callBackendFunction` method calls the back-end Azure function to retrieve the stylist bot\u2019s reply. It takes the prompt and the image as parameters and sends a POST request to the back-end function URL with the data and the options. The data object contains the prompt, image, and context.\\r\\n\\r\\nThe context is an array of objects that store the previous prompts and responses from the `messages` array. The options object contains the headers for the request, such as the content type. The `response` object contains the response data from the back-end function, including the response, images, and context. \\r\\n\\r\\nFinally, the function creates a new `message` object with the response data and the stylist bot\u2019s ID, and then adds it to the `messages` array.\\r\\n\\r\\n:::info\\r\\nRegister for [Episode 2](https://aka.ms/serverless-learn-live/ep2?ocid=buildia24_60days_blogs) of the new learning series on **Intelligent Apps with Serverless on Azure**. Join the community along with MVPs, and the Azure Product Group on how to leverage AI with Serverless on Azure technologies\u2014Azure Functions and Azure Container Apps\u2014to build intelligent applications.\\r\\n:::\\r\\n\\r\\n#### Integrating Components into the App Component\\r\\n\\r\\nIn this section, you\u2019ll integrate the components you just created into the `src/App.vue` file\u2014your main app component. You\u2019ll import the header, footer, chat window, and image upload button components and display them in a simple layout.\\r\\n\\r\\nTo start, open the `App.vue` file in the project\u2019s `src` folder and replace the existing code with the following:\\r\\n\\r\\n```\\r\\n<template>\\r\\n  <div class=\\"app\\">\\r\\n    <Header />\\r\\n    <div class=\\"main\\">\\r\\n      <ChatWindow ref=\\"chat\\" />\\r\\n      <ImageUploadButton :chat=\\"chat\\" />\\r\\n    </div>\\r\\n    <Footer />\\r\\n  </div>\\r\\n</template>\\r\\n\\r\\n<script lang=\\"ts\\">\\r\\nimport { defineComponent, ref } from \\"vue\\";\\r\\nimport Header from \\"./components/Header.vue\\";\\r\\nimport Footer from \\"./components/Footer.vue\\";\\r\\nimport ChatWindow from \\"./components/ChatWindow.vue\\";\\r\\n\\r\\nexport default defineComponent({\\r\\n  name: \\"App\\",\\r\\n  components: {\\r\\n    Header,\\r\\n    Footer,\\r\\n    ChatWindow\\r\\n  },\\r\\n  setup() {\\r\\n    // Define a ref for the chat component\\r\\n    const chat = ref(ChatWindow);\\r\\n    // Return the ref\\r\\n    return {\\r\\n      chat,\\r\\n    };\\r\\n  },\\r\\n});\\r\\n<\/script>\\r\\n<style>\\r\\n.app {\\r\\n  @apply min-h-screen flex flex-col;\\r\\n}\\r\\n\\r\\n.main {\\r\\n  @apply flex-1 flex flex-col;\\r\\n}\\r\\n</style>\\r\\n```\\r\\n\\r\\nThis code defines the app component that uses the header, footer, chat window, and image upload button components. It also defines a [`ref`](https://vuejs.org/guide/essentials/template-refs) for the chat component and passes it as a prop to the image upload button component. This action allows the image upload button component to access the chat component\u2019s methods, such as `onFileUpload`.\\r\\n\\r\\nWith that, you\u2019re ready to deploy!\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nPart 2 of this series equipped you with the necessary skills to create a dynamic chatbot interface for your virtual stylist app. By setting up your project, installing dependencies, and coding the chatbot interface, you laid the groundwork for the final deployment and testing phase. Now, you\u2019re ready to see your virtual stylist in action.\\r\\n\\r\\nJump to the third part of this series, where you\u2019ll deploy and test your Intelligent App."},{"id":"creating-a-virtual-stylist-chatbot-part-3","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-3","source":"@site/blog-60daysofIA/2024-03-28/creating-a-virtual-stylist-chatbot-part-3.md","title":"6.3 Creating a Virtual Stylist Chatbot \u2014 Part 3: Deploying the App","description":"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In the final article of this series, you\u2019ll deploy and test the Intelligent App.","date":"2024-03-28T09:00:00.000Z","formattedDate":"March 28, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.32,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-28T09:00","slug":"creating-a-virtual-stylist-chatbot-part-3","title":"6.3 Creating a Virtual Stylist Chatbot \u2014 Part 3: Deploying the App","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In the final article of this series, you\u2019ll deploy and test the Intelligent App.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"6.2 Creating a Virtual Stylist Chatbot \u2014 Part 2: Adding a Chatbot Interface","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-2"},"nextItem":{"title":"6.4 Building a Multichannel Notification System (1)","permalink":"/Cloud-Native/60DaysOfIA/building-a-multichannel-notification-system-1"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/creating-a-virtual-stylist-chatbot-part-3\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In the final article of this series, you\u2019ll deploy and test the Intelligent App.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-3\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In the final article of this series, you\u2019ll deploy and test the Intelligent App.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-3\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![A minimalist graphic features a t-shirt, pants, and robot head in a rounded square, connected by broken line to a smartphone that displays a chatbot conversation.](../../static/img/60-days-of-ia/blogs/2024-03-28/6-3-1.jpeg)\\r\\n\\r\\n## Creating a Virtual Stylist Chatbot\u2014Part 3: Deploying the App\\r\\n\\r\\nIn [part 1](https://azure.github.io/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-1) of this series, you used AI to analyze images of clothing and generate a text description of each piece. Then, in [part 2](https://azure.github.io/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-2), you designed the chatbot\u2019s interface.\\r\\n\\r\\nIn this third and final installment, you\u2019ll deploy the app as an Azure Static Web App using the Azure command-line interface (CLI). The Azure Static Web Apps service provides a hassle-free means of hosting static web apps with serverless APIs. It also features global distribution, custom domains, SSL certificates, authentication, authorization, and GitHub integration.\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nTo follow along, ensure you have:\\r\\n\\r\\n* The complete code from [part 2](https://azure.github.io/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-2)\\r\\n* The Azure CLI installed and signed in to your Azure account\\r\\n* A GitHub account, [an empty repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/quickstart-for-repositories) to push the app\u2019s code to, and a [personal access token](https://docs.github.com/en/enterprise-server@3.9/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-personal-access-token) granting read and write access to the repository\\r\\n\\r\\nFor a preview of the project, check out the [complete project code available on GitHub](https://github.com/rogerwinter/Microsoft-Creating-a-Virtual-Stylist-Chatbot/).\\r\\n\\r\\n### Pushing the App to GitHub\\r\\n\\r\\nYou can set up Azure Static Web Apps to deploy automatically every time you push a new commit to GitHub. Before proceeding, create a GitHub repository for the web app and push all its code to the repo.\\r\\n\\r\\n:::info\\r\\nRegister for [Episode 3](https://aka.ms/serverless-learn-live/ep3?ocid=buildia24_60days_blogs) of the new learning series on **Intelligent Apps with Serverless on Azure**. Join the community along with MVPs, and the Azure Product Group on how to leverage AI with Serverless on Azure technologies \u2013Azure Functions and Azure Container Apps \u2013 to build intelligent applications.\\r\\n:::\\r\\n\\r\\n### Creating an Azure Static Web Resource\\r\\n\\r\\nNext, you\u2019ll create an Azure Static Web App resource using the Azure CLI. The Azure Static Web App resource is the container for the app and its settings.\\r\\n\\r\\nTo create it, run the following command in your terminal:\\r\\n\\r\\n```\\r\\naz staticwebapp create \\\\\\r\\n  --name virtual-stylist-chat \\\\\\r\\n  --resource-group <your resource group> \\\\\\r\\n  --location westus2 \\\\\\r\\n  --source virtual-stylist-chat \\\\\\r\\n  --branch main \\\\\\r\\n  --app-location / \\\\\\r\\n  --output-location dist \\\\\\r\\n  --login-with-github\\r\\n```\\r\\n\\r\\nThis command will create an Azure Static Web App resource with the following parameters:\\r\\n\\r\\n* **`--name`**\u2014The name of the resource, which must be globally unique\\r\\n* **`--resource-group`**\u2014The name of the resource group to contain the resource\\r\\n* **`--location`** \u2014The location of the resource\\r\\n* **`--source`**\u2014The name of the GitHub repository that contains the app code\\r\\n* **`--branch`**\u2014The name of the GitHub branch that contains the app code\\r\\n* **`--app-location`**\u2014The location of the app code in the repository\\r\\n* **`--output-folder`**\u2014The folder where the app output is generated\\r\\n* **`--login-with-github`**\u2014The GitHub personal access token that grants access to the repository\\r\\n\\r\\nThe command creates a GitHub Actions workflow file in the repository that triggers the app build and deployment whenever a change is pushed to the branch. It also outputs some information about the resource, like this:\\r\\n\\r\\n```\\r\\n{ \\r\\n  \\"defaultHostname\\": \\"orange-beach-0c471f710.azurestaticapps.net\\",\\r\\n  \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/virtual-stylist-chat-rg/providers/Microsoft.Web/staticSites/virtual-stylist-chat\\",\\r\\n  \\"location\\": \\"West US 2\\",\\r\\n  \\"name\\": \\"virtual-stylist-chat\\",\\r\\n  \\"repositoryUrl\\": \\"https://github.com/username/virtual-stylist-chat\\",\\r\\n  \\"resourceGroup\\": \\"virtual-stylist-chat-rg\\",\\r\\n  \\"sku\\": \\"Free\\",\\r\\n  \\"type\\": \\"Microsoft.Web/staticSites\\",\\r\\n  \\"userId\\": \\"username\\",\\r\\n  \\"workflowFileUrl\\": \\"https://github.com/username/virtual-stylist-chat/blob/main/.github/workflows/azure-static-web-apps-virtual-stylist-chat.yml\\"\\r\\n}\\r\\n```\\r\\n\\r\\nYou\u2019ve now created an Azure Static Web App resource and a GitHub Actions workflow for the app.\\r\\n\\r\\nTo link the function app from part 1 as the back end for the Azure Static Web App, you use [`az staticwebapp backends link`](https://learn.microsoft.com/cli/azure/staticwebapp/backends?view=azure-cli-latest&ocid=buildia24_60days_blogs#az-staticwebapp-backends-link). This command links a pre-existing back end with a static web app, also known as \u201cBring your own API.\u201d You need to provide the function app\u2019s resource ID, the static web app\u2019s resource group, and the back-end region.\\r\\n\\r\\nLink the function app as the back end for the static web app by running the following:\\r\\n\\r\\n```\\r\\naz staticwebapp backends link \\\\\\r\\n  --backend-resource-id \\"/subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Web/sites/<function-app-name>\\" \\\\\\r\\n  --name virtual-stylist-chat \\\\\\r\\n  --resource-group <your-resource-group> \\\\\\r\\n  --backend-region westus\\r\\n```\\r\\n\\r\\n### Testing the App\\r\\n\\r\\nNow, you\u2019ll test the app by uploading some images of clothing items or outfits to see how the stylist bot responds and makes recommendations. You\u2019ll also witness how the app handles different types of inputs, such as images and text messages.\\r\\n\\r\\n#### Uploading an Image of a Fashion Item\\r\\n\\r\\nTo start, you\u2019ll upload an image of a blue denim jacket to see how the bot responds.\\r\\n\\r\\nClick **Upload** at the bottom of the chat window. Then, select the image file from your local machine. Alternatively, you can drag and drop the image file to the chat window.\\r\\n\\r\\nThe app will display the image as a chat message and send it to the back-end function. This function will analyze the image and generate a natural language response and recommendations using Azure Functions, Azure AI Services, and GPT-4 Vision using Azure OpenAI Service. It will then display the response and its recommendations as another chat message. Your result will look something like this:\\r\\n\\r\\n![The Virtual Stylist Chatbot sends a greeting message that invites the user to request fashion advice and recommendations. The user responds with an image of a red t-shirt and a request to find a matching outfit.](../../static/img/60-days-of-ia/blogs/2024-03-28/6-3-2.png)\\r\\n\\r\\nAs you can see, the stylist bot correctly identified the fashion item as a red t-shirt and provided some information and tips about it. It also suggested some images of other items to pair with red t-shirts, including blue jeans and a red hat:\\r\\n\\r\\n![The Virtual Stylist Chatbot returns a message with images of jeans and a red baseball cap](../../static/img/60-days-of-ia/blogs/2024-03-28/6-3-3.png)\\r\\n\\r\\nYou can click the images to view them full-size:\\r\\n\\r\\n![The full-size photo of the jeans includes a partially visible torso and arms.](../../static/img/60-days-of-ia/blogs/2024-03-28/6-3-4.png)\\r\\n\\r\\nIf you don\u2019t like the suggestions or just want to see more, you can reply with additional details or questions, and it will generate new suggestions based on the information you provide.\\r\\n\\r\\n:::info\\r\\nJoin the Azure Functions product group for an **[Ask The Expert](https://aka.ms/intelligent-apps/ate-functions?ocid=buildia24_60days_blogs)** session on how to focus on the pieces of code that matter most to you in AI application development, while Azure Functions handles the rest for you at extreme scale.\\r\\n:::\\r\\n\\r\\n### Conclusion\\r\\n\\r\\nIn this tutorial series, you learned how to create a virtual stylist chatbot app using Azure and OpenAI. You built the app\u2019s back end using Azure Functions, Azure AI, and GPT-4 Vision on Azure OpenAI Service. You then learned how to use these services to analyze images and generate natural language responses and recommendations based on the images. Next, you created the chatbot interface for our app using Vite, Vue, TypeScript, Tailwind CSS, and vue-advanced-chat.\\r\\n\\r\\nYou learned how to use these tools to build a web application that allows you conversationally interact with your stylist bot. Finally, you deployed the app as an Azure Static Web App using the Azure CLI.\\r\\n\\r\\nGet your hands on the newly released [Azure Functions Flex Consumption Plan](https://aka.ms/flexconsumption/signup?ocid=buildia24_60days_blogs) for private networking, instance size selection, concurrency control, and fast and large scale out features on a serverless compute model."},{"id":"building-a-multichannel-notification-system-1","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/building-a-multichannel-notification-system-1","source":"@site/blog-60daysofIA/2024-04-01/building-a-multichannel-notification-system-1.md","title":"6.4 Building a Multichannel Notification System (1)","description":"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.","date":"2024-04-01T09:00:00.000Z","formattedDate":"April 1, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":12.645,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-04-01T09:00","slug":"building-a-multichannel-notification-system-1","title":"6.4 Building a Multichannel Notification System (1)","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"6.3 Creating a Virtual Stylist Chatbot \u2014 Part 3: Deploying the App","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-3"},"nextItem":{"title":"6.5 Building a Multichannel Notification System (2)","permalink":"/Cloud-Native/60DaysOfIA/building-a-multichannel-notification-system-2"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/building-a-multichannel-notification-system-1\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/building-a-multichannel-notification-system-1\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/building-a-multichannel-notification-system-1\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n## Building a Multichannel Notification System with Azure Functions and Azure Communication Services\u202f(1)\\r\\n\\r\\nIn the interconnected digital era, it\'s crucial for businesses and services to communicate effectively with their audience. A robust notification system that spans various communication channels can greatly enhance user engagement and satisfaction. This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.\\r\\n\\r\\nLeveraging serverless architecture and the reach of Azure Communication Services, your application can dynamically generate and send messages via SMS, Email, and WhatsApp. By incorporating OpenAI GPTs, the system can create content that is not only relevant and timely but personalized, making communication more impactful.\\r\\n\\r\\n![image of email composer assist from Chat GPT](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-1.gif)\\r\\n\\r\\nArchitecture Diagram\\r\\n\\r\\n![Architecture Diagram](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-2.png)\\r\\n\\r\\nHere are some practical scenarios where a multichannel notification system is valuable: \\r\\n\\r\\n  1. **Financial Alerts**: Banks and financial services can send fraud alerts, transaction confirmations, and account balance updates.\\r\\n  2. **Healthcare Reminders**: Clinics and pharmacies can notify patients about appointment schedules, vaccinations, or prescription refills.\\r\\n  3. **Security Verification**: Services requiring secure authentication can utilize two-factor authentication prompts sent via SMS or WhatsApp.\\r\\n  4. **Marketing and Promotions**: Retailers can craft and distribute targeted marketing messages and promotions, thereby driving customer engagement.\\r\\n  5. **Infrastructure Notifications**: Utility companies can alert customers about service disruptions, maintenance schedules, or conservation tips.\\r\\n  6. **E-commerce Updates**: Online retailers can inform customers about order confirmations, shipping details, and delivery tracking.\\r\\n\\r\\nThe foundation of this solution is [Azure Functions](https://docs.microsoft.com/azure/azure-functions/?ocid=buildia24_60days_blogs), a flexible, event-driven platform for running scalable applications with minimal overhead. We will utilize [Azure Communication Services](https://docs.microsoft.com/azure/communication-services/?ocid=buildia24_60days_blogs), which provides reliable APIs for Email, SMS, and WhatsApp messaging. To generate content, we use [OpenAI GPTs](https://openai.com/blog/introducing-gpts), which enables the creation of sophisticated, context-aware text that can be used in notifications.\\r\\n\\r\\nBy following this tutorial, you will gain the knowledge and practical experience necessary to implement a scalable multichannel notification platform that can serve a wide array of communication needs. Let\'s get started on your path to building a cutting-edge, serverless messaging system on Azure.\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nBefore we dive into building our multichannel notification system with Azure Functions and Azure Communication Services, you will need to ensure that the following tools and accounts set up: \\r\\n\\r\\n  1. **Azure Account**: You\'ll need a Microsoft Azure account to create and manage resources on Azure. If you haven\'t got one yet, you can [create a free account here](https://azure.microsoft.com/free/?ocid=buildia24_60days_blogs).\\r\\n  2. **Visual Studio Code**: We\'ll be using Visual Studio Code (VS Code) as our Integrated Development Environment (IDE) for writing and debugging our code. Download and install it from [here](https://code.visualstudio.com/).\\r\\n  3. **Azure Functions Extension for Visual Studio Code**: This extension provides you with a seamless experience for developing Azure Functions. It can be installed from the [VS Code marketplace](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions).\\r\\n  4. **C# Dev Kit**: Since we\'re writing our Azure Functions in C#, this extension is necessary for getting C# support in VS Code. You can install it from the [VS Code marketplace](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit).\\r\\n  5. **Azure CLI**: The Azure Command-Line Interface (CLI) will be used to create and manage Azure resources from the command line. For installation instructions, visit the Azure CLI installation [documentation page](https://docs.microsoft.com/cli/azure/install-azure-cli?ocid=buildia24_60days_blogs).\\r\\n  6. **Postman**: Although not strictly necessary, Postman is a handy tool for testing our HTTP-triggered Azure Functions without having to write a front-end application. You can download Postman from [getpostman.com](https://www.getpostman.com/).\\r\\n\\r\\nWith the prerequisites in place, you\'re ready to set up your development environment, which we will cover in the following section.\\r\\n\\r\\n:::info\\r\\nRegister for **[Episode 4](https://aka.ms/serverless-learn-live/ep4?ocid=buildia24_60days_blogs)**\u202fof the new hands-on live learning series with an SME **on\u202fIntelligent Apps with Serverless on Azure**.\\r\\n:::\\r\\n\\r\\n### Creating Resources\\r\\n\\r\\nTo get started with building a multichannel notification system, we\'ll need to create several resources within Azure. This section will walk you through setting up your Azure environment using the Azure CLI. Ensure that you have the Azure CLI installed on your machine and that you\'re logged into your Azure account.\\r\\n\\r\\n#### Azure Communication Services\\r\\n\\r\\n  1. Azure Communication Services (ACS) provides the backbone for our notification system, allowing us to send SMS, Email, and WhatsApp messages. The steps below create resources for all three communication channels. However, you can choose one or more depending upon your preference. **Log in to Azure**:\\r\\n\\r\\n`bash`\\r\\n  \\r\\n```\\r\\naz login \\r\\n```\\r\\n\\r\\n  2. **Create a Resource Group (if necessary)**: This groups all your resources in one collection.\\r\\n\\r\\n`bash`\\r\\n\\r\\n```\\r\\naz group create --name <YourResourceGroupName> --location <PreferredLocation>\\r\\n```\\r\\nReplace `<YourResourceGroupName>` with a name for your new resource group and `<PreferredLocation>` with the Azure region you prefer (e.g., eastus).\\r\\n\\r\\n  3. **Create ACS Resource**: This will be the main ACS resource where we manage communications capabilities.\\r\\n\\r\\n`bash`\\r\\n\\r\\n```\\r\\naz communication create --name <YourACSResourceName> --location Global --data-location UnitedStates --resource-group <YourResourceGroupName>\\r\\n```\\r\\n\\r\\nReplace `<YourACSResourceName>` with a unique name for your ACS resource and `<YourResourceGroupName>` with the name of your resource group.\\r\\n\\r\\nAfter creating the resource, retrieve the connection string as you will need it to connect your Azure Function to ACS. Copy the one marked as primary.\\r\\n\\r\\n`bash`\\r\\n\\r\\n```\\r\\naz communication list-key --name <YourACSResourceName> --resource-group <YourResourceGroupName>\\r\\n```\\r\\n\\r\\n#### Azure Communication Services for Email\\r\\n\\r\\nTo set up Azure Communication Services Email, you\'ll need to follow a few steps in the Azure Portal:\\r\\n\\r\\n  1. **Create the Email Communications Service resource using the portal**: Provision a new Email Communication Services resource in [Azure portal](https://portal.azure.com/) using the instructions [here](https://learn.microsoft.com/azure/communication-services/quickstarts/email/create-email-communication-resource?ocid=buildia24_60days_blogs). Make sure to select the same resource group as your ACS resource.\\r\\n\\r\\n![image of Email Communication Services in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-3.png)\\r\\n\\r\\n![image of Email Communication Services resource fields in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-4.png)\\r\\n\\r\\n  2. **Configure the Email Communications Service**: You will need to configure domains and sender authentication for email. Provision an [Azure Managed Domain](https://learn.microsoft.com/azure/communication-services/quickstarts/email/add-azure-managed-domains?ocid=buildia24_60days_blogs) or set up your [Custom Verified Domain](https://learn.microsoft.com/azure/communication-services/quickstarts/email/add-custom-verified-domains?ocid=buildia24_60days_blogs) depending on your use case.\\r\\n\\r\\n![image of Email Communication Services configurations in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-5.png)\\r\\n\\r\\n#### Azure Communication Services for SMS\\r\\n\\r\\nTo send SMS messages, you will need to acquire a phone number through ACS. The phone number has a cost associated with it. If you want to avoid that, continue with Email and WhatsApp only.\\r\\n\\r\\n  1. **Get a Phone Number**: Navigate to the **Phone Numbers** blade in your ACS resource on the [Azure portal](https://portal.azure.com/) and follow the steps to [get a phone number](https://learn.microsoft.com/azure/communication-services/quickstarts/telephony/get-phone-number?ocid=buildia24_60days_blogs) that\'s capable of sending and receiving SMS.\\r\\n\\r\\n![image of Email Communication Services phone number features in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-6.png)\\r\\n\\r\\n  2. **Note the Phone Number**: After acquiring a phone number, note it down as it will be used to send SMS messages from your Azure Function.\\r\\n\\r\\n![image of Email Communication Services resource once configured in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-7.png)\\r\\n\\r\\n#### WhatsApp for Business\\r\\n\\r\\nSending WhatsApp messages requires setting up a WhatsApp Business account. \\r\\n\\r\\n  1. **Set up a WhatsApp Business Account**: Follow the instructions for connecting a [WhatsApp business account](https://learn.microsoft.com/azure/communication-services/quickstarts/advanced-messaging/whatsapp/connect-whatsapp-business-account?ocid=buildia24_60days_blogs) with Azure Communication Services.\\r\\n  2. **Note the WhatsApp Configuration**: Once set up, make a note of the necessary configuration details such as the phone number and WhatsApp Business API credentials, as they will be needed in your Azure Function.\\r\\n\\r\\n![image of Email Communication Services configuration channels in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-8.png)\\r\\n\\r\\nBy following these steps, you will have created the necessary resources to build a multichannel notification system that can reach users through SMS, Email, and WhatsApp. Next, we\'ll proceed with setting up your Azure Function and integrating these services into it.\\r\\n\\r\\n### Setting Up Environment\\r\\n\\r\\nWith the prerequisites out of the way, let\'s prepare our environment to develop our multichannel notification system using Azure Functions and Azure Communication Services.\\r\\n\\r\\n#### Creating the Function App Project\\r\\n\\r\\nOpen Visual Studio Code and follow these steps to create a new Azure Functions project:\\r\\n\\r\\n  1. Click on the Azure icon in the Activity Bar on the side of Visual Studio Code to open the Azure Functions extension.\\r\\n  2. In the Azure Functions extension, click on the \'Create New Project\' icon, choose a directory for your project, and select \'Create New Project Here\'.\\r\\n\\r\\n![mage of a new Azure Functions project in Visual Studio Code](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-9.png)\\r\\n\\r\\n  3. Choose the language for your project. We will select C# for this tutorial.\\r\\n  4. Select the template for your first function. For this project, an HTTP-triggered function is a good starting point since we want to receive HTTP requests to send out notifications.\\r\\n  5. Provide a function name, such as `EmailTrigger`, and set the authorization level to anonymous or function, depending on your security preference.\\r\\n\\r\\nAfter you have completed these steps, your Azure Functions project will be set up with all the necessary files in the chosen directory.\\r\\n\\r\\n#### Installing the Necessary Packages\\r\\n\\r\\nNow it\u2019s time to add the packages necessary for integrating Azure Communication Services:\\r\\n\\r\\n  1. Open the integrated terminal in Visual Studio Code by clicking on \'Terminal\' in the top menu and then selecting \'New Terminal\'.\\r\\n  2. Add the Azure Communication Services packages to your project:\\r\\n\\r\\n`bash`\\r\\n```\\r\\ndotnet add package Azure.Communication.Email\\r\\ndotnet add package Azure.Communication.Sms\\r\\ndotnet add package Azure.Communication.Messages --prerelease\\r\\n```\\r\\n\\r\\n#### Setting Up Environment Variables\\r\\n\\r\\nYou should store configuration details like connection strings and phone numbers as environment variables instead of hardcoding them into your functions. To do so in Azure Functions, add them to the `local.settings.json` file, which is used for local development.\\r\\n\\r\\nEdit the `local.settings.json` file to include your Azure Communication Services (ACS) connection string and phone numbers:\\r\\n\\r\\n`json`\\r\\n```\\r\\n{ \\r\\n  \\"IsEncrypted\\": false,\\r\\n  \\"Values\\": {\\r\\n    \\"AzureWebJobsStorage\\": \\"\\",\\r\\n    \\"FUNCTIONS_WORKER_RUNTIME\\": \\"dotnet\\",\\r\\n    \\"COMMUNICATION_SERVICES_CONNECTION_STRING\\": \\"<acs_connection_string>\\",\\r\\n    \\"SENDER_PHONE_NUMBER\\": \\"<acs_sms_phone_number>\\",\\r\\n    \\"WHATSAPP_NUMBER\\": \\"<acs_whatsapp_number>\\", \\r\\n    \\"SENDER_EMAIL_ADDRESS\\": \\"<acs_email_address>\\"\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nBe sure to replace `<acs_connection_string>`, `<acs_sms_phone_number>`, `<acs_whatsapp_number>`, and `<acs_email_address>` with your actual Azure storage account connection string, Azure Communication Services connection string, SMS phone number, WhatsApp number, and sending email address. \\r\\n\\r\\nRemember not to commit the `local.settings.json` file to source control if it contains sensitive information. Configure similar settings in the Application Settings for your Azure Function when you deploy to Azure.\\r\\n\\r\\n### Coding the EmailTrigger\\r\\n\\r\\nCreating a functional `EmailTrigger` Azure Function involves starting from the default template provided by Azure Functions for C# and enhancing it with the necessary logic and services to handle email sending. In this section, we guide you through the steps to transform the default template into the finished `EmailTrigger` function.\\r\\n\\r\\n#### Step 1: Set Up the Function Template\\r\\n\\r\\nStart by using the default HTTP triggered function template provided by Visual Studio Code for creating an Azure Functions project. It will have the necessary usings, function name attribute, and a simple HTTP trigger that returns a welcome message. Select your project in the Workspace pane and click on the \'Create Function\' button in the Azure Functions extension. Choose \'HTTP trigger\' as the template and provide a name for the function, such as `EmailTrigger`. Set the authorization level to anonymous or function, depending on your security preference.\\r\\n\\r\\n![image of creating a new function in Visio Studio Code](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-10.png)\\r\\n\\r\\n#### Step 2: Add Azure Communication Services Email Reference\\r\\n\\r\\nAdd a reference to using Azure.Communication.Email then create a property in the EmailTrigger class to hold an instance of EmailClient and a property to hold the email sender address.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nprivate readonly EmailClient _emailClient;\\r\\nprivate string? sender = Environment.GetEnvironmentVariable(\\"SENDER_EMAIL_ADDRESS\\");\\r\\n```\\r\\n\\r\\n#### Step 3: Read Configuration and Initialize EmailClient\\r\\n\\r\\nWithin the `EmailTrigger` class constructor, read the Azure Communication Services connection string from the environment variables using `Environment.GetEnvironmentVariable()` method and initialize an instance of `EmailClient` with the connection string.\\r\\n\\r\\nMake sure to handle the possibility that the environment variable may be null and throw an appropriate exception if it is not set.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nstring? connectionString = Environment.GetEnvironmentVariable(\\"COMMUNICATION_SERVICES_CONNECTION_STRING\\");\\r\\nif (connectionString is null)\\r\\n{\\r\\n    throw new InvalidOperationException(\\"COMMUNICATION_SERVICES_CONNECTION_STRING environment variable is not set.\\");\\r\\n}\\r\\n_emailClient = new EmailClient(connectionString);\\r\\n```\\r\\n\\r\\n#### Step 4: Define the Request Model\\r\\n\\r\\nCreate a request model class `EmailRequest` inside the `EmailTrigger` class to represent the expected payload. This model includes the subject, HTML content, and recipient email address.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\npublic class EmailRequest \\r\\n\\r\\n{\\r\\n    public string Subject { get; set; } = string.Empty;\\r\\n    public string HtmlContent { get; set; } = string.Empty;\\r\\n    public string Recipient { get; set; } = string.Empty;\\r\\n}\\r\\n```\\r\\n\\r\\n#### Step 5: Parse the Request Body\\r\\n\\r\\nModify the `Run` function to be async since we\'ll be performing asynchronous operations.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\npublic async Task<IActionResult> Run([HttpTrigger(AuthorizationLevel.Anonymous, \\"post\\")] HttpRequest req)\\r\\n```\\r\\n\\r\\nUse `StreamReader` to read the request body and deserialize it into the `EmailRequest` object using `System.Text.Json.JsonSerializer`. \\r\\n\\r\\nHandle the case where the deserialization fails by returning a `BadRequestResult`.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nstring requestBody = await new StreamReader(req.Body).ReadToEndAsync();\\r\\nEmailRequest? data = JsonSerializer.Deserialize<EmailRequest>(requestBody, new JsonSerializerOptions() {\\r\\n                PropertyNamingPolicy = JsonNamingPolicy.CamelCase\\r\\n            });\\r\\nif (data is null)\\r\\n{\\r\\n    return new BadRequestResult();\\r\\n}\\r\\n```\\r\\n\\r\\n#### Step 6: Define the Sender and Send the Email\\r\\n\\r\\nInstantiate a sender email address string that will be passed to the `SendAsync` method of the EmailClient instance. Replace the static email \'DoNotReply@effaa622-a003-4676-b27e-6b9e7a783581.azurecomm.net\' with your configured sender address in the actual implementation. \\r\\n\\r\\nUse a try-catch block to send the email using the `SendAsync` method and catch any `RequestFailedException` to log any errors.\\r\\n\\r\\n`csharp`\\r\\n\\r\\n```\\r\\n_logger.LogInformation(\\"Sending email...\\");\\r\\nEmailSendOperation emailSendOperation = await _emailClient.SendAsync(\\r\\n    Azure.WaitUntil.Completed,\\r\\n    sender,\\r\\n    data.Recipient,\\r\\n    data.Subject,\\r\\n    data.HtmlContent\\r\\n); \\r\\n\\r\\n_logger.LogInformation($\\"Email Sent. Status = {emailSendOperation.Value.Status}\\");\\r\\n_logger.LogInformation($\\"Email operation id = {emailSendOperation.Id}\\");\\r\\n```\\r\\n\\r\\n#### Step 7: Return a Success Response\\r\\n\\r\\nOnce the email send operation is completed, return an `OkObjectResult` indicating the success of the operation.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nreturn new OkObjectResult(\\"Email sent successfully!\\");\\r\\n}\\r\\n```\\r\\n\\r\\n#### Final Code\\r\\n\\r\\nAfter completing all the above steps, your `EmailTriggerAzure Function` should look as follows:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nusing System;\\r\\nusing System.IO;\\r\\nusing System.Text.Json;\\r\\nusing System.Threading.Tasks;\\r\\nusing Azure;\\r\\nusing Azure.Communication.Email;\\r\\nusing Microsoft.AspNetCore.Mvc;\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Extensions.Logging;\\r\\n\\r\\nnamespace ACSGPTFunctions\\r\\n{ \\r\\n    public class EmailTrigger\\r\\n    { \\r\\n        private readonly ILogger<EmailTrigger> _logger;\\r\\n        private readonly EmailClient _emailClient;\\r\\n\\r\\n        public EmailTrigger(ILogger<EmailTrigger> logger)\\r\\n        {\\r\\n            _logger = logger;\\r\\n            string? connectionString = Environment.GetEnvironmentVariable(\\"COMMUNICATION_SERVICES_CONNECTION_STRING\\");\\r\\n            if (connectionString is null)\\r\\n            {\\r\\n                throw new InvalidOperationException(\\"COMMUNICATION_SERVICES_CONNECTION_STRING environment variable is not set.\\");\\r\\n            }\\r\\n            _emailClient = new EmailClient(connectionString);\\r\\n        } \\r\\n\\r\\n        public class EmailRequest\\r\\n        {\\r\\n            public string Subject { get; set; } = string.Empty;\\r\\n            public string HtmlContent { get; set; } = string.Empty;\\r\\n            public string Recipient { get; set; } = string.Empty;\\r\\n        }\\r\\n\\r\\n        [Function(\\"EmailTrigger\\")]\\r\\n        public async Task<IActionResult> Run([HttpTrigger(AuthorizationLevel.Anonymous, \\"post\\")] HttpRequest req) \\r\\n        {\\r\\n            _logger.LogInformation(\\"Processing request.\\");\\r\\n\\r\\n            string requestBody = await new StreamReader(req.Body).ReadToEndAsync();\\r\\n            EmailRequest? data = JsonSerializer.Deserialize<EmailRequest>(requestBody, new JsonSerializerOptions() {\\r\\n                PropertyNamingPolicy = JsonNamingPolicy.CamelCase\\r\\n            });\\r\\n\\r\\n            if (data is null)\\r\\n            {\\r\\n                return new BadRequestResult();\\r\\n            }\\r\\n\\r\\n            var sender = \\"DoNotReply@effaa622-a003-4676-b27e-6b9e7a783581.azurecomm.net\\";\\r\\n\\r\\n            try \\r\\n            {\\r\\n                _logger.LogInformation(\\"Sending email...\\");\\r\\n                EmailSendOperation emailSendOperation = await _emailClient.SendAsync(\\r\\n                    Azure.WaitUntil.Completed,\\r\\n                    sender,\\r\\n                    data.Recipient,\\r\\n                    data.Subject,\\r\\n                    data.HtmlContent\\r\\n                );\u202f \\r\\n\\r\\n                _logger.LogInformation($\\"Email Sent. Status = {emailSendOperation.Value.Status}\\");\\r\\n                _logger.LogInformation($\\"Email operation id = {emailSendOperation.Id}\\");\\r\\n            }\\r\\n            catch (RequestFailedException ex)\\r\\n            {\\r\\n                _logger.LogInformation($\\"Email send operation failed with error code: {ex.ErrorCode}, message: {ex.Message}\\");\\r\\n                return new ObjectResult(new { error = ex.Message }) { StatusCode = 500 };\\r\\n            }\\r\\n\\r\\n            return new OkObjectResult(\\"Email sent successfully!\\");\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThis completed `EmailTriggerAzure` Function is now ready to be part of a multichannel notification system, handling the email communication channel.\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nContinue to the [next part](https://azure.github.io/Cloud-Native/60DaysOfIA/building-a-multichannel-notification-system-2) of this topic to further explore building, deploying and testing your intelligent app for a multichannel notification system."},{"id":"building-a-multichannel-notification-system-2","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/building-a-multichannel-notification-system-2","source":"@site/blog-60daysofIA/2024-04-01/building-a-multichannel-notification-system-2.md","title":"6.5 Building a Multichannel Notification System (2)","description":"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services. ","date":"2024-04-01T09:01:00.000Z","formattedDate":"April 1, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":16.29,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-04-01T09:01","slug":"building-a-multichannel-notification-system-2","title":"6.5 Building a Multichannel Notification System (2)","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services. ","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"6.4 Building a Multichannel Notification System (1)","permalink":"/Cloud-Native/60DaysOfIA/building-a-multichannel-notification-system-1"},"nextItem":{"title":"7.1 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 1","permalink":"/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-1"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/building-a-multichannel-notification-system-2\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/building-a-multichannel-notification-system-2\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/building-a-multichannel-notification-system-2\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n## Building a Multichannel Notification System with Azure Functions and Azure Communication Services\u202f(2)\\r\\n\\r\\nIn the [first part](https://azure.github.io/Cloud-Native/60DaysOfIA/building-a-multichannel-notification-system-1) of this topic, we setup all the Azure resources like the Azure Communication Services for Email, SMS, WhatsApp for Business and developed the Azure Functions code for the email trigger. In this second part, we will complete coding the remaining Azure Functions triggers and then go ahead to deploy the multichannel notification system to Azure Functions, testing the Email, SMS, and WhatsApp triggers with OpenAI GPTs. Let\u2019s get started!\\r\\n\\r\\n### Prerequisite\\r\\n\\r\\nTo follow this tutorial, ensure you have completed the\u202ffirst part of this topic.\\r\\n\\r\\n### Coding the SMSTrigger\\r\\n\\r\\nEnhancing the `SMSTrigger` Azure Function from the default template involves a series of steps. These steps will transform the basic Function into one that can send SMS messages using Azure Communication Services. Below is a guide to get you from the default HTTP triggered function to the finished `SMSTrigger`.\\r\\n\\r\\n:::info\\r\\nJoin the Azure Functions product group for an\u202f**[Ask The Expert](https://aka.ms/intelligent-apps/ate-functions?ocid=buildia24_60days_blogs)**\u202fsession on how to build intelligent apps with serverless, top use cases to explore and what is coming next for the Era of AI.\\r\\n:::\\r\\n\\r\\n#### Step 1: Set Up the Function Template\\r\\n\\r\\nFollow the instructions for setting up the function template from the Email section and name the trigger as \u2018SMSTrigger\u2019 or any other string you prefer.\\r\\n\\r\\n#### Step 2: Add Azure Communication Services SMS Reference\\r\\n\\r\\nAdd a reference to using `Azure.Communication.Sms` then create a property in the SMS Trigger class to hold an instance of `SmsClient` and a property to hold the email sender address.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nprivate readonly SmsClient _smsClient;\\r\\nprivate string? sender = Environment.GetEnvironmentVariable(\\"SENDER_PHONE_NUMBER\\");\\r\\n```\\r\\n\\r\\n#### Step 3: Read Configuration and Initialize SmsClient\\r\\n\\r\\nIn the constructor of the `SMSTrigger` class, read the Azure Communication Services connection string from the environment variables using the `Environment.GetEnvironmentVariable()` method and initialize the `SmsClient` instance.\\r\\n\\r\\nBe sure to check if the connection string is null, and if so, throw an exception to indicate that the environment variable is missing:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nstring? connectionString = Environment.GetEnvironmentVariable(\\"COMMUNICATION_SERVICES_CONNECTION_STRING\\");\\r\\nif (connectionString is null)\\r\\n{\\r\\n    throw new InvalidOperationException(\\"COMMUNICATION_SERVICES_CONNECTION_STRING environment variable is not set.\\");\\r\\n}\\r\\n_smsClient = new SmsClient(connectionString);\\r\\n```\\r\\n\\r\\n#### Step 4: Define the Request Model\\r\\n\\r\\nCreate a request model class within the `SMSTrigger` class called `SmsRequest`. This model should contain properties for the message text and the phone number to which the message will be sent.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\npublic class SmsRequest\\r\\n{\\r\\n    public string Message { get; set; } = string.Empty;\\r\\n    public string PhoneNumber { get; set; } = string.Empty;\\r\\n}\\r\\n```\\r\\n\\r\\n#### Step 5: Parse the Request Body\\r\\n\\r\\nChange the `Run` function to be `async` as we will perform asynchronous operations. Use a `StreamReader` to read the request body as a string and deserialize it into an `SmsRequest` object using `JsonSerializer`.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\npublic async Task<IActionResult> Run([HttpTrigger(AuthorizationLevel.Anonymous, \\"post\\")] HttpRequest req)\\r\\n```\\r\\n\\r\\nIf the request body fails to deserialize into `SmsRequest`, return a `BadRequestResult`:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nstring requestBody = await new StreamReader(req.Body).ReadToEndAsync();\\r\\nSmsRequest? data = JsonSerializer.Deserialize<SmsRequest>(requestBody, new JsonSerializerOptions() {\\r\\n                PropertyNamingPolicy = JsonNamingPolicy.CamelCase\\r\\n            });\\r\\nif (data is null)\\r\\n{\\r\\n    return new BadRequestResult();\\r\\n}\\r\\n```\\r\\n\\r\\n#### Step 6: Define the Sender and Send an SMS\\r\\n\\r\\nRetrieve the sender\'s phone number from the environment variables with `Environment.GetEnvironmentVariable()`. Then, attempt to send the SMS with a try-catch block, handling any `RequestFailedException` that may occur and logging the relevant information:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\ntry\\r\\n{\\r\\n    _logger.LogInformation(\\"Sending SMS...\\");\\r\\n    SmsSendResult smsSendResult = await _smsClient.SendAsync(\\r\\n        sender,\\r\\n        data.PhoneNumber,\\r\\n        data.Message\\r\\n    );\\r\\n    _logger.LogInformation($\\"SMS Sent. Successful = {smsSendResult.Successful}\\");\\r\\n    _logger.LogInformation($\\"SMS operation id = {smsSendResult.MessageId}\\");\\r\\n}\\r\\ncatch (RequestFailedException ex)\\r\\n{\\r\\n    _logger.LogInformation($\\"SMS send operation failed with error code: {ex.ErrorCode}, message: {ex.Message}\\");\\r\\n    // Return an appropriate error response if needed\\r\\n}\\r\\n```\\r\\n\\r\\n#### Step 7: Return a Success Response\\r\\n\\r\\nIf sending the SMS is successful, return an `OkObjectResult` to the caller indicating that the SMS has been sent.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nreturn new OkObjectResult(\\"SMS sent successfully!\\");\\r\\n```\\r\\n\\r\\n#### Final Code\\r\\n\\r\\nThe final `SMSTrigger` Azure Function, with the steps implemented, should look as follows:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nusing Microsoft.AspNetCore.Http;\\r\\nusing Microsoft.AspNetCore.Mvc;\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Extensions.Logging; \\r\\n\\r\\nusing Azure;\\r\\nusing Azure.Communication.Messages;\\r\\nusing System.Text.Json;\\r\\nusing System.IO;\\r\\nusing System.Threading.Tasks;\\r\\nusing System.Linq;\\r\\nusing System.Collections.Generic; \\r\\n\\r\\nnamespace ACSGPTFunctions\\r\\n{\\r\\n    public class WhatsAppTrigger\\r\\n    {\\r\\n        private readonly ILogger<WhatsAppTrigger> _logger;\\r\\n        private readonly NotificationMessagesClient _messagesClient;\\r\\n        private string? sender = Environment.GetEnvironmentVariable(\\"WHATSAPP_NUMBER\\");\\r\\n\\r\\n        public WhatsAppTrigger(ILogger<WhatsAppTrigger> logger)\\r\\n        {\\r\\n            _logger = logger;\\r\\n            string? connectionString = Environment.GetEnvironmentVariable(\\"COMMUNICATION_SERVICES_CONNECTION_STRING\\");\\r\\n            if (connectionString is null)\\r\\n            {\\r\\n                throw new InvalidOperationException(\\"COMMUNICATION_SERVICES_CONNECTION_STRING environment variable is not set.\\");\\r\\n            }\\r\\n            _messagesClient = new NotificationMessagesClient(connectionString);\\r\\n        }\\r\\n\\r\\n        public class WhatsAppRequest\\r\\n        {\\r\\n            public string PhoneNumber { get; set; } = string.Empty;\\r\\n            public string TemplateName { get; set; } = \\"appointment_reminder\\";\\r\\n            public string TemplateLanguage { get; set; } = \\"en\\";\\r\\n            public List<string> TemplateParameters { get; set; } = new List<string>();\\r\\n        }\\r\\n\\r\\n        [Function(\\"WhatsAppTrigger\\")]\\r\\n        public async Task<IActionResult> Run([HttpTrigger(AuthorizationLevel.Function, \\"get\\", \\"post\\")] HttpRequest req)\\r\\n        {\\r\\n            _logger.LogInformation(\\"Processing request.\\");\\r\\n\\r\\n            string requestBody = await new StreamReader(req.Body).ReadToEndAsync();\\r\\n            WhatsAppRequest? data = JsonSerializer.Deserialize<WhatsAppRequest>(requestBody, new JsonSerializerOptions() {\\r\\n                PropertyNamingPolicy = JsonNamingPolicy.CamelCase\\r\\n            }); \\r\\n\\r\\n            if (data is null)\\r\\n            {\\r\\n                return new BadRequestResult();\\r\\n            } \\r\\n\\r\\n            var recipientList = new List<string> { data.PhoneNumber };\\r\\n            var values = data.TemplateParameters\\r\\n                .Select((parameter, index) => new MessageTemplateText($\\"value{index + 1}\\", parameter))\\r\\n                .ToList();\\r\\n            var bindings = new MessageTemplateWhatsAppBindings(\\r\\n                body: values.Select(value => value.Name).ToList()\\r\\n            );\\r\\n            var template = new MessageTemplate(data.TemplateName, data.TemplateLanguage, values, bindings);\\r\\n            var sendTemplateMessageOptions = new SendMessageOptions(sender, recipientList, template);\\r\\n\\r\\n            try\\r\\n            {\\r\\n                Response<SendMessageResult> templateResponse = await _messagesClient.SendMessageAsync(sendTemplateMessageOptions);\\r\\n                _logger.LogInformation(\\"WhatsApp message sent successfully!\\");\\r\\n            }\\r\\n            catch (RequestFailedException ex)\\r\\n            {\\r\\n                _logger.LogError($\\"WhatsApp send operation failed with error code: {ex.ErrorCode}, message: {ex.Message}\\");\\r\\n                return new ObjectResult(new { error = ex.Message }) { StatusCode = 500 };\\r\\n            }\\r\\n            return new OkObjectResult(\\"WhatsApp sent successfully!\\");\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThis completed `SMSTrigger` Azure Function can now facilitate SMS as part of your multichannel notification system.\\r\\n\\r\\n### Coding the WhatsAppTrigger\\r\\n\\r\\nCreating a functional `WhatsAppTrigger` Azure Function involves iterating on the default HTTP-triggered function template provided by Azure Functions for C#. We will modify this template to integrate Azure Communication Services for sending WhatsApp messages via template messages. Follow the steps below to transform this template into a complete `WhatsAppTrigger` function:\\r\\n\\r\\n#### Step 1: Set Up the Function Template\\r\\n\\r\\nFollow the instructions in the first step for setting up SMS trigger and name the function as `WhatsAppTrigger`. Set the authorization level to anonymous or function, depending on your security preference.\\r\\n\\r\\n#### Step 2: Reference the Azure Communication Services Messages Package\\r\\n\\r\\nEnsure the `Azure.Communication.Messages` NuGet package is included in your project to enable messaging features needed for WhatsApp. Install the package with the following command in Visual Studio Code\u2019s terminal:\\r\\n\\r\\n`bash`\\r\\n```\\r\\ndotnet add package Azure.Communication.Messages --prerelease\\r\\n```\\r\\n\\r\\nAdd a reference to using `Azure.Communication.Messages` then create a property in the `WhatsApp` Trigger class to hold an instance of `NotificationMessagesClient` and a property to hold the WhatsApp identifier.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nprivate readonly NotificationMessagesClient _messagesClient;\\r\\nprivate string? sender = Environment.GetEnvironmentVariable(\\"WHATSAPP_NUMBER\\");\\r\\n```\\r\\n\\r\\n#### Step 3: Read Configuration and Initialize NotificationMessagesClient\\r\\n\\r\\nUpdate the `WhatsAppTrigger` class constructor to read the Azure Communication Services connection string from environment variables using `Environment.GetEnvironmentVariable()` and initialize `NotificationMessagesClient` with this connection string:\\r\\n\\r\\n`csharp`\\r\\n\\r\\n```\\r\\nstring? connectionString = Environment.GetEnvironmentVariable(\\"COMMUNICATION_SERVICES_CONNECTION_STRING\\");\\r\\nif (connectionString is null)\\r\\n{\\r\\n    throw new InvalidOperationException(\\"COMMUNICATION_SERVICES_CONNECTION_STRING environment variable is not set.\\");\\r\\n}\\r\\n_messagesClient = new NotificationMessagesClient(connectionString);\\r\\n```\\r\\n\\r\\n#### Step 4: Define the Request Model\\r\\n\\r\\nCreate a request model class named `WhatsAppRequest` within the `WhatsAppTrigger` class, containing properties for the destination phone number, template name, language, and template parameters:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\npublic class WhatsAppRequest \\r\\n\\r\\n{\\r\\n    public string PhoneNumber { get; set; } = string.Empty;\\r\\n    public string TemplateName { get; set; } = \\"appointment_reminder\\";\\r\\n    public string TemplateLanguage { get; set; } = \\"en\\";\\r\\n    public List<string> TemplateParameters { get; set; } = new List<string>();\\r\\n}\\r\\n```\\r\\n\\r\\n#### Step 5: Parse the Request Body\\r\\n\\r\\nConvert the Run function to be `async` to enable asynchronous work. Use `StreamReader` to read the request body and deserialize it to a `WhatsAppRequest` instance using `System.Text.Json.JsonSerializer` with `JsonNamingPolicy.CamelCase`.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\npublic async Task<IActionResult> Run([HttpTrigger(AuthorizationLevel.Anonymous, \\"post\\")] HttpRequest req)\\r\\n```\\r\\n\\r\\nHandle potential deserialization failure by returning `BadRequestResult`:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nstring requestBody = await new StreamReader(req.Body).ReadToEndAsync(); \\r\\nWhatsAppRequest? data = JsonSerializer.Deserialize<WhatsAppRequest>(requestBody, new JsonSerializerOptions() {\\r\\n    PropertyNamingPolicy = JsonNamingPolicy.CamelCase\\r\\n});\\r\\n\\r\\nif (data is null)\\r\\n{\\r\\n    return new BadRequestResult();\\r\\n}\\r\\n```\\r\\n\\r\\n#### Step 6: Prepare Template Message and Send WhatsApp Message\\r\\n\\r\\nModify the try-catch block to construct a `SendMessageOptions` object using `MessageTemplateWhatsAppBindingsand MessageTemplate`, and then make a call to `_messagesClient.SendMessageAsync(sendTemplateMessageOptions)`:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\ntry\\r\\n{\\r\\n    _logger.LogInformation(\\"Sending WhatsApp message...\\");\\r\\n\\r\\n    List<string> recipientList = new List<string> { data.PhoneNumber };\\r\\n    List<MessageTemplateText> values = data.TemplateParameters\\r\\n        .Select((parameter, index) => new MessageTemplateText($\\"value{index + 1}\\", parameter))\\r\\n        .ToList();\\r\\n    MessageTemplateWhatsAppBindings bindings = new MessageTemplateWhatsAppBindings(\\r\\n        body: values.Select(value => value.Name).ToList()\\r\\n    );\\r\\n    MessageTemplate template = new MessageTemplate(data.TemplateName, data.TemplateLanguage, values, bindings);\\r\\n    SendMessageOptions sendTemplateMessageOptions = new SendMessageOptions(sender, recipientList, template);\\r\\n    Response<SendMessageResult> templateResponse = await _messagesClient.SendMessageAsync(sendTemplateMessageOptions); \\r\\n\\r\\n    _logger.LogInformation(\\"WhatsApp message sent successfully!\\");\\r\\n}\\r\\ncatch (RequestFailedException ex)\\r\\n{\\r\\n    _logger.LogError($\\"WhatsApp send operation failed with error code: {ex.ErrorCode}, message: {ex.Message}\\");\\r\\n    return new ObjectResult(new { error = ex.Message }) { StatusCode = 500 };\\r\\n}\\r\\n```\\r\\n\\r\\n#### Step 7: Return Success Response\\r\\n\\r\\nAfter sending the WhatsApp message successfully, return an OkObjectResult stating \\"WhatsApp sent successfully!\\".\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nreturn new OkObjectResult(\\"WhatsApp sent successfully!\\");\\r\\n```\\r\\n\\r\\n#### Final Code\\r\\n\\r\\nFollowing the described steps, the final `WhatsAppTrigger` Azure Function should look like this:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nusing Microsoft.AspNetCore.Http;\\r\\nusing Microsoft.AspNetCore.Mvc;\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Extensions.Logging;\\r\\n\\r\\nusing Azure;\\r\\nusing Azure.Communication.Messages;\\r\\nusing System.Text.Json;\\r\\nusing System.IO;\\r\\nusing System.Threading.Tasks;\\r\\nusing System.Linq;\\r\\nusing System.Collections.Generic; \\r\\n\\r\\nnamespace ACSGPTFunctions\\r\\n{\\r\\n    public class WhatsAppTrigger\\r\\n    {\\r\\n        private readonly ILogger<WhatsAppTrigger> _logger;\\r\\n        private readonly NotificationMessagesClient _messagesClient;\\r\\n        private string? sender = Environment.GetEnvironmentVariable(\\"WHATSAPP_NUMBER\\"); \\r\\n\\r\\n        public WhatsAppTrigger(ILogger<WhatsAppTrigger> logger)\\r\\n        {\\r\\n            _logger = logger;\\r\\n            string? connectionString = Environment.GetEnvironmentVariable(\\"COMMUNICATION_SERVICES_CONNECTION_STRING\\");\\r\\n            if (connectionString is null)\\r\\n            {\\r\\n                throw new InvalidOperationException(\\"COMMUNICATION_SERVICES_CONNECTION_STRING environment variable is not set.\\");\\r\\n            }\\r\\n            _messagesClient = new NotificationMessagesClient(connectionString);\\r\\n        }\\r\\n\\r\\n        public class WhatsAppRequest\\r\\n        {\\r\\n            public string PhoneNumber { get; set; } = string.Empty;\\r\\n            public string TemplateName { get; set; } = \\"appointment_reminder\\";\\r\\n            public string TemplateLanguage { get; set; } = \\"en\\";\\r\\n            public List<string> TemplateParameters { get; set; } = new List<string>();\\r\\n        }\\r\\n\\r\\n        [Function(\\"WhatsAppTrigger\\")]\\r\\n        public async Task<IActionResult> Run([HttpTrigger(AuthorizationLevel.Function, \\"get\\", \\"post\\")] HttpRequest req)\\r\\n        {\\r\\n            _logger.LogInformation(\\"Processing request.\\");\\r\\n\\r\\n            string requestBody = await new StreamReader(req.Body).ReadToEndAsync();\\r\\n            WhatsAppRequest? data = JsonSerializer.Deserialize<WhatsAppRequest>(requestBody, new JsonSerializerOptions() {\\r\\n                PropertyNamingPolicy = JsonNamingPolicy.CamelCase\\r\\n            });\\r\\n\\r\\n            if (data is null)\\r\\n            {\\r\\n                return new BadRequestResult();\\r\\n            }\\r\\n\\r\\n            var recipientList = new List<string> { data.PhoneNumber };\\r\\n            var values = data.TemplateParameters\\r\\n                .Select((parameter, index) => new MessageTemplateText($\\"value{index + 1}\\", parameter))\\r\\n                .ToList();\\r\\n            var bindings = new MessageTemplateWhatsAppBindings(\\r\\n                body: values.Select(value => value.Name).ToList()\\r\\n            );\\r\\n            var template = new MessageTemplate(data.TemplateName, data.TemplateLanguage, values, bindings);\\r\\n            var sendTemplateMessageOptions = new SendMessageOptions(sender, recipientList, template); \\r\\n\\r\\n            try\\r\\n            {\\r\\n                Response<SendMessageResult> templateResponse = await _messagesClient.SendMessageAsync(sendTemplateMessageOptions);\\r\\n                _logger.LogInformation(\\"WhatsApp message sent successfully!\\");\\r\\n            }\\r\\n            catch (RequestFailedException ex)\\r\\n            {\\r\\n                _logger.LogError($\\"WhatsApp send operation failed with error code: {ex.ErrorCode}, message: {ex.Message}\\");\\r\\n                return new ObjectResult(new { error = ex.Message }) { StatusCode = 500 };\\r\\n            }\\r\\n            return new OkObjectResult(\\"WhatsApp sent successfully!\\");\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThe `WhatsAppTrigger` Azure Function is now ready to send WhatsApp template messages. Be sure to test it extensively and remember to handle any issues related to input validation and communicate with the Azure Communication Services API correctly.\\r\\n\\r\\n### Deployment and Testing\\r\\n\\r\\nAfter developing the multichannel notification system using Azure Functions, the next step is to deploy and test the functions. This section will guide you through deploying your Azure Function to the cloud and testing the Email, SMS, and WhatsApp triggers.\\r\\n\\r\\n#### Deploying the Azure Function\\r\\n\\r\\nDeployment of your Azure Function can be done right from Visual Studio Code with the Azure Functions extension.\\r\\n\\r\\n  1. **Publish the Function App**: In Visual Studio Code, sign in to Azure if you haven\'t already. In the Azure Functions extension tab, find the \'Deploy to Function App...\' button and select it. \\r\\n\\r\\n![image of the Deploy to Function App button](../../static/img/60-days-of-ia/blogs/2024-04-01/6-5-1.png)\\r\\n\\r\\n  1. **Choose Your Function App**: You can either create a new Function App or deploy it to an existing one. If it\'s the first time you are deploying, choose \'Create New Function App in Azure...\'.\\r\\n  2. **Set the Configuration**: Provide a unique name for your Function App, select a runtime stack (.NET Core in this case), choose the appropriate region, and confirm your selections.\\r\\n  3. **Wait for Deployment**: The deployment process will take a few minutes. Monitor the output window for completion status and any potential errors.\\r\\n\\r\\n#### Set Up Application Settings\\r\\n\\r\\nfter deployment, you need to configure the application settings (environment variables) in Azure.\\r\\n\\r\\n  1. **Open the Function App**: Navigate to the [Azure Portal](https://portal.azure.com/), and find your Function App under \'All Resources\' or by searching the name you provided.\\r\\n  2. **Access Application Settings**: In the Function App\'s menu, go to \'Configuration\' under the \'Settings\' section.\\r\\n  3. **Add the Settings**: Click on \'New application setting\' and add the key-value pairs for the environment variables specified in your `local.settings.json`: `COMMUNICATION_SERVICES_CONNECTION_STRING`, `SENDER_EMAIL_ADDRESS`, `SENDER_PHONE_NUMBER`, `WHATSAPP_NUMBER`, etc.,\\r\\n\\r\\n`json`\\r\\n```\\r\\n{\\r\\n  \\"IsEncrypted\\": false,\\r\\n  \\"Values\\": {\\r\\n    \\"AzureWebJobsStorage\\": \\"\\",\\r\\n    \\"FUNCTIONS_WORKER_RUNTIME\\": \\"dotnet-isolated\\",\\r\\n    \\"COMMUNICATION_SERVICES_CONNECTION_STRING\\": \\"<<connection string>>\\",\\r\\n    \\"SENDER_PHONE_NUMBER\\": \\"<<phone number>>\\",\\r\\n    \\"SENDER_EMAIL_ADDRESS\\": \\"<<email address>>\\",\\r\\n    \\"WHATSAPP_NUMBER\\":\\"<<WhatsApp id>>\\"\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\n  4. **Save and Restart**: After adding the required settings, make sure to save the configurations and restart the Function App to ensure the new settings take effect.\\r\\n\\r\\nAlternatively, when the Function has finished deploying, you can click on \'Upload settings\' to upload your settings from local.settings.json. Don\'t forget to restart the Function App after uploading the settings.\\r\\n\\r\\n![image of stream logs settings selection](../../static/img/60-days-of-ia/blogs/2024-04-01/6-5-2.png)\\r\\n\\r\\n#### Testing the Function\\r\\n\\r\\nWith the deployment complete and the environment configured, it\'s time to verify that your function works as intended through each communication channel.\\r\\n\\r\\n##### Testing Email Notifications\\r\\n\\r\\nTo test the `EmailTrigger` function:\\r\\n\\r\\n  1. **Send an HTTP POST Request**: Use a tool like Postman to send a POST request to the Function App\'s URL suffixed with `/api/EmailTrigger`. The body should contain JSON with keys for `subject`, `htmlContent`, and `recipient`.\\r\\n  2. **Verify Email Receipt**: Check the recipient\'s email inbox for the message. Ensure that the subject and content match what you sent through the POST request.\\r\\n\\r\\n##### Testing SMS Notifications\\r\\n\\r\\nTo test the `SMSTrigger` function:\\r\\n\\r\\n  1. **Send an HTTP POST Request**: Using Postman, send a POST request to the Function App\'s URL with `/api/SMSTrigger` at the end. The body of your request should contain JSON with `message` and `phoneNumberkeys`. \\r\\n  2. **Check for SMS**: Ensure that the specified phone number receives the SMS and the message content matches the request.\\r\\n\\r\\n##### Testing WhatsApp Notifications\\r\\n\\r\\nTo test the `WhatsAppTrigger` function:\\r\\n\\r\\n  1. **Send an HTTP POST Request**: Use Postman again to POST to the Function URL, this time ending with `/api/WhatsAppTrigger`. Include a JSON body with keys for `phoneNumber`, `templateName`, `templateLanguage`, and `templateParameters`.\\r\\n  2. **Confirm WhatsApp Message**: Verify that the WhatsApp message reaches the intended recipient with correct template filling.\\r\\n\\r\\n### Integrate with OpenAI GPTs\\r\\n\\r\\nIn [OpenAI GPTs editor](https://chat.openai.com/gpts/editor), click \'new GPT\' and \'configure\'. Name it \\"Email Sender\\" and set the description and instructions as mentioned.\\r\\n\\r\\n```\\r\\nCompose wonderful emails and send them\\r\\n\\r\\nHelp author short and delightful emails. Ask for details on the nature of the email content and include creative ideas for topics. Compose the email with placeholders for the sender\'s name and receiver\'s name. You do not need a full name. Share a draft of the email and ask for the sender\'s name, and the receiver\'s name and email address. Provide a draft of the final email and confirm the user is happy with it. When the user provides a recipient\'s email address ask if it is correct before sending. Do not send the email until you provide a final draft and you have a confirmed recipient email address.\\r\\n```\\r\\n\\r\\n![image of email composer in Chat GPT configuration](../../static/img/60-days-of-ia/blogs/2024-04-01/6-5-3.png)\\r\\n\\r\\n### Add Actions and JSON Schema\\r\\n\\r\\nClick \'Create new action\' in your GPT configuration. Enter the following JSON:\\r\\n\\r\\n`json`\\r\\n```\\r\\n{\\r\\n  \\"openapi\\": \\"3.1.0\\",\\r\\n  \\"info\\": {\\r\\n    \\"title\\": \\"Send Message API\\",\\r\\n    \\"description\\": \\"API for sending a message to a specified email address.\\",\\r\\n    \\"version\\": \\"v1.0.0\\"\\r\\n  },\\r\\n  \\"servers\\": [\\r\\n    {\\r\\n      \\"url\\": \\"https://<<function-app-url>>.azurewebsites.net\\"\\r\\n    }\\r\\n  ],\\r\\n   \\"paths\\": {\\r\\n    \\"/api/emailtrigger\\": {\\r\\n      \\"post\\": {\\r\\n        \\"description\\": \\"Send a message to a given email address\\",\\r\\n        \\"operationId\\": \\"SendMessage\\",\\r\\n        \\"requestBody\\": {\\r\\n          \\"required\\": true,\\r\\n          \\"content\\": {\\r\\n            \\"application/json\\": {\\r\\n              \\"schema\\": {\\r\\n                \\"type\\": \\"object\\",\\r\\n                \\"properties\\": {\\r\\n                  \\"recipient\\": {\\r\\n                    \\"type\\": \\"string\\",\\r\\n                    \\"format\\": \\"email\\",\\r\\n                    \\"description\\": \\"Email address of the recipient\\"\\r\\n                  },\\r\\n                  \\"subject\\": {\\r\\n                    \\"type\\": \\"string\\",\\r\\n                    \\"description\\": \\"The message subject\\"\\r\\n                  },\\r\\n                  \\"htmlContent\\": {\\r\\n                    \\"type\\": \\"string\\",\\r\\n                    \\"description\\": \\"The body content of the email encoded as escaped HTML\\"\\r\\n                  }\\r\\n                },\\r\\n                \\"required\\": [\\r\\n                  \\"to\\",\\r\\n                  \\"message\\"\\r\\n                ]\\r\\n              }\\r\\n            }\\r\\n          }\\r\\n        },\\r\\n        \\"deprecated\\": false\\r\\n      }\\r\\n    }\\r\\n  },\\r\\n  \\"components\\": {\\r\\n    \\"schemas\\": {}\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nLeave Authentication to none, and Privacy Policy blank.\\r\\n\\r\\n### Test Your GPT\\r\\n\\r\\nFinally, try out your GPT in the preview pane to see it in action!\\r\\n\\r\\nBy following these steps, you can easily integrate Azure Communication Services with OpenAI GPTs to send emails effortlessly.\\r\\n\\r\\n![image of email composer in Chat GPT results](../../static/img/60-days-of-ia/blogs/2024-04-01/6-5-4.png)\\r\\n\\r\\n### Conclusion and Further Reading\\r\\n\\r\\nWe have successfully walked through the journey of building a serverless multichannel notification system using Azure Functions and Azure Communication Services. This system can send timely and personalized notifications across multiple channels, such as Email, SMS, and WhatsApp. In addition, we have explored how to enhance our system with sophisticated content generation capabilities using OpenAI GPTs.\\r\\n\\r\\nThe modular nature of the Azure Functions framework allows your application to scale and adapt easily to changing requirements and traffic demands. Meanwhile, Azure Communication Services enrich the user experience by meeting customers on their preferred platforms, contributing to a seamless and cohesive communication strategy.\\r\\n\\r\\nAs developers, there\'s always room to expand our knowledge and add robust features to our applications. Here are some suggestions for further exploration and resources that can assist you in taking your applications to the next level:\\r\\n\\r\\n  1. **Azure Communication Services AI samples**: One stop shop for GitHub samples for [AI-powered communication solutions](https://aka.ms/acs-ai-index?ocid=buildia24_60days_blogs).\\r\\n  2. **Azure Functions Best Practices**: Learn about best practices for designing and implementing Azure Functions by visiting [Azure Functions best practices](https://docs.microsoft.com/azure/azure-functions/functions-best-practices?ocid=buildia24_60days_blogs).\\r\\n  3. **Azure Communication Services Documentation**: Explore the full capabilities of Azure Communication Services including chat, phone numbers, video calling, and more on the [Azure Communication Services documentation](https://docs.microsoft.com/azure/communication-services/?ocid=buildia24_60days_blogs).\\r\\n  4. **Security and Compliance in Azure**: Understand the best practices for security and compliance in Azure applications, particularly relevant for handling sensitive user communication data. Check the [Microsoft Azure Trust Center](https://azure.microsoft.com/support/trust-center/?ocid=buildia24_60days_blogs).\\r\\n  5. **OpenAI GPT Documentation**: For more insight into using and customizing OpenAI GPTs, refer to the [OpenAI API documentation](https://beta.openai.com/docs/).\\r\\n  6. **Azure AI Services**: Azure offers a range of AI services beyond just communication. Explore Azure AI services for more advanced scenarios such as speech recognition, machine translation, and anomaly detection at [Azure AI services documentation](https://learn.microsoft.com/azure/ai-services/?ocid=buildia24_60days_blogs).\\r\\n  7. **Handling Large-scale Data**: To handle a large amount of data and improve the performance of communication systems, consider learning about Azure\'s data-handling services like Azure Cosmos DB, Azure SQL Database, and Azure Cache for Redis. Start with the [Azure Data storage documentation](https://docs.microsoft.com/azure/storage/?ocid=buildia24_60days_blogs).\\r\\n  8. **Monitoring and Diagnostics**: Improve the reliability of your applications by implementing robust monitoring and diagnostics tools. Azure offers several tools such as Azure Monitor and Application Insights. Dive into [Application Insights for Azure Functions](https://docs.microsoft.com/azure/azure-functions/functions-monitoring?ocid=buildia24_60days_blogs).\\r\\n  9. **Serverless Workflow Automation with Azure Logic Apps**: Enhance your serverless applications using Azure Logic Apps to automate and simplify workflows. Learn more about Azure Logic Apps at [What is Azure Logic Apps?](https://docs.microsoft.com/azure/logic-apps/logic-apps-overview?ocid=buildia24_60days_blogs).\\r\\n\\r\\nWe encourage you to continue exploring and experimenting with Azure services and OpenAI GPTs to build more personalized and efficient communication solutions. Get your hands on the newly released\u202f[Azure Functions Flex Consumption Plan](https://aka.ms/flexconsumption/signup?ocid=buildia24_60days_blogs)\u202ffor private networking, instance size selection, concurrency control, and fast and large scale out features on a serverless compute model. Happy coding!"},{"id":"personalizing-education-with-generative-ai-and-retrieval-augmented-generation-1","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-1","source":"@site/blog-60daysofIA/2024-04-08/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-1.md","title":"7.1 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 1","description":"In this three-part series, you\u2019ll use Azure Container Apps, Azure OpenAI Service, and Retrieval Augmented Generation to create a personal tutor chatbot that dynamically adjusts educational materials and quizzes based on user interactions. This article shows how to set up the core Azure AI services required to build your Intelligent App.","date":"2024-04-08T09:00:00.000Z","formattedDate":"April 8, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":9.595,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-04-08T09:00","slug":"personalizing-education-with-generative-ai-and-retrieval-augmented-generation-1","title":"7.1 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 1","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this three-part series, you\u2019ll use Azure Container Apps, Azure OpenAI Service, and Retrieval Augmented Generation to create a personal tutor chatbot that dynamically adjusts educational materials and quizzes based on user interactions. This article shows how to set up the core Azure AI services required to build your Intelligent App.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"6.5 Building a Multichannel Notification System (2)","permalink":"/Cloud-Native/60DaysOfIA/building-a-multichannel-notification-system-2"},"nextItem":{"title":"7.2 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 2","permalink":"/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-2"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-1\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this three-part series, you\u2019ll use Azure Container Apps, Azure OpenAI Service, and Retrieval Augmented Generation to create a personal tutor chatbot that dynamically adjusts educational materials and quizzes based on user interactions. This article shows how to set up the core Azure AI services required to build your Intelligent App.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-1\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this three-part series, you\u2019ll use Azure Container Apps, Azure OpenAI Service, and Retrieval Augmented Generation to create a personal tutor chatbot that dynamically adjusts educational materials and quizzes based on user interactions. This article shows how to set up the core Azure AI services required to build your Intelligent App.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-1\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Graphic of three blocks, surrounded by a circle in the top right corner. At the bottom of the graphic is text that reads, \\"Personalizing Education with Generative AI and Retrieval Augmented Generation: Laying the Groundwork with Azure Container Apps.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-1.jpeg)\\r\\n\\r\\n## Personalizing Education with Generative AI and Retrieval Augmented Generation Part 1: Laying the Groundwork with Azure Container Apps\\r\\n\\r\\nTired of one-size-fits-all learning? Imagine a virtual tutor that adjusts to your specific needs and helps you understand complex topics. That\u2019s the power of Intelligent Apps \u2014 apps that leverage machine learning (ML), data analytics, and predictive/generative artificial intelligence (AI) to create dynamic, interactive user experiences.\\r\\n\\r\\nIn this three-part series, you\u2019ll build an education app that uses generative AI and Retrieval Augmented Generation (RAG) to create customized lessons, answer users\u2019 questions in detail, and generate quizzes that adapt to their progress. You\u2019ll use several exciting Azure technologies:\\r\\n\\r\\n* [Azure OpenAI Service](https://azure.microsoft.com/products/ai-services/openai-service?ocid=buildia24_60days_blogs) \u2014 Provides the core AI capabilities for your app\\r\\n* [Azure AI Search](https://azure.microsoft.com/products/ai-services/ai-search?ocid=buildia24_60days_blogs) \u2014 Offers a knowledge base that your app can access using RAG, allowing the large language model (LLM) to provide more accurate explanations\\r\\n* [Azure Container Apps](https://azure.microsoft.com/products/container-apps?ocid=buildia24_60days_blogs) \u2014 Lets you easily package and run apps in a flexible and scalable environment.\\r\\n\\r\\n![The Intelligent Application\'s architecture comprises Azure OpenAI, Azure AI Search, an Azure Storage Account, and the Knowledge Base.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-2.png)\\r\\n\\r\\nLet\u2019s get started!\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nTo follow this tutorial, ensure you have the following:\\r\\n\\r\\n* [Python](https://www.python.org/downloads/) 3.10 or later\\r\\n* Dave Kuhlman\u2019s open-source [Python knowledge base](https://www.davekuhlman.org/python_book_01.html) downloaded as an HTML file\\r\\n* An Azure subscription with access to the [Azure OpenAI Service](https://azure.microsoft.com/products/ai-services/openai-service?ocid=buildia24_60days_blogs). Note that you need to [request access](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUNTZBNzRKNlVQSFhZMU9aV09EVzYxWFdORCQlQCN0PWcu) to this service.\\r\\n* The [Azure command-line interface (CLI)](https://learn.microsoft.com/cli/azure/?ocid=buildia24_60days_blogs) installed\\r\\n* An Azure [resource group](https://learn.microsoft.com/azure/azure-resource-manager/management/manage-resource-groups-portal?ocid=buildia24_60days_blogs#create-resource-groups)\\r\\n\\r\\n### Build a Personal Tutor with AI and Azure Container Apps\\r\\n\\r\\nIn this first part of the series, you\u2019ll build the foundation for an AI-powered Python tutor. This tutor will use a knowledge base to answer your Python questions of varying difficulties and create tailored quizzes.\\r\\n\\r\\n#### Creating an Azure Storage Account for RAG Data\\r\\n\\r\\nStart by creating a space to store the knowledge base that your chatbot will use for RAG. Sign in to the Azure portal and search \u201cStorage accounts\u201d in the search bar. Click + **Create** to start a new storage account.\\r\\n\\r\\nOn the **Basics** tab, set the following configurations:\\r\\n\\r\\n* **Subscription** \u2014 Select the same subscription as your resource group.\\r\\n* **Resource group** \u2014 Select the resource group you created earlier.\\r\\n* **Storage account name** \u2014 Enter a unique name for your storage account.\\r\\n* **Region** \u2014 Choose a region according to your preference.\\r\\n* **Pricing tier** \u2014 Select Standard S0.\\r\\n\\r\\n![The storage account creation page has seven tabs: Basics, Advanced, Networking, Data protection, Encryption, Tags, and Review. Basics is open. Below are two sections: Project details and Instance details. Under Project details are fields for Subscription (Azure subscription 1) and Resource group (personal-tutor). Under Instance details are fields for Storage account name (blank) and Region ((US) East US). At the bottom are two buttons: Review and Next: Advanced.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-3.png)\\r\\n\\r\\nFor the other tabs, keep the default settings.\\r\\n\\r\\nFinally, click **Review + Create**, and then **Create**.\\r\\n\\r\\nNext, you\u2019ll upload the knowledge base linked in this tutorial\u2019s prerequisites. Use HTML formatting for this project.\\r\\n\\r\\nOnce you have created your storage account, navigate to it from the Azure portal and click **Upload**. In the window that appears, create a new container, give it a descriptive name, and check **Private (no anonymous access)**. Select the newly created container and click **Browse for files**. Select the file or folder you want to upload. Then, click **Upload** to upload the blob.\\r\\n\\r\\n![The Edututor storage account page lists menu options including Overview, Activity log, Tags, Diagnose and solve problems, Access Control (IAM), Data migration, Events, and Storage Mover. There are also menus for Data storage (Containers, File shares, Queues, and Tables), and Security + monitoring (Networking, Front Door and CDN), and Access keys. To the right of the page is a window to upload a blob, with the option to drag and drop a file or select an existing container.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-4.jpeg)\\r\\n\\r\\n:::info\\r\\nRegister for\u202f**[Episode 4](https://aka.ms/serverless-learn-live/ep4?ocid=buildia24_60days_blogs)**\u202fof the new learning series on\u202f**Intelligent Apps with Serverless on Azure**.\\r\\n:::\\r\\n\\r\\nJoin the community along with MVPs, and the Azure Product Group on how to leverage AI with Serverless on Azure technologies\u2014Azure Container Apps and Azure Functions\u2014to build intelligent applications.\\r\\n\\r\\n#### Creating an Azure OpenAI Service\\r\\n\\r\\nWith your knowledge base ready, you\u2019ll now set up the core AI component: an Azure OpenAI service that uses RAG to access this knowledge base and generate customized lessons and quizzes.\\r\\n\\r\\nIn the Azure portal search bar, search \u201cAzure OpenAI.\u201d Click + **Create** to start a new service.\\r\\n\\r\\n![The Azure OpenAI service page lets the user create a new service.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-5.png)\\r\\n\\r\\nMatch the **Basics** tab configurations to the following:\\r\\n\\r\\n* **Subscription** and **Resource group** \u2014 Ensure these match your previous Azure resources for consistency.\\r\\n* **Region** \u2014 This demonstration uses Sweden Central as the region. Note that some models are only available in [limited regions](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-preview), so be sure to select the right region.\\r\\n* **Name** \u2014 Create a unique name for your OpenAI resource.\\r\\n* **Pricing tier** \u2014 Select **Standard S0**.\\r\\n\\r\\n![The page to create an Azure OpenAI service has four tabs: Basics, Network, Tags, and Review + submit. Basics is open. There are three sections: Project Details, Instance Details, and Content review policy (not visible here). Under Project Details are fields for Subscription (Azure subscription 1) and Resource group (personal-tutor). Under Instance Details are region (Sweden Central), Name (pythontutor), and Pricing tier (Standard S0). At the bottom of the page are Previous and Next buttons.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-6.png)\\r\\n\\r\\nClick **Next**.\\r\\n\\r\\nOn the **Networking** tab, select **All networks, including the internet, can access this resource**. \\r\\n\\r\\nFinally, click **Review + Create**, and then **Create**. \\r\\n\\r\\nOnce your Azure OpenAI service is ready, you\u2019ll set up the necessary AI models. Within your Azure OpenAI resource, click **Go to Azure OpenAI Studio**. Then, under **Management** on the left pane, select **Deployments**.\\r\\n\\r\\n![Screenshot of the Deployments page.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-7.png)\\r\\n\\r\\nClick **+ Create new deployment**. You\u2019ll deploy two models for your application:\\r\\n\\r\\n* `text-embedding-ada-002` to create the embeddings of the knowledge base\\r\\n* GPT-4 to generate personalized courses and quizzes\\r\\n\\r\\n![The form to deploy a model contains three fields: Select a model (a dropdown menu), Model version (a dropdown menu), and Deployment name. At the bottom are two buttons: Create and Cancel.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-8.png)\\r\\n\\r\\n#### Creating an Azure AI Search Resource\\r\\n\\r\\nWith your AI models in place, it\u2019s time to make your knowledge base easily accessible to them.\\r\\n\\r\\nIn the Azure OpenAI Studio, navigate to **Playground** and select **Chat**. Locate the **Add your data** option and click + **Add a data source**.\\r\\n\\r\\n![The Chat playground within Azure OpenAI Studio.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-9.png)\\r\\n\\r\\nA window will open where you can connect to the Azure Blob Storage you created earlier. Use the following configurations for the data source:\\r\\n\\r\\n* **Select data source** \u2014 Select **Azure Blob Storage**.\\r\\n* **Subscription** \u2014 Choose the same subscription as your other services.\\r\\n* **Select Azure Blob storage resource** \u2014 Select the storage account you created.\\r\\n* **Select storage container** \u2014 Select the container that has your knowledge base.\\r\\n* **Select Azure AI Search resource** \u2014 Create a new Azure AI Search resource.\\r\\n\\r\\n![Screenshot of the for to select or add\xa0 a data source . It includes the following fields: Select data source, Subscription. Select Azure Blob storage resource, Select storage container, Select Azure AI Search resource, Enter the index name, and Index schedule. There\'s a checkbox, selected here, for Add vector search to this search resource. Below is an Embedding model heading, with a Select an embedding model field below. Azure OpenAI - embedding-model is selected. Below is a checkbox, selected, for acknowledging you\'ll incur costs for usage. The bottom of the page has two buttons: Next and Cancel.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-10.png)\\r\\n\\r\\nSince you don\u2019t have an existing Search resource, select **Create a new Azure AI Search resource**, hyperlinked under the **Select Azure AI Search resource** dropdown. Azure AI Search makes the content of your knowledge base searchable by converting the data into numeric representations called vectors. These vectors capture the meaning and relationships between words.\\r\\n\\r\\nOn the **Basics** tab, configure the resource as follows:\\r\\n\\r\\n* **Subscription** and **Resource Group** \u2014 Choose the same subscription and resource group as your other services.\\r\\n* **Service name** \u2014 Enter a name for your search service.\\r\\n* **Location** \u2014 Choose a location according to your preference.\\r\\n* **Pricing tier** \u2014 Select **Basic**.\\r\\n\\r\\n![The form to create a search service features five tabs: Basics, Scale, Networking, Tags, and Review + create. Basics is open. There are two sections: Project details (containing Subscription and Resource Group) and Instance details (containing Service name, Location, and Pricing tier). At the bottom are three buttons: Review + create, Previous, and Next: Scale.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-11.png)\\r\\n\\r\\nClick **Review + create**.\\r\\n\\r\\nReturn to the **Select or add data source** window. Configure the rest of the options as follows:\\r\\n\\r\\n* **Enter the index name** \u2014 Provide an index name that will reference your data source.\\r\\n* **Indexer schedule** \u2014 Choose **Once**.\\r\\n* Enable the **Add vector search to this search resource** option.\\r\\n* **Select an embedding model** \u2014 Choose the text-embedding-ada-002 model that you deployed earlier.\\r\\n\\r\\nCheck the acknowledgment box and click **Next**.\\r\\n\\r\\nThen, in the **Data management** window, select **Vector** as the **Search type**. Check the acknowledgment and click **Next**.\\r\\n\\r\\n![The Data management form includes a Search type dropdown, where Vector is selected. Below is a checkbox, selected here, to acknowledge that costs will incur with usage. At the bottom are three buttons: Back, Next, and Cancel.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-12.png)\\r\\n\\r\\nFinally, click **Save and close**.\\r\\n\\r\\n![The Review and finish page of the form summarizes the options and data choices from previous steps. At the bottom are three buttons: Back, Save and close, and Cancel.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-1-13.png)\\r\\n\\r\\nNow that your data and services are connected, the next tutorial will focus on building the core application logic and creating a web interface to interact with your chatbot.\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nAnd with that, you\u2019ve reached the end of the first tutorial of this series. You\u2019ve established a solid Azure foundation and connected your knowledge base to powerful AI services. In the next part of this series, you\u2019ll use Python to build a chatbot that enables dynamic interaction and personalized learning experiences for your users. See you there!\\r\\n\\r\\nTo build on the skills you\u2019ve gained here, explore more AI and cloud topics by registering for [Serverless on Azure Learn Live](https://aka.ms/intelligent-apps/serverless-learnlive?ocid=buildia24_60days_blogs)\u202fseries. Then, put your knowledge to the test \u2014 and earn a Microsoft Learn badge \u2014 by joining our [Cloud Skills Challenge](https://developer.microsoft.com/offers/30-days-to-learn-it?challenge_option=B7BFF832-B9EC-495C-8213-26471EB30B38&wt.mc_id=cloudskillschallenge_B7BFF832-B9EC-495C-8213-26471EB30B38?wt.mc_id=30daystolearnitblog_techcommunity_blog_wwl&ocid=buildia24_60days_blogs)."},{"id":"personalizing-education-with-generative-ai-and-retrieval-augmented-generation-2","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-2","source":"@site/blog-60daysofIA/2024-04-08/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-2.md","title":"7.2 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 2","description":"In this three-part series, you\u2019ll use Azure Container Apps, Azure OpenAI Service, and Retrieval Augmented Generation to create a personal tutor chatbot that dynamically adjusts educational materials and quizzes based on user interactions. This article shows how to set up the core Azure AI services required to build your Intelligent App.","date":"2024-04-08T09:05:00.000Z","formattedDate":"April 8, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":9.25,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-04-08T09:05","slug":"personalizing-education-with-generative-ai-and-retrieval-augmented-generation-2","title":"7.2 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 2","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this three-part series, you\u2019ll use Azure Container Apps, Azure OpenAI Service, and Retrieval Augmented Generation to create a personal tutor chatbot that dynamically adjusts educational materials and quizzes based on user interactions. This article shows how to set up the core Azure AI services required to build your Intelligent App.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"7.1 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 1","permalink":"/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-1"},"nextItem":{"title":"7.3 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 3","permalink":"/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-3"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-2\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this three-part series, you\u2019ll use Azure Container Apps, Azure OpenAI Service, and Retrieval Augmented Generation to create a personal tutor chatbot that dynamically adjusts educational materials and quizzes based on user interactions. This article shows how to set up the core Azure AI services required to build your Intelligent App.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-2\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-2\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Graphic with a chat bubble-meets-robot head in the top right corner. At the bottom of the graphic is text that reads, \\"Personalizing Education with Generative AI and Retrieval Augmented Generation: Creating the Chatbot.\\"](../../static/img/60-days-of-ia/blogs/2024-04-08/7-2-1.jpeg)\\r\\n\\r\\n*In this three-part series, you\u2019ll use Azure Container Apps, Azure OpenAI Service, and Retrieval Augmented Generation to create a personal tutor chatbot that dynamically adjusts educational materials and quizzes based on user interactions. This article walks you through building the chatbot application\u2019s logic.*\\r\\n\\r\\n## Personalizing Education with Generative AI and Retrieval Augmented Generation Part 2: Creating the Chatbot\\r\\n\\r\\nIn the [first article](https://azure.github.io/cloud-native/60daysofia/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-1) of this series, you set up essential Azure services and prepared your knowledge base. Now, you\u2019ll build on this foundation to develop a chatbot that interacts with users and takes the lead in dynamically adjusting educational materials.\\r\\n\\r\\nLet\u2019s get coding!\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nTo follow this tutorial, ensure you have the following:\\r\\n\\r\\n* An Azure subscription with access to the [Azure OpenAI Service](https://azure.microsoft.com/products/ai-services/openai-service?ocid=buildia24_60days_blogs)\\r\\n* Access to the Azure services set up in Part 1\\r\\n* [Python](https://www.python.org/downloads/) 3.10 or later \\r\\n* A [Streamlit](https://streamlit.io/) account. Streamlit is an open-source Python framework that supports machine learning (ML) tasks. \\r\\n* A code editor or integrated development environment (IDE), like [Visual Studio Code](https://code.visualstudio.com/download)\\r\\n\\r\\nFor a preview of the project, check out the [complete code](https://github.com/contentlab-io/Personalizing-Education-with-Generative-AI-and-RAG) for this tutorial. \\r\\n\\r\\n:::info\\r\\nCheckout the demo bytes for [Intelligent Apps](https://developer.microsoft.com/en-us/reactor/series/S-1308/?wt.mc_id=blog_S-1308_webpage_reactor&ocid=buildia24_60days_blogs) with Azure Container Apps where the product engineering team gives a walkthrough on using open-source vector databases and building a multi-LLM chat application. \\r\\n:::\\r\\n\\r\\n### Code a Web Interface for Your Personal Tutor\\r\\n\\r\\nIn this tutorial, you\u2019ll design a chatbot using Streamlit and the Azure OpenAI Service. This chatbot will:\\r\\n\\r\\n* Ask users for their experience level (beginner, intermediate, or advanced). \\r\\n* Ask a question about the topic based on the user\u2019s experience level. If the user struggles to answer the question correctly, the chatbot will explain what the correct answer is and why. \\r\\n* Use Retrieval Augmented Generation (RAG) to access the knowledge base for answers, additional information, and dynamic question generation. \\r\\n* Adapt the difficulty of questions based on how the user performs on quizzes.\\r\\n\\r\\n#### Retrieving API Keys and Endpoints\\r\\n\\r\\nBefore you start coding, ensure you have the following credentials, which allow you to connect your application to the Azure OpenAI Service and make a call:\\r\\n\\r\\n* **Azure OpenAI endpoint** \u2014 Locate your Azure OpenAI service in the Azure portal. Find the endpoint in the **Overview** section.  \\r\\n* **Azure OpenAI key** \u2014 To retrieve the key, navigate to **Keys and Endpoint** under **Resource Management**.  \\r\\n* **Azure OpenAI deployment ID** \u2014 This is the name of the GPT-4 model you deployed in Part 1. You can find this value in the Azure AI Studio under **Resource Management** > **Deployments**. \\r\\n* **Azure AI Search endpoint** \u2014 Find this value by opening your Azure AI Search service from the Azure portal and locating the **URL** value in the **Overview** section.  \\r\\n* **Azure AI Search key** \u2014 Navigate to **Settings** > **Keys** in your Azure AI Search resource to find this value.  \\r\\n* **Azure AI Search index** \u2014 This value is the name of your Azure AI Search index. You can find it in the **Overview** section of your Azure AI Search resource in the Azure portal.\\r\\n\\r\\nOnce you have the values for all the above variables, export them as environment variables. Set up your Python project by creating a new directory. From this directory, run the following commands in the terminal, replacing the placeholders with the values: \\r\\n\\r\\n```\\r\\nexport AOAIEndpoint=<Azure OpenAI Endpoint> \\r\\nexport AOAIKey=<Azure OpenAI Key> \\r\\nexport AOAIDeploymentId=<Azure OpenAI Deployment ID> \\r\\nexport SearchEndpoint=<Search Endpoint> \\r\\nexport SearchKey=<Search Key> \\r\\nexport SearchIndex=<Search Index> \\r\\n```\\r\\n\\r\\n**Note:** The method will vary slightly based on your operating system.\\r\\n\\r\\n### Create the Python App\\r\\n\\r\\nNext, you\u2019ll build the chatbot. In your project directory, create a new file named `main.py`. This will be the heart of your application.\\r\\n\\r\\n#### Adding Libraries\\r\\n\\r\\nA couple of libraries are essential to build your chatbot. In your terminal or command prompt, install `streamlit` and `openai` using a package manager, such as pip.\\r\\n\\r\\nStart by creating a function, `read_markdown_file`, to load and read the course content from different existing `.md` files ([`basic_python.md`](https://github.com/contentlab-io/Personalizing-Education-with-Generative-AI-and-RAG/blob/main/basic_python.md), [`intermediate_python.md`](https://github.com/contentlab-io/Personalizing-Education-with-Generative-AI-and-RAG/blob/main/intermediate_python.md), [`advanced_python.md`](https://github.com/contentlab-io/Personalizing-Education-with-Generative-AI-and-RAG/blob/main/advanced_python.md)). You\u2019ll display the course content using `streamlit`:\\r\\n\\r\\n```\\r\\nimport streamlit as st \\r\\nfrom pathlib import Path \\r\\nimport openai  # We\'ll configure OpenAI access soon \\r\\n\\r\\n# Function to read course content \\r\\ndef read_markdown_file(markdown_file): \\r\\n    \\"\\"\\"Reads a Markdown file (.md) and returns its contents as text.\\"\\"\\" \\r\\n    return Path(markdown_file).read_text() \\r\\n\\r\\n# Welcome Message \\r\\nst.markdown(\\"\\"\\"Welcome to this e-learning course on Python programming!  \\r\\n               This course is designed for learners of all levels. Whether you\'re  \\r\\n               just starting out or want to master advanced concepts, our adaptive  \\r\\n               system will tailor lessons and quizzes to your needs.\\"\\"\\") \\r\\n\\r\\n# Tabs for Skill Levels \\r\\ntab1, tab2, tab3 = st.tabs([\\"Beginner\\", \\"Intermediate\\", \\"Advanced\\"]) \\r\\n\\r\\n# Load Course Content (You\'ll need to create these .md files) \\r\\nwith tab1: \\r\\n   st.header(\\"Beginner\\") \\r\\n   st.markdown(read_markdown_file(\\"beginner_python.md\\")) \\r\\n\\r\\nwith tab2: \\r\\n   st.header(\\"Intermediate\\") \\r\\n   st.markdown(read_markdown_file(\\"intermediate_python.md\\")) \\r\\n\\r\\nwith tab3: \\r\\n   st.header(\\"Advanced\\") \\r\\n   st.markdown(read_markdown_file(\\"advanced_python.md\\")) \\r\\n```\\r\\n\\r\\n#### Retrieving Environment Variables\\r\\n\\r\\nNext, retrieve the environment variables you set earlier and initialize the client object, establishing the connection to the Azure OpenAI Service: \\r\\n\\r\\n```\\r\\nfrom openai import AzureOpenAI \\r\\n\\r\\nendpoint = os.environ.get(\\"AOAIEndpoint\\") \\r\\napi_key = os.environ.get(\\"AOAIKey\\") \\r\\ndeployment = os.environ.get(\\"AOAIDeploymentId\\") \\r\\n\\r\\nclient = AzureOpenAI( \\r\\n    base_url=f\\"{endpoint}/openai/deployments/{deployment}/extensions\\", \\r\\n    api_key=api_key, \\r\\n    api_version=\\"2023-08-01-preview\\", \\r\\n) \\r\\n```\\r\\n\\r\\n#### Setting Up the Chatbot Logic\\r\\n\\r\\nLastly, set up the logic for the chatbot: \\r\\n\\r\\n```\\r\\n# Set a default model \\r\\nif \\"openai_model\\" not in st.session_state: \\r\\n    st.session_state[\\"openai_model\\"] = \\"gpt-4\\" \\r\\n\\r\\n# Initialize chat history \\r\\nif \\"messages\\" not in st.session_state: \\r\\n    st.session_state.messages = [] \\r\\n    st.session_state.messages= [{\\"role\\": \\"assistant\\", \\"content\\": \\"\\"\\"Hi! I am your Python Programming instructor. To get started, please share your level of experience with Python. Choose from the following options: Beginner, Intermediate, or Advanced.\\"\\"\\"}] \\r\\n\\r\\n# Display chat messages from history on app rerun \\r\\nfor message in st.session_state.messages: \\r\\n    with st.chat_message(message[\\"role\\"]): \\r\\n        st.markdown(message[\\"content\\"]) \\r\\n\\r\\n# React to user input \\r\\nif prompt:= st.chat_input(\\"Ask something!\\"): \\r\\n    # Display user message in chat message container \\r\\n    st.chat_message(\\"user\\").markdown(prompt) \\r\\n    # Add user message to chat history \\r\\n    st.session_state.messages.append({\\"role\\": \\"user\\", \\"content\\": prompt}) \\r\\n\\r\\n    try: \\r\\n        # Display assistant response in chat message container \\r\\n        with st.chat_message(\\"assistant\\"): \\r\\n            completion = client.chat.completions.create( \\r\\n                model = deployment, \\r\\n                messages = [ \\r\\n                    {\\"role\\": m[\\"role\\"], \\"content\\": m[\\"content\\"]} \\r\\n                    for m in st.session_state.messages \\r\\n                ], \\r\\n            extra_body={ \\r\\n            \\"dataSources\\": [ \\r\\n                { \\r\\n                    \\"type\\": \\"AzureCognitiveSearch\\", \\r\\n                    \\"parameters\\": { \\r\\n                        \\"endpoint\\": os.environ[\\"SearchEndpoint\\"], \\r\\n                        \\"key\\": os.environ[\\"SearchKey\\"], \\r\\n                        \\"indexName\\": os.environ[\\"SearchIndex\\"], \\r\\n                        \\"queryType\\": \\"vector\\", \\r\\n                        \\"semanticConfiguration\\": \\"default\\", \\r\\n                        \\"inScope\\": True, \\r\\n                        \\"filter\\": None, \\r\\n                        \\"strictness\\": 3, \\r\\n                        \\"topNDocuments\\": 5, \\r\\n                        \\"embeddingDeploymentName\\": \\"embedding-model\\", \\r\\n                        \\"roleInformation\\": \\"\\"\\"You are a dynamic Python programming instructor. Your role is to assess the user\'s knowledge and provide a tailored learning experience. Based on the user\'s response, you will take a quiz on a specific Python topic suitable for their level. If the user answers correctly, increase the difficulty of subsequent questions. If incorrect, provide hints, then either simplify the question or present a similar one at the same level. Your responses should evolve based on the user\'s experience level and their performance on the quizzes.\\"\\"\\" \\r\\n                        } \\r\\n                } \\r\\n                ]  \\r\\n                }, \\r\\n                n = 1, \\r\\n                temperature=0, \\r\\n                top_p=1, \\r\\n                max_tokens=200, \\r\\n                stop=[], \\r\\n                stream=False \\r\\n            ) \\r\\n\\r\\n            response = completion.choices[0].message.content \\r\\n            st.markdown(response) \\r\\n\\r\\n        # Add assistant response to chat history \\r\\n        st.session_state.messages.append({\\"role\\": \\"assistant\\", \\"content\\": response}) \\r\\n\\r\\n    except Exception as e: \\r\\n        st.error(f\\"An error occurred: {e}\\") \\r\\n```\\r\\n\\r\\nThere\u2019s a lot of code here, so let\u2019s review the chatbot logic code step by step:\\r\\n\\r\\n* Streamlit uses the `st.session_state` variable to set the session state and preserve information across interactions with your app. The `messages` variable inside `st.session_state` maintains the chat history as a list of dictionaries, each with a \\"`role`\\" (`user` or `assistant`) and \\"`content`\\". \\r\\n* `st.chat_input` handles user input and displays it within the conversation. \\r\\n* You use the `chat.completions` API to make requests to your Azure OpenAI deployment. The chat history is included for context using the `messages` parameter.  \\r\\n* The `extra_body` parameter sets up RAG with configuration details for the Azure AI Search instance, embedding model, and instructions for the AI\u2019s behavior. The parameters within the `extra_body` include: \\r\\n  * `type`, specifying the knowledge source as \u201cAzureCognitiveSearch\u201d (now known as [Azure AI Search](https://learn.microsoft.com/azure/search/whats-new?ocid=buildia24_60days_blogs#new-service-name)). \\r\\n  * `parameters`, providing essential details like your Azure AI Search endpoint, API key, index name, and various query parameters to target the appropriate information. \\r\\n  * The `roleInformation` section within `extra_body`, guiding the model\u2019s behavior. It explains the role of a dynamic Python instructor, with instructions on assessing the user\u2019s knowledge, conducting quizzes, and adapting the learning experience. \\r\\n  * The temperature parameter, controlling the level of randomness and creativity injected into the model\u2019s response. By setting this to 0, the model favors words with the highest probabilities, leading to more predictable, safe, and potentially generic responses.\\r\\n\\r\\nThis code snippet only works if you\u2019ve set up an Azure AI Search instance and correctly configured your environment variables (`SearchEndpoint`, `SearchKey`, and `SearchIndex`). \\r\\n\\r\\n#### Running the Chatbot App\\r\\n\\r\\nTo start your web server locally, run the following command in your terminal:\\r\\n\\r\\n```\\r\\npython -m streamlit run main.py\\r\\n```\\r\\n\\r\\n![The Personal Python Tutor web app opens with an introduction and asks the user to select their experience level: Beginner, Intermediate, or Advanced. Below that is a heading that reflects the selected experience level (here, Beginner) with a \\"What is Python\\" definition and a list of key features. At the bottom of the page is a space for the user to ask the chatbot a question.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-2-2.png)\\r\\n\\r\\nUpon launching the app, you\u2019ll see a welcome message and three tabs: Beginner, Intermediate, and Advanced. Each tab contains preconfigured course materials sourced from .md files. You can click any tab to access the relevant content.\\r\\n\\r\\nFurther down, you\u2019ll find the chatbot interface. The chatbot initiates the conversation by asking you to specify your difficulty level.  \\r\\n\\r\\nAfter you provide your answer, the chatbot will administer a short quiz to assess your knowledge at that level. If you answer correctly, the app will progressively increase the difficulty of subsequent questions. \\r\\n\\r\\n![The chatbot provides a beginner-level quiz about a Python output.](../../static/img/60-days-of-ia/blogs/2024-04-08/7-2-3.png)\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nIn this tutorial, you set up your chatbot interface using Streamlit, connected it to Azure OpenAI, and implemented the core conversation logic. Your chatbot is in good shape and ready for action! In the final part of this series, you\u2019ll deploy your app as a web application using Azure Container Apps. See you there!\\r\\n\\r\\nWant to continue your learning journey? We\u2019re glad to hear it \u2014 and we have a few ways for you to stay engaged. Watch the first demo byte covering [Intelligent Apps with Azure Container Apps](https://developer.microsoft.com/reactor/series/S-1308/?wt.mc_id=blog_S-1308_webpage_reactor&ocid=buildia24_60days_blogs) using Qdrant and register for the upcoming live episode for the [Serverless on Azure Learn Live](https://developer.microsoft.com/reactor/events/22146/?ocid=buildia24_60days_blogs) series."},{"id":"personalizing-education-with-generative-ai-and-retrieval-augmented-generation-3","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-3","source":"@site/blog-60daysofIA/2024-04-10/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-3.md","title":"7.3 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 3","description":"In this three-part series, you\u2019ll use Azure Container Apps, Azure OpenAI Service, and Retrieval Augmented Generation to create a personal tutor chatbot that dynamically adjusts educational materials and quizzes based on user interactions. This final article demonstrates how to deploy using Azure Container Apps.","date":"2024-04-10T09:00:00.000Z","formattedDate":"April 10, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.995,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-04-10T09:00","slug":"personalizing-education-with-generative-ai-and-retrieval-augmented-generation-3","title":"7.3 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 3","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this three-part series, you\u2019ll use Azure Container Apps, Azure OpenAI Service, and Retrieval Augmented Generation to create a personal tutor chatbot that dynamically adjusts educational materials and quizzes based on user interactions. This final article demonstrates how to deploy using Azure Container Apps.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"7.2 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 2","permalink":"/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-2"},"nextItem":{"title":"7.4 Real-time Voice Sentiment Analysis System 1","permalink":"/Cloud-Native/60DaysOfIA/real-time-voice-sentiment-analysis-system-1"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-3\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Join us on a learning journey to build intelligent apps on Azure. Read all about the upcoming #BuildIntelligentApps initiative on this post!\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-3\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-3\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Graphic with a chat bubble-meets-robot head in the top right corner. At the bottom of the graphic is text that reads, \\"Personalizing Education with Generative AI and Retrieval Augmented Generation: Creating the Chatbot.\\"](../../static/img/60-days-of-ia/blogs/2024-04-10/7-3-1.jpeg)\\r\\n\\r\\n## Personalizing Education with Generative AI and Retrieval Augmented Generation Part 3: Deploying a Web Interface\\r\\n\\r\\nWelcome to the final installment of our three-part tutorial series on building a personalized Python tutor with Generative AI and Retrieval Augmented Generation (RAG)! If you\u2019re new to this series, be sure to check out [Part 1](https://azure.github.io/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-1), which shows you how to set up the essential Azure resources, and [Part 2](https://azure.github.io/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-2), which explains how to build out the chatbot\u2019s core functionality.\\r\\n\\r\\nIn this final tutorial, you\u2019ll take your chatbot from the development environment to a live web application where anyone can interact with it.\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nTo follow this tutorial, ensure you have the following:\\r\\n\\r\\n* The Azure services set up in Part 1\\r\\n* The [Python web app](https://github.com/contentlab-io/Personalizing-Education-with-Generative-AI-and-RAG/blob/main/main.py) built in Part 2\\r\\n* An active Azure subscription\\r\\n* The [Azure command-line interface (CLI)](https://learn.microsoft.com/cli/azure/?ocid=buildia24_60days_blogs) installed\\r\\n* [Docker](https://www.docker.com/get-started) installed\\r\\n\\r\\nTo preview the final application, take a look at the [complete project code](https://github.com/contentlab-io/Personalizing-Education-with-Generative-AI-and-RAG).\\r\\n\\r\\n:::info\\r\\nRegister for\u202f**[Episode 4](https://aka.ms/serverless-learn-live/ep4?ocid=buildia24_60days_blogs)** of the new hands-on live learning series with an SME\u202fon **Intelligent Apps with Serverless on Azure**.\\r\\n:::\\r\\n\\r\\n### Deploy the Web Interface for the Educational Chatbot\\r\\n\\r\\nWith the chatbot\u2019s intelligence and functionality in place, it\u2019s time to make it accessible to the world. You\u2019ll use [Azure Container Apps](https://azure.microsoft.com/products/container-apps?ocid=buildia24_60days_blogs) for a smooth and scalable deployment.\\r\\n\\r\\nAzure Container Apps is a serverless platform designed to streamline the deployment and management of containerized applications. It handles infrastructure complexities for you, letting you focus on your application\u2019s code.\\r\\n\\r\\nContainerization packages your chatbot\u2019s code, dependencies, and runtime into a self-contained image. This means it will run consistently across different environments. Since it\u2019s lightweight, you can scale up or down based on demand.\\r\\n\\r\\n#### Creating an Azure Container Registry\\r\\n\\r\\n[Azure Container Registry](https://azure.microsoft.com/products/container-registry?ocid=buildia24_60days_blogs) (ACR) is your private storage space for container images. To get started with ACR, use the Azure CLI to log in to your Azure account by running:\\r\\n\\r\\n```\\r\\naz login\\r\\n```\\r\\n\\r\\nAfter authenticating, run the following, ensuring you replace `personaltutor` with your chosen registry name:\\r\\n\\r\\n```\\r\\naz acr login --name personaltutor\\r\\n```\\r\\n\\r\\n#### Building the Container Image\\r\\n\\r\\nNext, create a file named `Dockerfile` in your project\u2019s root directory. This file provides Docker with instructions for building your image:\\r\\n\\r\\n```\\r\\nFROM python:3.11-slim-bullseye \\r\\n\\r\\nWORKDIR /app\\r\\n\\r\\n# Install Streamlit and other dependencies\\r\\nCOPY requirements.txt ./\\r\\nRUN pip install -r requirements.txt\\r\\n\\r\\n# Copy your application files\\r\\nCOPY . ./\\r\\n\\r\\n# Expose the port used by Streamlit\\r\\nEXPOSE 8501\\r\\n\\r\\n# Command to start your app\\r\\nCMD streamlit run main.py\\r\\n```\\r\\n\\r\\nExecute the following command to build the image, making sure to use your registry name:\\r\\n\\r\\n```\\r\\naz acr build --registry personaltutor --image python-tutor:latest --file Dockerfile .\\r\\n```\\r\\n\\r\\nThis command instructs Azure to build a container image named `python-tutor:latest`, using your Dockerfile and the code in the current directory (indicated by the period). The image is then stored in your ACR.\\r\\n\\r\\nOnce you push your image to ACR, open the Azure portal and navigate to your container registry. Enable admin access by selecting **Access keys** under **Settings**, and then clicking **Enable** under **Admin** user.\\r\\n\\r\\nAlternatively, run the following command from the terminal to enable admin access (again, updating `personaltutor` to reflect your selected registry name):\\r\\n\\r\\n```\\r\\naz acr update -n personaltutor --admin-enabled true\\r\\n```\\r\\n\\r\\n#### Creating an Environment\\r\\n\\r\\nAn [Azure Container Apps environment](https://learn.microsoft.com/azure/container-apps/environment?ocid=buildia24_60days_blogs) acts as a logical boundary for your apps. Think of it as the neighborhood where your chatbot will live.\\r\\n\\r\\nTo create a container app environment, run:\\r\\n\\r\\n```\\r\\naz containerapp env create \\\\\\r\\n   --name python-tutor-app-env \\\\\\r\\n   --resource-group personal-tutor \\\\\\r\\n   --location eastus\\r\\n```\\r\\n\\r\\nThis command creates an environment named `python-tutor-app-env` within your existing resource group, located in the `eastus` region.\\r\\n\\r\\n#### Creating the Container App\\r\\n\\r\\nNow, you can deploy your application using Azure Container Apps!\\r\\n\\r\\nSign in to the Azure portal and search for \u201cAzure Container Apps\u201d in the search bar at the top. Select the service. Then, click **+ Create** to start a new container app.\\r\\n\\r\\n![The Container Apps page in the Azure portal.](../../static/img/60-days-of-ia/blogs/2024-04-10/7-3-2.png)\\r\\n\\r\\nOn the **Basics** tab, configure the settings as follows:\\r\\n\\r\\n* **Subscription** and **Resource group** \u2014 For consistency, ensure these match the resources you set in the first two parts of this series.\\r\\n* **Container app name** \u2014 Choose a unique name, such as \u201cpersonaltutor.\u201d\\r\\n* **Region** \u2014 Select the same region where you created your environment.\\r\\n* **Container Apps Environment** \u2014 Select the environment you created earlier.\\r\\n\\r\\n![Screenshot of the page to create the Container App. It has five tabs: Basics, Container, Bindings, Tags, and Review + create. Basics is open.\xa0 The page has two sections: Project details (Subscription, Resource group, and Container app name) and Container Apps Environment (Region and Container Apps Environment). At the bottom are two buttons: Review + Create and Next: Container.](../../static/img/60-days-of-ia/blogs/2024-04-10/7-3-3.png)\\r\\n\\r\\nClick **Next: Container >** to proceed.\\r\\n\\r\\nOn the **Container** tab, configure the settings as follows:\\r\\n\\r\\n* Uncheck **Use quickstart image**, as you\u2019ll use your custom image.\\r\\n* **Image Source** \u2014 Select **Azure Container Registry**.\\r\\n* **Registry** \u2014 Choose the ACR you created previously.\\r\\n* **Image** \u2014 Select the **python-tutor:latest** image you built.\\r\\n* **Tag** \u2014 Keep the default **latest**.\\r\\n\\r\\nThen, click **Environment Variables** and add the following, ensuring you replace the placeholders with your actual values:\\r\\n\\r\\n* AOAIEndpoint\\r\\n* AOAIKey\\r\\n* AOAIDeploymentId\\r\\n* SearchEndpoint\\r\\n* SearchKey\\r\\n* SearchIndex\\r\\n\\r\\nYour chatbot needs these variables to connect to and interact with your Azure OpenAI and AI Search services. Review the section [\u201cRetrieving Environment Variables\u201d](https://azure.github.io/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-2#retrieving-api-keys-and-endpoints) in Part 2 of this series if you need a refresher on how to retrieve these.\\r\\n\\r\\n![The Create Container App page lists the container details (Name, Image source, Registry, Image, Image tag, and Command override), the Container resource allocation (Workload profile and CPU and memory), and Environment variables. At the bottom are three buttons: Review + create, Previous, and Next: Bindings).](../../static/img/60-days-of-ia/blogs/2024-04-10/7-3-4.png)\\r\\n\\r\\nClick **Next: Bindings >**. Leave the **Bindings** section with the default settings and click **Next: Ingress >**.\\r\\n\\r\\nConfigure as follows:\\r\\n\\r\\n* **Ingress** \u2014 Select **Enabled** to make your app publicly accessible.\\r\\n* **Ingress traffic** \u2014 Choose **Accepting traffic from anywhere**.\\r\\n* **Ingress type** \u2014 Keep HTTP.\\r\\n* **Target port** \u2014 Enter \u201c8501\u201d (assuming your Dockerfile specifies this).\\r\\n* **Session affinity** \u2014 Select **Enabled**. This helps maintain user sessions.\\r\\n\\r\\n![The Create Container App page lists fields for Ingress, Ingress traffic, Ingress type, selecting a Client certificate note, Transport, a checkbox for accepting Insecure connections, the option to enter a Target port, and a checkbox to enable Session affinity. At the bottom are three buttons: Review + create, Previous, and Next: Tags.](../../static/img/60-days-of-ia/blogs/2024-04-10/7-3-5.png)\\r\\n\\r\\nClick **Review + create**.\\r\\n\\r\\nDouble-check your settings. If everything is correct, click **Create**.\\r\\n\\r\\n![Screenshot of the Create Container App page. Here, you review the settings. At the bottom are two buttons: Create and Previous.](../../static/img/60-days-of-ia/blogs/2024-04-10/7-3-6.png)\\r\\n\\r\\nAzure will now deploy your application as a container app. Once finished, you\u2019ll see a URL in the **Overview** section (**Application Url**) where your app is live!\\r\\n\\r\\n![The personaltutor app overview shows an application URL indicating that the Intelligent App has been successfully deployed.](../../static/img/60-days-of-ia/blogs/2024-04-10/7-3-7.png)\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nCongratulations! Over this three-part series, you built an incredible AI-powered educational chatbot using Azure OpenAI, Azure AI Search, and Azure Container Apps for scalable deployment. You\u2019ve seen how simple it is to build a dynamic, scalable, and high-impact Intelligent App with the help of Azure services.\\r\\n\\r\\nNow that you have some boots-on-the-ground experience building an Intelligent App, test your knowledge by joining our **[Cloud Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs)**. And to keep the learning going, register for an upcoming demo bytes for **[Intelligent Apps with Azure Container Apps](https://developer.microsoft.com/reactor/series/S-1308/?wt.mc_id=blog_S-1308_webpage_reactor&ocid=buildia24_60days_blogs)** where the product engineering team gives a walkthrough on using open source vector databases and building a multi-LLM chat application."},{"id":"real-time-voice-sentiment-analysis-system-1","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/real-time-voice-sentiment-analysis-system-1","source":"@site/blog-60daysofIA/2024-04-17/real-time-voice-sentiment-analysis-system-1.md","title":"7.4 Real-time Voice Sentiment Analysis System 1","description":"In today\'s fast-paced digital world, understanding customer sentiment in real-time during voice calls can provide businesses with a competitive edge. This guide will show developers how to build a robust real-time voice sentiment analysis application using several key Azure services.","date":"2024-04-17T09:00:00.000Z","formattedDate":"April 17, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":14.645,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-04-17T09:00","slug":"real-time-voice-sentiment-analysis-system-1","title":"7.4 Real-time Voice Sentiment Analysis System 1","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In today\'s fast-paced digital world, understanding customer sentiment in real-time during voice calls can provide businesses with a competitive edge. This guide will show developers how to build a robust real-time voice sentiment analysis application using several key Azure services.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"7.3 Personalizing Education with Generative AI and Retrieval Augmented Generation Part 3","permalink":"/Cloud-Native/60DaysOfIA/personalizing-education-with-generative-ai-and-retrieval-augmented-generation-3"},"nextItem":{"title":"7.5 Real-time Voice Sentiment Analysis System 2","permalink":"/Cloud-Native/60DaysOfIA/real-time-voice-sentiment-analysis-system-2"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/real-time-voice-sentiment-analysis-system-1\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Join us on a learning journey to build intelligent apps on Azure. Read all about the upcoming #BuildIntelligentApps initiative on this post!\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/real-time-voice-sentiment-analysis-system-1\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/real-time-voice-sentiment-analysis-system-1\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n## Real-time Voice Sentiment Analysis System Using Azure Communication Services, Azure AI and Azure OpenAI (Part 1)\\r\\n\\r\\n### Introduction\\r\\n\\r\\nIn today\'s fast-paced digital world, understanding customer sentiment in real-time during voice calls can provide businesses with a competitive edge. This guide will show developers how to build a robust real-time voice sentiment analysis application using several key Azure services. Specifically, we\'ll leverage:\\r\\n\\r\\n* [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?ocid=buildia24_60days_blogs) to deploy the backend web API \\r\\n* ASP.NET Core web API backend for processing \\r\\n* [Azure Communication Services](https://learn.microsoft.com/azure/communication-services/overview?ocid=buildia24_60days_blogs) for handling voice calls \\r\\n* [Azure AI Language](https://learn.microsoft.com/azure/ai-services/language-service/overview?ocid=buildia24_60days_blogs) and [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview?ocid=buildia24_60days_blogs) for analyzing sentiment \\r\\n* Node.js frontend for user interactions. \\r\\n\\r\\n#### Architecture diagram\\r\\n\\r\\n![a diagram of the voice sentiment analysis application architecture](../../static/img/60-days-of-ia/blogs/2024-04-17/7-4-1.png)\\r\\n\\r\\n#### Frontend of the sample app\\r\\n\\r\\n![image of the Sentiment Score in a sample application](../../static/img/60-days-of-ia/blogs/2024-04-17/7-4-2.png)\\r\\n\\r\\nThe sample app will manage Voice over IP (VoIP) and Public Switched Telephone Network (PSTN) calling capabilities, converting speech to text on the fly, and will evaluate the sentiment of the conversation in real-time. There are many scenarios where these capabilities could be used:\\r\\n\\r\\n1. **Customer Support Call Centers**: Analyze customer sentiment in real-time during support calls, allowing agents to adjust their approach based on the emotional tone of the conversation. This can lead to improved customer satisfaction and faster resolution times. \\r\\n\\r\\n1. **Market Research Interviews**: Conduct telephonic interviews with participants and use sentiment analysis to gauge reactions to new product ideas or advertisements, providing valuable insights into market trends and consumer preferences. \\r\\n\\r\\n1. **Telehealth Services**: In virtual healthcare consultations, use the system to assess the sentiment of patients as they describe their symptoms or concerns. This can help healthcare providers better understand the patient\'s emotional state and potentially improve diagnosis and patient care. \\r\\n\\r\\n1. **Remote Education and E-Learning**: For online classes or training sessions, analyze the sentiment of students\' responses during voice interactions to assess engagement, comprehension, and the effectiveness of the teaching material. \\r\\n\\r\\n1. **Financial Services**: Use in banking and financial advice call centers to detect customer sentiment during calls, identifying potential issues or opportunities for additional services based on the emotional tone of the customer. \\r\\n\\r\\n1. **Sales and Lead Generation**: During sales calls, analyze potential customers\' sentiment to tailor the pitch dynamically, improving conversion rates by aligning with the customers\' emotional responses. \\r\\n\\r\\nBy the end of this blog, you will understand how to:\\r\\n\\r\\n* Set up and manage voice calling with Azure Communication Services. \\r\\n* Convert speech to text using Azure Communication Service Calling Captions. \\r\\n* Perform sentiment analysis on the text with both Azure AI Language and Azure Open AI Service. \\r\\n* Use ASP.NET Core and Node.js to tie these services together in a seamless application.\\r\\n\\r\\nLet\'s get started.\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\n#### Tools and Accounts Required:\\r\\n\\r\\n1. **Azure Account**: An active Azure account is required to access Azure services like Azure Communication Services, Azure AI Language, and Azure OpenAI. If you don\'t already have an account, you can sign up for a free trial [here](https://azure.com/free?ocid=buildia24_60days_blogs). \\r\\n\\r\\n1. **Visual Studio Code**: This powerful and lightweight code editor from Microsoft supports development in multiple languages including Node.js and C#. Download it from [here](https://code.visualstudio.com/). \\r\\n\\r\\n1. **Azure CLI**: The Azure Command Line Interface is a set of commands used to manage Azure resources directly from the terminal or command prompt. Download and installation instructions can be found [here](https://docs.microsoft.com/cli/azure/install-azure-cli?ocid=buildia24_60days_blogs). \\r\\n\\r\\n1. **Postman**: An API development tool that makes it easier to create, share, test, and document APIs. This is optional but recommended for testing your application\u2019s back-end services. Download Postman from [here](https://www.postman.com/downloads/). \\r\\n\\r\\n1. **Node.js**: Node.js is a JavaScript runtime that allows you to build scalable network applications. It\'s essential for developing the front-end of our application. Download and install it from [here](https://nodejs.org/). \\r\\n\\r\\n1. **Docker**: Docker is a platform for developing, shipping, and running applications. We\'ll use it for containerizing and deploying our ASP.NET Core backend. Download and install it from [here](https://www.docker.com/products/docker-desktop).\\r\\n\\r\\n##### VS Code Extensions:\\r\\n\\r\\n1. **C# Dev Kit for Visual Studio Code**: While VS Code does support C# development, this specific extension provides advanced features and functionalities tailored for C# development within Azure. It\'s especially handy for Azure Functions development in C#. Install it from [here](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit). \\r\\n\\r\\n1. **Azure Tools**: This extension provides a set of tools for working with Azure services directly from VS Code. It\'s a must-have for managing Azure resources and services. Install it from [here](https://marketplace.visualstudio.com/items?itemName=ms-vscode.vscode-node-azure-pack). \\r\\n\\r\\n1. **Docker Extension**: This extension provides a set of tools for working with Docker containers directly from VS Code. It\'s essential for containerizing and deploying our ASP.NET Core backend. Install it from [here](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker). \\r\\n\\r\\n1. **Azure Container Apps Extension**: This extension provides a set of tools for working with Azure Container Apps directly from VS Code. It\'s essential for deploying our ASP.NET Core backend. Install it from [here](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurecontainerapps). \\r\\n\\r\\n1. **Azure Static Web Apps Extension**: This extension provides a set of tools for working with Azure Static Web Apps directly from VS Code. It\'s essential for deploying our Node.js frontend. Install it from [here](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurestaticwebapps).\\r\\n\\r\\n:::info\\r\\nCheckout the demo bytes for\u202f[Intelligent Apps](https://developer.microsoft.com/en-us/reactor/series/S-1308/?wt.mc_id=blog_S-1308_webpage_reactor&ocid=buildia24_60days_blogs)\u202fwith Azure Container Apps where the product team gives a walkthrough on using open-source vector databases and building a multi-LLM chat application. \\r\\n:::\\r\\n\\r\\n### Creating Azure Resources\\r\\n\\r\\nWith the prerequisites set, the next step is to create necessary Azure resources. These resources include Azure Communication Services for handling voice calls, Azure AI Language for sentiment analysis, and Azure OpenAI as an alternative for more complex sentiment analysis tasks. Here\u2019s how to set each of these up. \\r\\n\\r\\n#### Azure Communication Services Resource\\r\\n\\r\\n**Azure Communication Services (ACS)** will manage all aspects of voice calling within our application. To create a new ACS resource:\\r\\n\\r\\n1. **Open Azure Portal**: Navigate to the [Azure Portal](https://portal.azure.com) and sign in with your Azure account. \\r\\n1. **Create Resource**: Click on the \\"+ Create a resource\\" button found on the upper-left corner of the Azure Portal dashboard. \\r\\n1. **Search for Communication Services**: In the \\"Search the Marketplace\\" field, enter \\"Communication Services\\" and select it from the dropdown list. \\r\\n1. **Create**: Click the \\"Create\\" button.\\r\\n    1. On the \\"Basics\\" tab, fill in the required fields:\\r\\n\\r\\n        1. **Subscription**: Choose your Azure subscription.\\r\\n        1. **Resource group**: Create a new resource group or select an existing one.\\r\\n        1. **Resource name**: Enter a name for your Communication Services resource.\\r\\n        1. **Region**: Select a region near you or your target audience.\\r\\n    1. Click \\"Review + create\\" to review your settings and then click the \\"Create\\" button to provision the resource. \\r\\n1. **Configure**:\\r\\n    1. Once the deployment is complete, navigate to your resource.\\r\\n    1. Under the \\"Keys and Endpoint\\" section, note down your connection string. You\'ll need this to configure the backend of your application. \\r\\n1. **Get a UserToken**  - For initial frontend testing, we need a user token. In your Azure Communication Services resource, navigate to the \\"Identities & User Access Tokens\\" section and create a new user with \'Voice and video calling (VOIP)\' services. Note down the user token for use later in the tutorial. \\r\\n\\r\\n#### Obtain a Free Trial Phone Number\\r\\n\\r\\nTo enable PSTN calling capabilities (optional), you\'ll need a phone number. Azure Communication Services offers a free trial phone number for testing purposes. Here\'s how to obtain one:\\r\\n\\r\\n1. **Navigate to Your ACS Resource**: Sign in to the [Azure Portal](https://portal.azure.com) and go to your newly created Azure Communication Services resource. \\r\\n\\r\\n1. **Phone Numbers Section**: On the left-hand menu of your ACS resource overview page, find and select the **Phone Numbers** option. This section allows you to manage phone numbers associated with your ACS resource. \\r\\n\\r\\n1. **Get Trial Number**: Inside the Phone Numbers section, you\'ll see an option to get a free trial phone number. Click on the **Get** button next to the Free Trial Number. Azure will prompt you with the terms and conditions for using a trial number. Please read through these carefully as they contain important information regarding the limitations and permitted use of the free trial number. \\r\\n\\r\\n1. **Confirmation**: After accepting the terms, Azure will automatically allocate a trial phone number to your ACS resource. This process may take a few moments. Once completed, the number will be displayed on the screen. Note this number as you will need it for configuring voice calling capabilities in your application.\\r\\n\\r\\n* **Note**: The free trial phone number comes with certain limitations. For example, there may be restrictions on the number of calls that can be made or received, call duration, and available features compared to a purchased number. These restrictions are in place to manage the service\'s use during the trial period. \\r\\n\\r\\n* **Note**: If you don\'t, or can\'t, obtain a free trial phone number, you can still proceed with the rest of the tutorial. The application will work with VoIP calling capabilities, and you can always add a phone number later if needed. Either clone and run the [Azure Communication Service Calling Quickstart](https://learn.microsoft.com/azure/communication-services/quickstarts/voice-video-calling/getting-started-with-calling?tabs=uwp&pivots=platform-web&ocid=buildia24_60days_blogs) or the [Calling Hero App](https://learn.microsoft.com/azure/communication-services/samples/calling-hero-sample?pivots=platform-web&ocid=buildia24_60days_blogs) and use a user ID or group ID respective when making a call. \\r\\n\\r\\nNote down the phone number for use later in the tutorial.\\r\\n\\r\\n#### Azure AI Resource\\r\\n\\r\\nFor basic sentiment analysis, we\'ll use **Azure AI Language** services. Here\'s how to set up this resource:\\r\\n\\r\\n1. **Create Resource**: From the Azure Portal dashboard, click \\"+ Create a resource\\". \\r\\n1. **Search for AI Language**: Type \\"Language\\" in the search bar and select \\"Language\\" from the results. \\r\\n1. **Create**: Press the \\"Create\\" button.\\r\\n    1. Complete the form:\\r\\n        1. **Subscription** and **Resource group**: As before.\\r\\n        1. **Name**: Give your AI Language service a unique name.\\r\\n        1. **Pricing tier**: Select the pricing tier that fits your needs (you can start with the free tier for testing purposes). \\r\\n        1. **Region**: Choose the same region as your Communication Services to minimize latency.\\r\\n    1. Proceed to \\"Review + create\\" and then click \\"Create\\".\\r\\n1. **Configure**:\\r\\n    1. After your Language resource is deployed, go to it and note down the Key1 and Endpoint from the \\"Keys and endpoint\\" section.\\r\\n\\r\\n#### Azure OpenAI Resource\\r\\n\\r\\nTo leverage more advanced AI capabilities, we also integrate **Azure OpenAI Service**. Setting up this resource involves:\\r\\n\\r\\n1. **Create Resource**: Again, from the Azure Portal dashboard, click \\"+ Create a resource\\". \\r\\n1. **Search for OpenAI**: Enter \\"OpenAI\\" in the search field and select \\"OpenAI Service\\" from the dropdown. \\r\\n1. **Create**: Hit the \\"Create\\" button.\\r\\n    1. Fill in the details like you did for the Language service, taking special note of the **Subscription**, **Resource group**, **Name**, **Pricing tier**, and **Region**. \\r\\n    1. Review your settings and click \\"Create\\".\\r\\n1. **Configure**: Before you can generate text or inference, you need to deploy a model. You can select from one of several available models in Azure OpenAI Studio. To deploy a model, follow these steps:\\r\\n    * Sign in to [Azure OpenAI Studio](https://oai.azure.com/)\\r\\n    * Choose the subscription and the Azure OpenAI resource to work with and select Use resource. \\r\\n    * Under Management select Deployments.\\r\\n    * Select Create new deployment and configure the following fields: Model, Deployment name\\r\\n1. **Keys**: Once deployed, navigate to the OpenAI resource in Azure Portal. - In the \\"Keys and endpoint\\" section, copy Key1 and Endpoint. These will be crucial for integrating advanced sentiment analysis into your application.\\r\\n\\r\\n### Developing the Node.js Frontend\\r\\n\\r\\nIn this section, we\'ll dive into creating the front-end of our real-time voice sentiment analysis application using Node.js. The front end will provide a simple yet functional user interface (UI) for making voice calls and displaying closed captions generated from the conversation. We\'ll also ensure it can display the sentiment analysis results in real-time.\\r\\n\\r\\n#### Setting Up Your Node.js Environment\\r\\n\\r\\nFirst, ensure Node.js is installed on your development machine. If not, download and install it from the [official Node.js website](https://nodejs.org/). With Node.js installed, you can now set up your project environment.\\r\\n\\r\\n1. Create a New Directory: Create a new folder for your project. This is where all your Node.js front-end files will reside.\\r\\n\\r\\n    `bash`\\r\\n    ``` \\r\\n    mkdir voice-sentiment\\r\\n    cd voice-sentiment \\r\\n    ```\\r\\n\\r\\n1. **Initialize Your Project**: Run the following command to create a `package.json` file, which will keep track of your project dependencies.\\r\\n\\r\\n    `bash`\\r\\n    ```\\r\\n    npm init -y\\r\\n    ```\\r\\n\\r\\n1. **Install Parcel**: We\'ll use Parcel, a web application bundler, for a seamless development experience. Install Parcel as a development dependency.\\r\\n\\r\\n    `bash`\\r\\n    ```\\r\\n    npm install parcel --save-dev\\r\\n    ```\\r\\n\\r\\n1. **Install Azure Communication Calling SDK**: This SDK is essential for integrating voice calling features in our front-end.\\r\\n\\r\\n    `bash`\\r\\n    ```\\r\\n    npm install @azure/communication-calling --save\\r\\n    ```\\r\\n\\r\\n#### Creating the UI for Calling\\r\\n\\r\\nNow, let\'s create the basic structure of our frontend application. This includes setting up HTML, CSS, and JavaScript files.\\r\\n\\r\\n1. **HTML**: Create an index.html file in your project root. This file will serve as the entry point for your application.\\r\\n\\r\\n    `html`\\r\\n    ```\\r\\n    <!DOCTYPE html> \\r\\n    <html lang=\\"en\\"> \\r\\n    <head> \\r\\n        <meta charset=\\"UTF-8\\"> \\r\\n        <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1.0\\"> \\r\\n        <title>Real-time Voice Sentiment Analysis</title> \\r\\n        <link rel=\\"stylesheet\\" href=\\"styles.css\\">\\r\\n    </head> \\r\\n    <body> \\r\\n        <div id=\\"app\\"> \\r\\n            <input type=\\"text\\" id=\\"callee-phone-input\\" placeholder=\\"Enter Phone Number\\"> \\r\\n            <button id=\\"call-phone-button\\">Start Call</button> \\r\\n            <button id=\\"hang-up-phone-button\\" disabled>Hang Up</button> \\r\\n            <div id=\\"captionsArea\\"></div> \\r\\n        </div> \\r\\n        <script src=\\"app.js\\"><\/script> \\r\\n    </body> \\r\\n    </html> \\r\\n    ```\\r\\n\\r\\n1. **CSS**: Create a `styles.css` file to style your application. Feel free to customize the styles as per your preference.\\r\\n\\r\\n    `css`\\r\\n    ```\\r\\n    body { \\r\\n        font-family: Arial, sans-serif; \\r\\n        display: flex; \\r\\n        justify-content: center; \\r\\n        align-items: center; \\r\\n        height: 100vh; \\r\\n        margin: 0; \\r\\n        background-color: #f5f5f5; \\r\\n    } \\r\\n\\r\\n    #app { \\r\\n        text-align: center; \\r\\n    } \\r\\n\\r\\n    input[type=\\"text\\"], button { \\r\\n        padding: 10px; \\r\\n        margin: 10px; \\r\\n        border: 1px solid #ccc; \\r\\n        border-radius: 5px; \\r\\n    } \\r\\n\\r\\n    #captionsArea { \\r\\n        margin-top: 20px; \\r\\n    } \\r\\n    ```\\r\\n\\r\\n1. **JavaScript**: Create an `app.js` file. This is where we\'ll write the logic for initializing voice calling and handling sentiment analysis.\\r\\n\\r\\nLet\'s start by setting up a basic structure for making and ending calls using the Azure Communication Calling SDK you previously installed. \\r\\n\\r\\n    `javascript`\\r\\n    ```\\r\\n    import { CallClient, CallAgent } from \'@azure/communication-calling\'; \\r\\n\\r\\n    let callAgent; \\r\\n    let acsPhoneNumber; \\r\\n    let tokenCredential; \\r\\n    let captions; \\r\\n\\r\\n    const calleeInput = document.getElementById(\'callee-phone-input\'); \\r\\n    const callButton = document.getElementById(\'call-phone-button\'); \\r\\n    const hangUpButton = document.getElementById(\'hang-up-phone-button\'); \\r\\n\\r\\n    async function initCallAgent() { \\r\\n        const callClient = new CallClient(); \\r\\n\\r\\n        // Hard code token for now, will be replaced with actual token later \\r\\n        tokenCredential = new AzureCommunicationTokenCredential(\\"<<TOKEN_FROM_AZURE_PORTAL>>\\"); \\r\\n        acsPhoneNumber = \\"<<PHONE_NUMBER_FROM_AZURE_PORTAL>>\\"; \\r\\n\\r\\n        callAgent = await callClient.createCallAgent(tokenCredential);  \\r\\n        attachCallListeners(); \\r\\n    } \\r\\n\\r\\n    function attachCallListeners() { \\r\\n        callButton.addEventListener(\'click\', startCall); \\r\\n        hangUpButton.addEventListener(\'click\', endCall); \\r\\n    } \\r\\n\\r\\n    async function startCall() { \\r\\n        const callee = calleeInput.value; \\r\\n        if (callee) { \\r\\n            const call = callAgent.startCall([{phoneNumber: callee}]); \\r\\n            // More call handling code here \\r\\n        } \\r\\n    } \\r\\n\\r\\n    async function endCall() { \\r\\n        if (call) { \\r\\n            await call.hangUp({forEveryone: true}); \\r\\n            // toggle button states \\r\\n            hangUpPhoneButton.disabled = true; \\r\\n            callPhoneButton.disabled = false; \\r\\n        } \\r\\n    } \\r\\n\\r\\n    initCallAgent(); \\r\\n    ```\\r\\n\\r\\n**Note**: Make sure to replace placeholders like `tokenCredential` with actual values obtained from your backend or Azure Communication Services resource.\\r\\n\\r\\n#### Testing and Running Your Application\\r\\n\\r\\n1. **Run Your Application**: Use Parcel to serve your application. Run the following command in your project root.\\r\\n\\r\\n    `bash`\\r\\n    ```\\r\\n    npx parcel index.html \\r\\n    ```\\r\\n    * Visit the URL provided by Parcel to view your application.\\r\\n\\r\\n1. **Testing**: Test the calling functionality by entering a valid phone number and clicking the \\"Start Call\\" button. Ensure you have backend support to handle getting a token for Azure Communication Services.\\r\\n\\r\\n#### Handle UserIds and GroupIds\\r\\n\\r\\nIf you didn\'t acquire a phone number above, we\'ll add code here to handle user and group ids. This will allow you to make calls without a phone number. To startCall() add:\\r\\n\\r\\n`javascript`\\r\\n```\\r\\n// start a call to phone \\r\\nconst callee = calleePhoneInput.value; \\r\\n\\r\\nconst guidPattern = /^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i; \\r\\nconst phoneNumberPattern = /^\\\\+\\\\d+$/; \\r\\nconst userIdPattern = /^8:acs:[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}_[0-9a-f-]+$/i; \\r\\n\\r\\nif (guidPattern.test(callee)) { \\r\\n    call = callAgent.join({ groupId: callee}); \\r\\n} else if (phoneNumberPattern.test(callee)) { \\r\\n    call = callAgent.startCall( \\r\\n        [{phoneNumber: callee}], { alternateCallerId: {phoneNumber: acsPhoneNumber} \\r\\n    }); \\r\\n} else if (userIdPattern.test(callee)) { \\r\\n    call = callAgent.startCall({ communicationUserId: callee }); \\r\\n} else { \\r\\n    console.error(\'Invalid input. Must be a phone number, user ID, or GUID\'); \\r\\n    return; \\r\\n} \\r\\n```\\r\\n#### Adding Captions\\r\\n\\r\\nUnder `More calling code here` add the following code to check for, and enable, captions:\\r\\n\\r\\n`javascript`\\r\\n```\\r\\ncaptions = call.feature(Features.Captions).captions; \\r\\ntranscript = []; \\r\\ntry { \\r\\n    if (!captions.isCaptionsFeatureActive) { \\r\\n        await captions.startCaptions({ spokenLanguage: \'en-us\' }); \\r\\n    } \\r\\n    captions.on(\'CaptionsReceived\', captionsReceivedHandler); \\r\\n} catch (e) { \\r\\n    console.error(\'startCaptions failed\', e); \\r\\n} \\r\\n```\\r\\n\\r\\nAdd a `captionsReceivedHandler` function to parse the captions and add it to the caption area:\\r\\n\\r\\n`javascript`\\r\\n```\\r\\nfunction captionsReceivedHandler(captionsReceivedEvent) { \\r\\n    let mri = \'\'; \\r\\n\\r\\n    if (captionData.speaker.identifier.kind === \'communicationUser\') { \\r\\n        mri = captionData.speaker.identifier.communicationUserId; \\r\\n        mri = mri.slice(-8); \\r\\n    } else if (captionData.speaker.identifier.kind === \'microsoftTeamsUser\') { \\r\\n        mri = captionData.speaker.identifier.microsoftTeamsUserId; \\r\\n        mri = mri.slice(-8); \\r\\n    } else if (captionData.speaker.identifier.kind === \'phoneNumber\') { \\r\\n        mri = captionData.speaker.identifier.phoneNumber; \\r\\n    } \\r\\n\\r\\n    let displayName = captionData.speaker.displayName || mri; \\r\\n\\r\\n    // Get the captions area container \\r\\n    let captionAreasContainer = document.getElementById(\'captionsArea\'); \\r\\n\\r\\n    // Generate a class name based on the MRI \\r\\n    const newClassName = `prefix${mri.replace(/[:\\\\-+]/g, \'\')}`; \\r\\n\\r\\n    // Generate the caption text \\r\\n    const captionText = `${captionData.timestamp.toUTCString()} ${displayName}: ${captionData.captionText ?? captionData.spokenText}`; \\r\\n\\r\\n    // Try to find an existing caption container \\r\\n    let captionContainer = captionAreasContainer.querySelector(`.${newClassName}[isNotFinal=\'true\']`); \\r\\n\\r\\n    // If no existing caption container was found, create a new one \\r\\n    if (!captionContainer) { \\r\\n        captionContainer = document.createElement(\'div\'); \\r\\n        captionContainer.setAttribute(\'isNotFinal\', \'true\'); \\r\\n        captionContainer.classList.add(newClassName, \'caption-item\'); \\r\\n        captionAreasContainer.appendChild(captionContainer); \\r\\n    } \\r\\n\\r\\n    // Set the caption text \\r\\n    captionContainer.textContent = captionText; \\r\\n\\r\\n    // If the caption is final, update the \'isNotFinal\' attribute \\r\\n    if (captionData.resultType === \'Final\') { \\r\\n        captionContainer.setAttribute(\'isNotFinal\', \'false\'); \\r\\n    } \\r\\n} \\r\\n```\\r\\n\\r\\nTest the calling functionality by entering a valid phone number (or user ID, or group ID) and clicking the \\"Start Call\\" button.\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nContinue to the\u202f[next part](https://azure.github.io/cloud-native/60daysofia/real-time-voice-sentiment-analysis-system-2)\u202fof this topic to further explore building, deploying and testing your intelligent app for a real=time voice analysis system."},{"id":"real-time-voice-sentiment-analysis-system-2","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/real-time-voice-sentiment-analysis-system-2","source":"@site/blog-60daysofIA/2024-04-17/real-time-voice-sentiment-analysis-system-2.md","title":"7.5 Real-time Voice Sentiment Analysis System 2","description":"In today\'s fast-paced digital world, understanding customer sentiment in real-time during voice calls can provide businesses with a competitive edge. This guide will show developers how to build a robust real-time voice sentiment analysis application using several key Azure services. In this second part, we will develop the backend to handle interactions with Azure Communication Services, Azure AI Language and Azure OpenAI, connect it to the frontend, and deploy to Azure Container Apps .","date":"2024-04-17T09:05:00.000Z","formattedDate":"April 17, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":15.745,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-04-17T09:05","slug":"real-time-voice-sentiment-analysis-system-2","title":"7.5 Real-time Voice Sentiment Analysis System 2","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In today\'s fast-paced digital world, understanding customer sentiment in real-time during voice calls can provide businesses with a competitive edge. This guide will show developers how to build a robust real-time voice sentiment analysis application using several key Azure services. In this second part, we will develop the backend to handle interactions with Azure Communication Services, Azure AI Language and Azure OpenAI, connect it to the frontend, and deploy to Azure Container Apps .","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"7.4 Real-time Voice Sentiment Analysis System 1","permalink":"/Cloud-Native/60DaysOfIA/real-time-voice-sentiment-analysis-system-1"},"nextItem":{"title":"8 Managing the Cost of Intelligent Apps","permalink":"/Cloud-Native/60DaysOfIA/managing-the-cost-of-intelligent-apps"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/real-time-voice-sentiment-analysis-system-2\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Join us on a learning journey to build intelligent apps on Azure. Read all about the upcoming #BuildIntelligentApps initiative on this post!\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/real-time-voice-sentiment-analysis-system-2\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/real-time-voice-sentiment-analysis-system-2\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n## Real-time Voice Sentiment Analysis System Using Azure Communication Services, Azure AI and Azure OpenAI (Part 2)\\r\\n\\r\\nIn the\u202f[first part](https://azure.github.io/Cloud-Native/60daysofIA/real-time-voice-sentiment-analysis-system-1)\u202fof this topic, we setup all the Azure resources like the [Azure Communication Services](https://learn.microsoft.com/azure/communication-services/overview?ocid=buildia24_60days_blogs) for VoIP and PSTN, [Azure AI Language](https://learn.microsoft.com/azure/ai-services/language-service/overview?ocid=buildia24_60days_blogs), [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview?ocid=buildia24_60days_blogs), and developed the frontend for a simple yet functional UI to make voice calls and display closed captions generated from the conversation. In this second part, we will develop the backend to handle interactions with Azure Communication Services, Azure AI Language and Azure OpenAI, connect it to the frontend, and deploy to [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?ocid=buildia24_60days_blogs). Let\u2019s get started!\\r\\n\\r\\n### Prerequisite\\r\\n\\r\\nTo follow this tutorial, ensure you have completed the [first part](https://azure.github.io/Cloud-Native/60daysofIA/real-time-voice-sentiment-analysis-system-1)\u202fof this topic.\\r\\n\\r\\n### Setting Up the Backend with ASP.NET Core\\r\\n\\r\\nThe backend of our real-time voice sentiment analysis application, built with ASP.NET Core, plays a crucial role. It will handle interactions with Azure Communication Services for voice calling, manage sentiment analysis with Azure AI Language and Azure OpenAI, and serve the necessary tokens and endpoints to our frontend. Let\'s set this up step by step.\\r\\n\\r\\n#### Step 1: Create a New ASP.NET Core Project\\r\\n\\r\\n1. **Open Your Command Line**: Navigate to the directory where you want to create your project.\\r\\n\\r\\n1. **Create the Project**: Execute the following command to create a new ASP.NET Core Web API project. This command will generate a basic template for a RESTful API application.\\r\\n\\r\\n    `bash`\\r\\n    ```\\r\\n    dotnet new webapi --no-https \\r\\n    ```\\r\\n**Note**: We are turning off HTTPS for simplicity in this guide. For production environments, HTTPS should be enabled and properly configured.\\r\\n\\r\\n#### Step 2: Add Azure SDK Packages\\r\\n\\r\\nOur backend needs to interact with Azure Communication Services, Azure AI Language, and Azure OpenAI. We\'ll add the necessary NuGet packages for these services. \\r\\n\\r\\n1. **Azure Communication Services SDK**: This package allows us to manage voice calling and access tokens.\\r\\n\\r\\n    `bash`\\r\\n    ```\\r\\n    dotnet add package Azure.Communication.CallingServer \\r\\n    ```\\r\\n\\r\\n1. **Azure AI Text Analytics SDK**: We\'ll use this for sentiment analysis through Azure AI Language.\\r\\n\\r\\n    `bash`\\r\\n    ```\\r\\n    dotnet add package Azure.AI.TextAnalytics \\r\\n    ```\\r\\n\\r\\n1. **Azure OpenAI SDK**: This package is used to interact with Azure OpenAI for more complex sentiment analysis.\\r\\n\\r\\n    `bash`\\r\\n    ```\\r\\n    dotnet add package Azure.AI.OpenAI --prerelease \\r\\n    ```\\r\\n\\r\\n#### Step 3: Set Up Configuration\\r\\n\\r\\n1. **appsettings.json**: Open or create the `appsettings.json` file in your project root. Add placeholders for the keys and endpoints for the Azure services. Here\u2019s an example configuration:\\r\\n\\r\\n    `json`\\r\\n    ```\\r\\n    { \\r\\n        \\"Logging\\": { \\r\\n            \\"LogLevel\\": { \\r\\n            \\"Default\\": \\"Information\\", \\r\\n            \\"Microsoft.AspNetCore\\": \\"Warning\\" \\r\\n            } \\r\\n        }, \\r\\n        \\"AllowedHosts\\": \\"*\\", \\r\\n        \\"AzureSettings\\": { \\r\\n            \\"AZURE_LANGUAGE_SERVICE_KEY\\": \\"YOUR_AZURE_LANGUAGE_KEY\\", \\r\\n            \\"AZURE_LANGUAGE_SERVICE_ENDPOINT\\": \\"YOUR_AZURE_OPENAI_ENDPOINT\\", \\r\\n            \\"OPENAI_ENDPOINT\\": \\"YOUR_AZURE_OPENAI_ENDPOINT\\", \\r\\n            \\"OPENAI_KEY\\": \\"YOUR_AZURE_OPENAI_KEY\\", \\r\\n            \\"OPENAI_DEPLOYMENT_NAME\\": \\"YOUR_AZURE_OPENAI_ENDPOINT_DEPLOYMENT\\", \\r\\n            \\"COMMUNICATION_CONNECTION_STRING\\": \\"YOUR_AZURE_COMMUNICATION_SERVICES_CONNECTION_STRING\\" \\r\\n        } \\r\\n    } \\r\\n    ```\\r\\n    * Replace the placeholders with your actual Azure resource keys and endpoints.\\r\\n\\r\\n1. **Configure Services in Program.cs**: Read the endpoint from appsettings.json and configure the services in the `Program.cs` file.\\r\\n\\r\\n    `csharp`\\r\\n    ```\\r\\n    // Set keys and configuration \\r\\n    var azureSettings = builder.Configuration.GetSection(\\"AzureSettings\\"); \\r\\n\\r\\n    string GetSetting(string key) => azureSettings[key] ?? throw new Exception($\\"{key} is not set\\"); \\r\\n\\r\\n    var languageKey = GetSetting(\\"AZURE_LANGUAGE_SERVICE_KEY\\"); \\r\\n    var languageEndpoint = GetSetting(\\"AZURE_LANGUAGE_SERVICE_ENDPOINT\\"); \\r\\n    AzureKeyCredential credentials = new AzureKeyCredential(languageKey); \\r\\n    Uri endpoint = new Uri(languageEndpoint); \\r\\n    var textAnalyticsClient = new TextAnalyticsClient(endpoint, credentials); \\r\\n\\r\\n    var OPENAI_ENDPOINT = GetSetting(\\"OPENAI_ENDPOINT\\"); \\r\\n    var OPENAI_KEY = GetSetting(\\"OPENAI_KEY\\"); \\r\\n    var OPENAI_DEPLOYMENT_NAME = GetSetting(\\"OPENAI_DEPLOYMENT_NAME\\"); \\r\\n    OpenAIClient openAIClient = new OpenAIClient( \\r\\n        new Uri(OPENAI_ENDPOINT), \\r\\n        new AzureKeyCredential(OPENAI_KEY)); \\r\\n\\r\\n    var communicationConnectionString = GetSetting(\\"COMMUNICATION_CONNECTION_STRING\\"); \\r\\n    var communicationClient = new CommunicationIdentityClient(communicationConnectionString); \\r\\n    ```\\r\\n\\r\\n1. **Implement the Token Endpoint**:\\r\\n\\r\\nIn your ASP.NET Core project Program.cs, create a new endpoint to generate and return an access token. We\'ll call this endpoint from our client later.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\napp.MapGet(\\"api/token\\", () => \\r\\n{ \\r\\n    var identityAndTokenResponse = communicationClient.CreateUserAndToken(scopes: [CommunicationTokenScope.VoIP]); \\r\\n\\r\\n\\r\\n    var result = new TokenResponse() \\r\\n    { \\r\\n        Token = identityAndTokenResponse.Value.AccessToken.Token, \\r\\n        PhoneNumber = \\"+12533192954\\", \\r\\n    }; \\r\\n\\r\\n    return Results.Json(result); \\r\\n}) \\r\\n.WithName(\\"Token\\") \\r\\n.WithOpenApi(); \\r\\n```\\r\\n\\r\\nWe\'ll also need a TokenResponse object to return the token and phone number:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\npublic class TokenResponse \\r\\n{ \\r\\n    public string Token { get; set; } \\r\\n    public string PhoneNumber { get; set; } \\r\\n} \\r\\n```\\r\\n\\r\\n#### Step 4: Implement Sentiment Analysis Endpoints\\r\\n\\r\\nWith the services configured, you can now implement the API endpoints needed for your application. Here are the essentials:\\r\\n\\r\\n1. **Token Provisioning**: An endpoint to provide access tokens for Azure Communication Services.\\r\\n\\r\\n1. **Sentiment Analysis**: Endpoints to analyze sentiment using either Azure AI Language or Azure OpenAI.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\napp.MapPost(\\"api/sentiment\\", (SentimentRequest request) => \\r\\n{ \\r\\n    var result = SentimentAnalysisWithAzureLanguage(request.Content); \\r\\n\\r\\n\\r\\n    return Results.Json(result); \\r\\n}) \\r\\n.WithName(\\"Sentiment\\") \\r\\n.WithOpenApi(); \\r\\n```\\r\\n\\r\\nThis endpoint takes a SentimentRequest object and returns a SentimentResponse object and calls one of our sentiment analysis methods, which we\'ll implement shortly\\r\\n\\r\\n`csharp`\\r\\n```\\r\\npublic class SentimentRequest \\r\\n{ \\r\\n    public string Analyzer { get; set; } = string.Empty; \\r\\n    public string ParticipantToAnalyze { get; set; } = string.Empty; \\r\\n    public string Content { get; set; } = string.Empty; \\r\\n} \\r\\n\\r\\npublic class SentimentResult \\r\\n{ \\r\\n    public string Sentiment { get; set; } = string.Empty; \\r\\n    public double PositiveContentScore { get; set; } \\r\\n} \\r\\n```\\r\\n\\r\\nWe\'ll create a method that uses Azure AI Language and another that uses Azure OpenAI. Here\'s an example of how to implement sentiment analysis using Azure AI Language:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nSentimentResult SentimentAnalysisWithAzureLanguage(string document) \\r\\n{ \\r\\n    var review = textAnalyticsClient.AnalyzeSentiment(document); \\r\\n    return new SentimentResult() \\r\\n    { \\r\\n        Sentiment = review.Value.Sentiment.ToString(), \\r\\n        PositiveContentScore = review.Value.ConfidenceScores.Positive \\r\\n    }; \\r\\n} \\r\\n```\\r\\n\\r\\nAnd we\'ll create method endpoint that uses Azure OpenAI:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nSentimentResult SentimentAnalysisWithGPT(string document) \\r\\n{ \\r\\n    var chatCompletionsOptions = new ChatCompletionsOptions() \\r\\n    { \\r\\n        DeploymentName = \\"shawn-deployment\\", \\r\\n        Messages = \\r\\n            { \\r\\n                new ChatRequestSystemMessage(SentimentAnalysisPrompt), \\r\\n                new ChatRequestUserMessage(document), \\r\\n            }, \\r\\n        Temperature = (float)1, \\r\\n        MaxTokens = 800             \\r\\n    }; \\r\\n\\r\\n    Response<ChatCompletions> response = openAIClient.GetChatCompletions(chatCompletionsOptions); \\r\\n\\r\\n    return new SentimentResult() { \\r\\n        Sentiment = response.Value.Choices[0].Message.Content \\r\\n    }; \\r\\n} \\r\\n```\\r\\n\\r\\n#### Step 5: Test Your Backend\\r\\n\\r\\nBefore moving forward, make sure to test your backend. Use tools like Postman or Swagger UI to ensure your endpoints are responsive and returning the expected results. \\r\\n\\r\\nYour final backend should look like this: \\r\\n\\r\\n`csharp`\\r\\n```\\r\\nusing Azure; \\r\\nusing Azure.AI.TextAnalytics; \\r\\nusing Azure.AI.OpenAI; \\r\\nusing Azure.Communication.Identity; \\r\\n\\r\\nvar builder = WebApplication.CreateBuilder(args); \\r\\n\\r\\n// Add services to the container. \\r\\n// Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle \\r\\nbuilder.Services.AddEndpointsApiExplorer(); \\r\\nbuilder.Services.AddSwaggerGen(); \\r\\n\\r\\n// Set keys and configuration \\r\\nvar azureSettings = builder.Configuration.GetSection(\\"AzureSettings\\"); \\r\\n\\r\\nstring GetSetting(string key) => azureSettings[key] ?? throw new Exception($\\"{key} is not set\\"); \\r\\n\\r\\nvar languageKey = GetSetting(\\"AZURE_LANGUAGE_SERVICE_KEY\\"); \\r\\nvar languageEndpoint = GetSetting(\\"AZURE_LANGUAGE_SERVICE_ENDPOINT\\"); \\r\\nAzureKeyCredential credentials = new AzureKeyCredential(languageKey); \\r\\nUri endpoint = new Uri(languageEndpoint); \\r\\nvar textAnalyticsClient = new TextAnalyticsClient(endpoint, credentials); \\r\\n\\r\\nvar OPENAI_ENDPOINT = GetSetting(\\"OPENAI_ENDPOINT\\"); \\r\\nvar OPENAI_KEY = GetSetting(\\"OPENAI_KEY\\"); \\r\\nvar OPENAI_DEPLOYMENT_NAME = GetSetting(\\"OPENAI_DEPLOYMENT_NAME\\"); \\r\\nOpenAIClient openAIClient = new OpenAIClient( \\r\\n    new Uri(OPENAI_ENDPOINT), \\r\\n    new AzureKeyCredential(OPENAI_KEY)); \\r\\n\\r\\nvar communicationConnectionString = GetSetting(\\"COMMUNICATION_CONNECTION_STRING\\"); \\r\\nvar communicationClient = new CommunicationIdentityClient(communicationConnectionString); \\r\\n\\r\\nconst string SentimentAnalysisPrompt = \\"Please analyze the sentiment of the following text. The sentiment can be positive, negative, or neutral. Response on with one of those three values\\"; \\r\\n\\r\\nvar app = builder.Build(); \\r\\n\\r\\n// Configure the HTTP request pipeline. \\r\\nif (app.Environment.IsDevelopment()) \\r\\n{ \\r\\n    app.UseSwagger(); \\r\\n    app.UseSwaggerUI(); \\r\\n} \\r\\n\\r\\napp.MapGet(\\"api/token\\", () => \\r\\n{ \\r\\n    var identityAndTokenResponse = communicationClient.CreateUserAndToken(scopes: [CommunicationTokenScope.VoIP]); \\r\\n\u202f \\r\\n\\r\\n    var result = new TokenResponse() \\r\\n    { \\r\\n        Token = identityAndTokenResponse.Value.AccessToken.Token, \\r\\n        PhoneNumber = \\"+12533192954\\", \\r\\n    }; \\r\\n\\r\\n    return Results.Json(result); \\r\\n}) \\r\\n.WithName(\\"Token\\") \\r\\n.WithOpenApi(); \\r\\n\\r\\napp.MapPost(\\"api/sentiment\\", (SentimentRequest request) => \\r\\n{ \\r\\n    var result = SentimentAnalysisWithAzureLanguage(request.Content); \\r\\n\u202f \\r\\n\\r\\n    return Results.Json(result); \\r\\n}) \\r\\n.WithName(\\"Sentiment\\") \\r\\n.WithOpenApi(); \\r\\n\\r\\n\u202f\\r\\napp.Run(); \\r\\n\\r\\n\\r\\nSentimentResult SentimentAnalysisWithAzureLanguage(string document) \\r\\n{ \\r\\n    var review = textAnalyticsClient.AnalyzeSentiment(document); \\r\\n    return new SentimentResult() \\r\\n    { \\r\\n        Sentiment = review.Value.Sentiment.ToString(), \\r\\n        PositiveContentScore = review.Value.ConfidenceScores.Positive \\r\\n    }; \\r\\n} \\r\\n\\r\\n// // Function to get a response from OpenAI\'s ChatGPT \\r\\nSentimentResult SentimentAnalysisWithGPT(string document) \\r\\n{ \\r\\n    var chatCompletionsOptions = new ChatCompletionsOptions() \\r\\n    { \\r\\n        DeploymentName = \\"shawn-deployment\\", \\r\\n        Messages = \\r\\n            { \\r\\n                new ChatRequestSystemMessage(SentimentAnalysisPrompt), \\r\\n                new ChatRequestUserMessage(document), \\r\\n            }, \\r\\n        Temperature = (float)1, \\r\\n        MaxTokens = 800             \\r\\n    }; \\r\\n\\r\\n    Response<ChatCompletions> response = openAIClient.GetChatCompletions(chatCompletionsOptions); \\r\\n\\r\\n    return new SentimentResult() { \\r\\n        Sentiment = response.Value.Choices[0].Message.Content \\r\\n    }; \\r\\n} \\r\\npublic class TokenResponse \\r\\n{ \\r\\n    public string Token { get; set; } = string.Empty; \\r\\n    public string PhoneNumber { get; set; } = string.Empty; \\r\\n} \\r\\n\\r\\npublic class SentimentRequest \\r\\n{ \\r\\n    public string Analyzer { get; set; } = string.Empty; \\r\\n    public string ParticipantToAnalyze { get; set; } = string.Empty; \\r\\n    public string Content { get; set; } = string.Empty; \\r\\n} \\r\\n\\r\\npublic class SentimentResult \\r\\n{ \\r\\n    public string Sentiment { get; set; } = string.Empty; \\r\\n    public double PositiveContentScore { get; set; } \\r\\n} \\r\\n```\\r\\n\\r\\n:::info\\r\\nEarn Microsoft-verified credentials for cloud-native app development skills by passing the [Azure Container Apps](https://learn.microsoft.com/credentials/applied-skills/deploy-cloud-native-apps-using-azure-container-apps/?ocid=buildia24_60days_blogs) assessment to elevate your professional profile. \\r\\n:::\\r\\n\\r\\n### Connecting the Frontend with the ASP.NET Core Backend\\r\\n\\r\\nAfter setting up the backend and the frontend of our real-time voice sentiment analysis application separately, it\u2019s time to connect them. This connection ensures that our Node.js frontend can communicate with the ASP.NET Core backend to do things like obtaining access tokens for Azure Communication Services and conducting sentiment analysis through Azure AI Language and Azure OpenAI.\\r\\n\\r\\n#### Step 1: Get the Access Token\\r\\n\\r\\nFor our application to establish a call, the frontend needs to obtain an access token from Azure Communication Services. In the `app.js` file within your Node.js project, modify the `initCallAgent` function to fetch the token from the backend before initializing the call agent. Use the `fetch` API for this purpose:\\r\\n\\r\\n`javascript`\\r\\n```\\r\\nasync function initCallAgent() { \\r\\n    try { \\r\\n        const response = await fetch(\'/api/communications/token\'); \\r\\n        const { token, userId } = await response.json(); \\r\\n\\r\\n        const tokenCredential = new AzureCommunicationTokenCredential(token); \\r\\n        const callClient = new CallClient(); \\r\\n        callAgent = await callClient.createCallAgent(tokenCredential, { displayName: \'Caller\' }); \\r\\n\\r\\n        attachCallListeners(); \\r\\n    } catch (error) { \\r\\n        console.error(\'Failed to obtain token:\', error); \\r\\n    } \\r\\n} \\r\\n```\\r\\n\\r\\n#### Step 2: Implement Sentiment Analysis on Captions\\r\\n\\r\\nOur application should analyze the sentiment of the transcribed call captions. We\u2019ll capture these captions on the frontend, then send them to our backend for sentiment analysis.\\r\\n\\r\\n1. **Make Sentiment Analysis Request:**\\r\\n\\r\\nIn the `captionsReceivedHandler`, make a request to your backend sentiment analysis endpoint whenever captions are received. Update your captions event handler, under `if (captionData.resultType === \'Final\')` to include this:\\r\\n\\r\\n`javascript`\\r\\n```\\r\\nif (captionData.resultType === \'Final\') { \\r\\n    captionContainer.setAttribute(\'isNotFinal\', \'false\'); \\r\\n    transcript.push(captionText); \\r\\n\\r\\n    // Call the sentiment service \\r\\n    fetch(\'/api/sentiment\', { \\r\\n        method: \'POST\', \\r\\n        headers: { \\r\\n            \'Content-Type\': \'application/json\' \\r\\n        }, \\r\\n        body: JSON.stringify({ content: transcript.join(\'\\\\n\') }) \\r\\n    }) \\r\\n    .then(response => response.json()) \\r\\n    .then(data => { \\r\\n        // Handle the sentiment response \\r\\n        const sentimentText = `Sentiment:${data.sentiment} (${data.positiveContentScore})`; \\r\\n        document.getElementById(\'sentimentScore\').textContent = captionText; \\r\\n\\r\\n        //update sentiment UI \\r\\n        updateSentimentMeter(data.positiveContentScore); \\r\\n\\r\\n    }); \\r\\n} \\r\\n```\\r\\n\\r\\n#### Step 3: Display Analysis Results\\r\\n\\r\\nFinally, utilize the sentiment analysis data received from the backend to inform the user about the call\u2019s sentiment in real-time.\\r\\n\\r\\n1. **Update the UI Dynamically**:\\r\\n\\r\\nEnhance your frontend to dynamically display sentiment analysis results as they are received. We\'ll add an indicator for positive, neutral, or negative sentiment.\\r\\n\\r\\n`javascript`\\r\\n```\\r\\nfunction updateSentimentMeter(score) { \\r\\n    // Ensure score is between 0 and 1 \\r\\n    score = Math.max(0, Math.min(score, 1)); \\r\\n\\r\\n    // Convert score to angle: -90\xb0 for 0, 90\xb0 for 1 \\r\\n    var angle = score * 180 - 90; \\r\\n\\r\\n    // Rotate the arrow to the corresponding angle \\r\\n    document.getElementById(\'meterArrow\').style.transform = \'rotate(\' + angle + \'deg)\'; \\r\\n} \\r\\n```\\r\\n\\r\\nCreate new elements in the HTML to display the sentiment score and meter:\\r\\n\\r\\n`html`\\r\\n```\\r\\n<div id=\\"captionsArea\\" ></div> \\r\\n<h4>Sentiment Score</h4> \\r\\n<div id=\\"sentimentMeter\\" class=\\"meter\\"> \\r\\n    <div id=\\"meterArrow\\" class=\\"arrow\\"></div> \\r\\n<div id=\\"sentimentScore\\" ></div> \\r\\n```\\r\\n\\r\\nYour complete code for app.js should look like this:\\r\\n\\r\\n`javascript`\\r\\n```\\r\\nimport { CallClient, CallAgent, Features } from \\"@azure/communication-calling\\"; \\r\\nimport { AzureCommunicationTokenCredential } from \'@azure/communication-common\'; \\r\\n\\r\\nlet call; \\r\\nlet callAgent; \\r\\n\\r\\nconst calleePhoneInput = document.getElementById(\\"callee-phone-input\\"); \\r\\nconst callPhoneButton = document.getElementById(\\"call-phone-button\\"); \\r\\nconst hangUpPhoneButton = document.getElementById(\\"hang-up-phone-button\\"); \\r\\n\\r\\nlet acsPhoneNumber; \\r\\nlet tokenCredential; \\r\\n\\r\\nlet captions; \\r\\n\\r\\nasync function init() { \\r\\n\\r\\n    const response = await fetch(\'/api/token\');  \\r\\n    const { token, phoneNumber } = await response.json(); \\r\\n\\r\\n    tokenCredential = new AzureCommunicationTokenCredential(token); \\r\\n    acsPhoneNumber = phoneNumber; \\r\\n\\r\\n    const callClient = new CallClient(); \\r\\n    callAgent = await callClient.createCallAgent(tokenCredential); \\r\\n} \\r\\n\\r\\ninit(); \\r\\n\\r\\ncallPhoneButton.addEventListener(\\"click\\", () => { \\r\\n    // start a call to phone \\r\\n    const endpointToCall = calleePhoneInput.value; \\r\\n\\r\\n    const guidPattern = /^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i; \\r\\n    const phoneNumberPattern = /^\\\\+\\\\d+$/; \\r\\n    const userIdPattern = /^8:acs:[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}_[0-9a-f-]+$/i; \\r\\n\\r\\n    if (guidPattern.test(endpointToCall)) { \\r\\n        call = callAgent.join({ groupId: endpointToCall}); \\r\\n    } else if (phoneNumberPattern.test(endpointToCall)) { \\r\\n        call = callAgent.startCall( \\r\\n            [{phoneNumber: endpointToCall}], { alternateCallerId: {phoneNumber: acsPhoneNumber} \\r\\n        }); \\r\\n    } else if (userIdPattern.test(endpointToCall)) { \\r\\n        call = callAgent.startCall({ communicationUserId: endpointToCall }); \\r\\n    } else { \\r\\n        console.error(\'Invalid input. Must be a phone number, user ID, or GUID\'); \\r\\n        return; \\r\\n    } \\r\\n\u202f \\r\\n\\r\\n    call.on(\'stateChanged\', async () => { \\r\\n        console.log(`Call state: ${call.state}`); \\r\\n        if(call.state === \'Connected\') { \\r\\n            console.log(\'Call connected\'); \\r\\n                 \\r\\n\\r\\n            captions = call.feature(Features.Captions).captions; \\r\\n            transcript = []; \\r\\n            try { \\r\\n                if (!captions.isCaptionsFeatureActive) { \\r\\n                    await captions.startCaptions({ spokenLanguage: \'en-us\' }); \\r\\n                } \\r\\n                captions.on(\'CaptionsReceived\', captionsReceivedHandler); \\r\\n            } catch (e) { \\r\\n                console.error(\'startCaptions failed\', e); \\r\\n            } \\r\\n        } \\r\\n    }); \\r\\n\\r\\n    hangUpPhoneButton.disabled = false; \\r\\n    callPhoneButton.disabled = true; \\r\\n  }); \\r\\n\\r\\n  hangUpPhoneButton.addEventListener(\\"click\\", () => { \\r\\n    // end the current call \\r\\n    call.hangUp({ \\r\\n      forEveryone: true \\r\\n    }); \\r\\n\\r\\n    // toggle button states \\r\\n    hangUpPhoneButton.disabled = true; \\r\\n    callPhoneButton.disabled = false; \\r\\n  }); \\r\\n\\r\\n  const captionsReceivedHandler = (captionData) => { \\r\\n    let mri = \'\'; \\r\\n\\r\\n    if (captionData.speaker.identifier.kind === \'communicationUser\') { \\r\\n        mri = captionData.speaker.identifier.communicationUserId; \\r\\n        mri = mri.slice(-8); \\r\\n    } else if (captionData.speaker.identifier.kind === \'microsoftTeamsUser\') { \\r\\n        mri = captionData.speaker.identifier.microsoftTeamsUserId; \\r\\n        mri = mri.slice(-8); \\r\\n    } else if (captionData.speaker.identifier.kind === \'phoneNumber\') { \\r\\n        mri = captionData.speaker.identifier.phoneNumber; \\r\\n    } \\r\\n\\r\\n    let displayName = captionData.speaker.displayName || mri; \\r\\n\\r\\n    // Get the captions area container \\r\\n    let captionAreasContainer = document.getElementById(\'captionsArea\'); \\r\\n\\r\\n    // Generate a class name based on the MRI \\r\\n    const newClassName = `prefix${mri.replace(/[:\\\\-+]/g, \'\')}`; \\r\\n\\r\\n    // Generate the caption text \\r\\n    const captionText = `${captionData.timestamp.toUTCString()} ${displayName}: ${captionData.captionText ?? captionData.spokenText}`; \\r\\n\\r\\n    // Try to find an existing caption container \\r\\n    let captionContainer = captionAreasContainer.querySelector(`.${newClassName}[isNotFinal=\'true\']`); \\r\\n\\r\\n    // If no existing caption container was found, create a new one \\r\\n    if (!captionContainer) { \\r\\n        captionContainer = document.createElement(\'div\'); \\r\\n        captionContainer.setAttribute(\'isNotFinal\', \'true\'); \\r\\n        captionContainer.classList.add(newClassName, \'caption-item\'); \\r\\n        captionAreasContainer.appendChild(captionContainer); \\r\\n    } \\r\\n\\r\\n    // Set the caption text \\r\\n    captionContainer.textContent = captionText; \\r\\n\\r\\n    // If the caption is final, update the \'isNotFinal\' attribute \\r\\n    if (captionData.resultType === \'Final\') { \\r\\n        captionContainer.setAttribute(\'isNotFinal\', \'false\'); \\r\\n        transcript.push(captionText); \\r\\n\\r\\n\\r\\n         // Call the sentiment service \\r\\n         fetch(\'/api/sentiment\', { \\r\\n            method: \'POST\', \\r\\n            headers: { \\r\\n                \'Content-Type\': \'application/json\' \\r\\n            }, \\r\\n            body: JSON.stringify({ content: transcript.join(\'\\\\n\') }) \\r\\n        }) \\r\\n        .then(response => response.json()) \\r\\n        .then(data => { \\r\\n            // Handle the sentiment response \\r\\n            const sentimentText = `Sentiment:${data.sentiment} (${data.positiveContentScore})`; \\r\\n            document.getElementById(\'sentimentScore\').textContent = captionText; \\r\\n            updateSentimentMeter(data.positiveContentScore); \\r\\n        }); \\r\\n    } \\r\\n\\r\\n    function updateSentimentMeter(score) { \\r\\n        // Ensure score is between 0 and 1 \\r\\n        score = Math.max(0, Math.min(score, 1)); \\r\\n\\r\\n        // Convert score to angle: -90\xb0 for 0, 90\xb0 for 1 \\r\\n        var angle = score * 180 - 90; \\r\\n\\r\\n        // Rotate the arrow to the corresponding angle \\r\\n        document.getElementById(\'meterArrow\').style.transform = \'rotate(\' + angle + \'deg)\'; \\r\\n    } \\r\\n}; \\r\\n```\\r\\n\\r\\n... and the HTML should look like this\\r\\n\\r\\n`HTML`\\r\\n```\\r\\n<!DOCTYPE html> \\r\\n<html lang=\\"en\\"> \\r\\n<head> \\r\\n    <meta charset=\\"utf-8\\"> \\r\\n    <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1.0\\"> \\r\\n    <title>Azure Communication Service - Realtime Sentiment Analysis</title> \\r\\n    <link rel=\\"stylesheet\\" href=\\"styles.css\\"> \\r\\n  </head> \\r\\n  <body> \\r\\n    <h4>Azure Communication Services</h4> \\r\\n    <input  \\r\\n      id=\\"callee-phone-input\\" \\r\\n      type=\\"text\\" \\r\\n      placeholder=\\"Who would you like to call?\\" \\r\\n    /> \\r\\n    <div> \\r\\n      <button id=\\"call-phone-button\\" type=\\"button\\"> \\r\\n        Start Call \\r\\n      </button> \\r\\n      &nbsp; \\r\\n      <button id=\\"hang-up-phone-button\\" type=\\"button\\" disabled=\\"true\\"> \\r\\n        Hang Up \\r\\n      </button> \\r\\n      <div id=\\"captionsArea\\" ></div> \\r\\n      <h4>Sentiment Score</h4> \\r\\n      <div id=\\"sentimentMeter\\" class=\\"meter\\"> \\r\\n        <div id=\\"meterArrow\\" class=\\"arrow\\"></div> \\r\\n      <div id=\\"sentimentScore\\" ></div> \\r\\n    </div> \\r\\n\\r\\n    </div> \\r\\n    <script src=\\"./app.js\\" type=\\"module\\"><\/script> \\r\\n  </body> \\r\\n</html> \\r\\n```\\r\\n\\r\\n#### Testing the Complete System\\r\\n\\r\\nWith the frontend and backend now connected, thoroughly test your application:\\r\\n\\r\\n* Ensure the voice call setup and teardown works flawlessly.\\r\\n* Verify that captions are accurately captured and displayed. \\r\\n* Confirm that sentiment analysis requests are successfully handled and that the results make sense for the given captions.\\r\\n\\r\\n:::info\\r\\nExplore a variety of Azure Container Apps [code samples](https://learn.microsoft.com/samples/browse/?terms=azure%20container%20apps&ocid=buildia24_60days_blogs) for a quick start to your intelligent app development with cloud-native technologies. \\r\\n:::\\r\\n\\r\\n### Deployment\\r\\n\\r\\nWith the real-time voice sentiment analysis system fully developed, including both the frontend and backend components, the next critical step is deployment. We\'ll deploy the ASP.NET Core backend using Azure Container Apps and the Node.js frontend through Azure Static Web Apps. \\r\\n\\r\\n#### Step 1: Prepare the Backend for Deployment\\r\\n\\r\\n##### Create a Dockerfile\\r\\n\\r\\n1. **Navigate to Your Backend Project Directory**: Open the terminal or command prompt and ensure you\'re in the root directory of your ASP.NET Core project.\\r\\n\\r\\n1. **Create a Dockerfile**: In the root of your project, create a file named `Dockerfile` with no file extension. This file will contain instructions for building a Docker image for your application.\\r\\n\\r\\n1. **Define the Dockerfile Contents**: Open the `Dockerfile` in your editor and add the following content:\\r\\n\\r\\n`Dockerfile`\\r\\n```\\r\\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base \\r\\nWORKDIR /app \\r\\nEXPOSE 80 \\r\\n\\r\\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build \\r\\nWORKDIR /src \\r\\nCOPY [\\"VoiceSentimentBackend.csproj\\", \\".\\"] \\r\\nRUN dotnet restore \\"./VoiceSentimentBackend.csproj\\" \\r\\nCOPY . . \\r\\nWORKDIR \\"/src/.\\" \\r\\nRUN dotnet build \\"VoiceSentimentBackend.csproj\\" -c Release -o /app/build \\r\\n\\r\\nFROM build AS publish \\r\\nRUN dotnet publish \\"VoiceSentimentBackend.csproj\\" -c Release -o /app/publish \\r\\n\\r\\nFROM base AS final \\r\\nWORKDIR /app \\r\\nCOPY --from=publish /app/publish . \\r\\nENTRYPOINT [\\"dotnet\\", \\"VoiceSentimentBackend.dll\\"] \\r\\n```\\r\\n\\r\\n**Note**: Adjust `\\"VoiceSentimentBackend.csproj\\"` to match your project\'s name.\\r\\n\\r\\n##### Create and Test the Docker Image Locally\\r\\n\\r\\n1. **Build the Docker Image**: Run the following command in your terminal:\\r\\n\\r\\n    `bash`\\r\\n    ```\\r\\n    docker build -t voicesentimentbackend .\\r\\n    ```\\r\\n\\r\\n1. **Run Your Docker Container**: To test the Docker container locally, execute:\\r\\n\\r\\n    `bash`\\r\\n    ```\\r\\n    docker run -d -p 8080:80 --name myapp voicesentimentbackend\\r\\n    ```\\r\\n\\r\\n1. **Verify**: Open your browser and navigate to `http://localhost:8080/api/token` to ensure your application is running correctly in the container.\\r\\n\\r\\n#### Step 2: Deploy the Backend to Azure Container Apps\\r\\n\\r\\nAzure Container Apps offers a fully managed serverless container service. It\'s an excellent choice for deploying containers without managing complex infrastructure. \\r\\n\\r\\n1. **Create a Container App Environment**: Use VS Code to create an Azure Container App and deploy your backend. Follow the [official documentation](https://learn.microsoft.com/azure/container-apps/deploy-visual-studio-code?ocid=buildia24_60days_blogs) for detailed instructions. Type Crtl+Shift+P to open the command palette and type \\"Azure Container Apps: Create New Container App\\" and follow the prompts.\\r\\n\\r\\n1. **Verify Deployment**: Use the FQDN obtained from the previous command to verify your application is accessible via the internet.\\r\\n\\r\\n#### Step 3: Deploy the Frontend to Azure Static Web Apps\\r\\n\\r\\nAzure Static Web Apps is a service tailored for static web applications, providing global distribution, serverless APIs, and seamless integration with GitHub for continuous deployment.\\r\\n\\r\\n##### Setting Up Continuous Deployment with GitHub\\r\\n\\r\\n1. **Push Your Frontend Code to GitHub**: Ensure your Node.js frontend code is in a GitHub repository.\\r\\n\\r\\n1. **Create a Static Web App Resource**: In the Azure Portal, create a new Static Web App resource and connect it to your GitHub repository. Specify your build details during the setup.\\r\\n\\r\\n1. **Verify Deployment**: Once the GitHub Actions workflow is completed, your Static Web App will be accessible via the provided URL. Check it to ensure your frontend is live and functional.\\r\\n\\r\\n#### Conclusion\\r\\n\\r\\nOver the course of this guide, you\'ve:\\r\\n\\r\\n* Set up and configured critical Azure resources, including Azure Communication Services for managing voice calls, Azure AI Language for basic sentiment analysis, and Azure OpenAI for more nuanced sentiment insights. \\r\\n* Developed a Node.js frontend to interface with users, initiating and managing voice calls, and dynamically displaying sentiment analysis results. \\r\\n* Implemented an ASP.NET Core backend to handle business logic, interact with Azure services, and provide APIs for the frontend. \\r\\n* Connected your frontend and backend, ensuring seamless communication and data flow within your application. \\r\\n* Deployed your application to Azure, leveraging Azure Container Apps for the backend and Azure Static Web Apps for the frontend, making your project accessible from anywhere.\\r\\n\\r\\n#### Future Enhancements\\r\\n\\r\\nWhile your current application is fully functional, there\'s always room for improvement and expansion. Consider the following possibilities for future development:\\r\\n\\r\\n* **Language Support**: Expand the application to support additional languages, enhancing its accessibility and utility across different geographical locations. \\r\\n* **Advanced AI Insights**: Explore deeper insights beyond sentiment, such as emotional tone, intent recognition, or specific topic detection, to provide more detailed analysis of voice calls."},{"id":"managing-the-cost-of-intelligent-apps","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/managing-the-cost-of-intelligent-apps","source":"@site/blog-60daysofIA/2024-04-15/managing-the-cost-of-intelligent-apps.md","title":"8 Managing the Cost of Intelligent Apps","description":"This article explores budgetary planning to maximize the value of incorporating Intelligent Apps  into your workflow without sacrificing your bottom line.","date":"2024-04-17T09:10:00.000Z","formattedDate":"April 17, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":8.73,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-04-17T09:10","slug":"managing-the-cost-of-intelligent-apps","title":"8 Managing the Cost of Intelligent Apps","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"This article explores budgetary planning to maximize the value of incorporating Intelligent Apps  into your workflow without sacrificing your bottom line.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"7.5 Real-time Voice Sentiment Analysis System 2","permalink":"/Cloud-Native/60DaysOfIA/real-time-voice-sentiment-analysis-system-2"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/managing-the-cost-of-intelligent-apps\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Join us on a learning journey to build intelligent apps on Azure. Read all about the upcoming #BuildIntelligentApps initiative on this post!\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/managing-the-cost-of-intelligent-apps\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/managing-the-cost-of-intelligent-apps\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![A two-dimensional digital illustration of interconnected dots in a complex web. Four dots are slightly larger and contain dollar signs.](../../static/img/60-days-of-ia/blogs/2024-04-15/8-1-1.jpeg)\\r\\n\\r\\nIntelligent Apps leverage advanced technologies like artificial intelligence (AI), machine learning (ML), and data analytics. In the commercial sphere, they harness business intelligence to facilitate and automate everyday employee and customer actions.\\r\\n\\r\\nThe democratization of these technologies has broadened access to Intelligent Apps, with AI- and ML-driven cloud platforms like Azure greatly reducing technical barriers. Scalable infrastructures, pre-built AI services, data storage and processing capabilities, and robust security and compliance features simplify development processes, foster collaboration, and accelerate innovation.\\r\\n\\r\\nWhile incorporating Intelligent Apps into your workflow comes with financial considerations, smart budgetary planning helps maximize their value without sacrificing your bottom line.\\r\\n\\r\\n### Understanding Costs in Intelligent App Development\\r\\n\\r\\nVarious factors affect the cost of generating and maintaining cloud-based Intelligent Apps, which are complex and often necessitate processing voluminous data.\\r\\n\\r\\nA key expense is the cloud platform itself, which normally offers tiered pricing plans that factor in data volume, number of users, and additional features. Next, you must account for compute resources \u2014 CPU, GPU, and TPU usage \u2014 and storage, which are essential for training data. Don\u2019t forget data transfer fees, staff and training, and developer tools and resources (like IDEs and CI/CD pipelines).\\r\\n\\r\\n#### Key Azure Services and Their Cost Implications\\r\\n\\r\\nSeveral essential Azure services help facilitate Intelligent App development, each with unique cost implications.\\r\\n\\r\\n#### **Azure Kubernetes Service (AKS)**\\r\\n\\r\\n[Azure Kubernetes](https://azure.microsoft.com/products/kubernetes-service?ocid=buildia24_60days_blogs) Service (AKS) is crucial in Intelligent App development, enabling scalable, reliable, and efficient deployment and management of containerized applications.\\r\\n\\r\\nThe [Free tier](https://azure.microsoft.com/pricing/details/kubernetes-service/?ocid=buildia24_60days_blogs) represents an excellent starting point, offering free cluster management and pay-for-use virtual machines (VMs), associated storage, and networking resources. For building at scale, consider upgrading to the Standard tier for a scalable Kubernetes control plane, a guaranteed service level agreement (SLA), and an elevated node limit per cluster. \\r\\n\\r\\n:::info\\r\\nIf you are trying to understand Retrieval Augmented Generation (RAG) applications, scenarios and how to leverage the power of GenAI in your enterprise apps, then\u202fcheckout this [live learning session](https://aka.ms/serverless-learn-live/ep4?ocid=buildia24_60days_blogs) with SMEs on how to build intelligent apps with Azure OpenAI, Semantic Kernal, Azure Functions and Azure Container Apps. \\r\\n:::\\r\\n\\r\\n#### **Azure Functions**\\r\\n\\r\\n[Azure Functions](https://learn.microsoft.com/azure/azure-functions/functions-overview?ocid=buildia24_60days_blogs?pivots=programming-language-csharp) provides a scalable, event-driven compute platform that integrates with other Azure AI services and supports microservices architecture. It also refines your development and deployment processes, blending easily into DevOps pipelines.\\r\\n\\r\\nThere are two straightforward [pricing options](https://azure.microsoft.com/pricing/details/functions/?ocid=buildia24_60days_blogs):\\r\\n\\r\\n* **Pay as you go** \u2014 Based on compute capacity per second\\r\\n* **Azure savings plan for compute** \u2014 A fixed hourly amount, accommodates fluctuations and dynamic workloads, one- or three-year commitment\\r\\n\\r\\n#### **Azure OpenAI Service**\\r\\n\\r\\n[Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview?ocid=buildia24_60days_blogs) Service provides access to state-of-the-art Open AI models and scalable computing resources. It integrates with Azure ecosystem services and developer-friendly APIs and remains committed to ethical AI practices.  \\r\\n\\r\\nLike Azure Functions, Azure OpenAI Service offers [two plans](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/?ocid=buildia24_60days_blogs):\\r\\n\\r\\n* **Pay-As-You-Go (PAYG)** \u2014 Pay only for resources you use.\\r\\n* **Provisioned Throughput Units (PTUs)** \u2014 Receive guaranteed throughput for scalable, predictable AI solutions.\\r\\n\\r\\n#### **Azure Container Apps**\\r\\n\\r\\n[Azure Container](https://learn.microsoft.com/azure/container-apps/overview?ocid=buildia24_60days_blogs) Apps provides a platform for building and deploying cloud-native applications utilizing serverless containers. A Kubernetes-based application platform, Azure Container Apps offers detailed observability, dynamic scaling, and end-to-end developer productivity capabilities in a scalable, portable managed platform. The [pricing model](https://azure.microsoft.com/pricing/details/container-apps/?ocid=buildia24_60days_blogs) includes two options:\\r\\n\\r\\n* **Pay as you go** \u2014 Pay for compute capacity by the second and increase or decrease consumption as needed. Consumption is measured by resource consumption in vCPU-seconds and GiB-seconds and the number of HTTP requests.\\r\\n\\r\\n* **Azure savings plan for compute** \u2014 A fixed hourly amount, this model accommodates fluctuations and dynamic workloads, one- or three-year commitment.\\r\\n\\r\\n:::info\\r\\nCheckout the quick demo bytes for **[Intelligent Apps with Azure Container Apps](https://developer.microsoft.com/en-us/reactor/series/S-1308/?wt.mc_id=blog_S-1308_webpage_reactor&ocid=buildia24_60days_blogs)** for a detailed walkthrough from the product team on using open-source vector database, Qrdrant, and building a multi-LLM chat application. \\r\\n:::\\r\\n\\r\\n#### **Azure Machine Learning**\\r\\n\\r\\n[Azure Machine Learning](https://learn.microsoft.com/azure/machine-learning/overview-what-is-azure-machine-learning?view=azureml-api-2?ocid=buildia24_60days_blogs) is also committed to ethical Intelligent App development at scale. It creates value through industry-leading ML operations (MLOps), open-source compatibility, and integrated tools.\\r\\n\\r\\nThis service has a [three-tier payment model](https://azure.microsoft.com/pricing/details/machine-learning/?ocid=buildia24_60days_blogs):\\r\\n\\r\\n* **Pay as you go** \u2014 Charges for compute capacity by the second  \\r\\n* **Azure savings plan for compute** \u2014 Offers fixed pricing for one or three years, accommodates dynamic workloads  \\r\\n* **Reservations** \u2014 Offers Azure Reserved Virtual Machine Instances for substantial cost savings. It\u2019s ideal for stable, predictable workloads.\\r\\n\\r\\n#### **Azure Cosmos DB**\\r\\n\\r\\n[Azure Cosmos DB](https://learn.microsoft.com/azure/cosmos-db/introduction?ocid=buildia24_60days_blogs) provides a scalable, globally distributed database service supporting multiple data models, low-latency data access, high availability, and seamless integration with other Azure services. Its [pricing model](https://azure.microsoft.com/pricing/details/cosmos-db/autoscale-provisioned/?ocid=buildia24_60days_blogs) centers on your preferred resource metric:\\r\\n\\r\\n* **Compute** \u2014 Billed per second using Request Units (RU), which are a proxy for compute, memory, and IO \\r\\n* **Storage** \u2014 You pay for transactional and analytical data and indexes, as well as backups. \\r\\n* **Bandwidth** \u2014 Azure charges for data that leaves the Azure cloud or travels through the Azure WAN between regions or availability zones.\\r\\n\\r\\nAdditionally, [Azure offers numerous AI/ML developer tools](https://azure.microsoft.com/products?ocid=buildia24_60days_blogs#developer-tools), data storage, processing, and hosting, enabling versatile and powerful Intelligent App creation.\\r\\n\\r\\nWhen budgeting and planning, consider the [Azure Pricing Calculator](https://learn.microsoft.com/azure/cost-management-billing/costs/pricing-calculator?ocid=buildia24_60days_blogs). It transforms expected resource usage into a projected cost, simplifying Azure services budgeting.\\r\\n\\r\\n#### Azure Kubernetes Service Cost Analysis Add-On\\r\\n\\r\\nFor more sophisticated and comprehensive cost analysis, there\u2019s the [Azure Kubernetes Service (AKS) cost analysis add-on](https://learn.microsoft.com/azure/aks/cost-analysis?ocid=buildia24_60days_blogs).\\r\\n\\r\\nThis tool provides detailed insights into the costs of AKS clusters and streamlines cost management within the Azure ecosystem. It\u2019s built on top of OpenCost, an open-source project under the Cloud Native Computing Foundation (CNCF) that tracks Kubernetes costs with high granularity. By leveraging OpenCost\'s capabilities within an Azure-native framework, the AKS cost analysis add-on eliminates the need for third-party solutions like Kubecost or the standalone OpenCost.\\r\\n\\r\\nThe AKS cost analysis add-on requires the AKS cluster to be in either the Standard or Premium tier. You also need certain Azure command-line interface (CLI) versions to register the ClusterCostAnalysis feature flag on your subscription.\\r\\n\\r\\nHere are the key benefits of the AKS cost analysis add-on. \\r\\n\\r\\n#### **Granular Cost Insights**\\r\\n\\r\\nThe AKS cost analysis add-on is integrated with Microsoft Cost Management (MCM), so you can break down your AKS costs into discrete categories, like compute (including CPU cores and memory), storage, and networking. You can distinguish between costs associated with individual applications and shared infrastructure costs, giving you a clear picture of where resources are consumed and where you can make savings. \\r\\n\\r\\n#### **Azure-Native**\\r\\n\\r\\nThe AKS cost analysis add-on is Azure-native. You can access cost data through the Azure portal via the MCM Cost Analysis portal experience. This direct integration with Azure ensures a seamless user experience and reduces the complexity and overheads of third-party cost management solutions. \\r\\n\\r\\n#### **Simplified Management**\\r\\n\\r\\nYou can easily activate and deactivate the AKS cost analysis feature with simple commands in the Azure CLI. This ease of management extends to viewing cost information, with the Azure portal providing a comprehensive overview of costs once the add-on is enabled. The ability to enable this feature when creating a new AKS cluster or when updating an existing cluster gives you flexibility in managing your cost analysis preferences.\\r\\n\\r\\n#### **Enhanced Cost Attribution**\\r\\n\\r\\nBy offering a detailed cost drill-down scoped to Kubernetes constructs, such as clusters and namespaces, in addition to Azure-specific categories like Compute, Network, and Storage, the add-on enhances cost attribution. This detailed view aids organizations in identifying high-cost areas and optimizing resource utilization for cost efficiency.\\r\\n\\r\\n### Managing Costs with Platform Engineering\\r\\n\\r\\nIt\u2019s crucial to select the right service tier and scaling options to balance performance needs with cost efficiency. One way to ensure this balance is through platform engineering. As a linchpin of cloud management, [platform engineering remains essential](https://azure.github.io/Cloud-Native/60DaysOfIA/the-role-of-platform-engineering-in-developing-intelligent-apps) in developing Intelligent Apps \u2014 particularly for forecasting and managing costs as you incorporate them into your workflow.  \\r\\n\\r\\nPlatform engineering practices enhance predictability for cloud cost management, helping you stay proactive throughout the process. Following these practices involves implementing cost monitoring tools, establishing budget thresholds, and enforcing cost limits.  \\r\\n\\r\\nControlling your costs also entails more thoughtful use of your resources. Adjusting resource allocation based on workload demand and performance metrics helps maximize your cloud infrastructure\u2019s efficiency. \\r\\n\\r\\nAdopting platform engineering principles also enhances operational simplicity. For example, you can minimize the potential for human error and increase reliability by standardizing infrastructure deployments, automating repetitive tasks, and promoting collaboration across teams. \\r\\n\\r\\nThese principles help you plan, implement, and maintain your Intelligent App development strategically and thoughtfully.\\r\\n\\r\\n### Strategies for Cost-Effective Intelligent App Development\\r\\n\\r\\nCost-effective Intelligent App development necessitates a strategic approach that maintains functionality and quality without straining budgets. Strategies to consider include:\\r\\n\\r\\n* **Leveraging managed services** \u2014 Transferring maintenance responsibility to cloud providers reduces operational burdens.  \\r\\n* **Implementing auto-scaling** \u2014 Aligning resources with demand avoids unnecessary expenses during lulls.\\r\\n* **Continuous monitoring and analysis** \u2014 Cloud environments require consistent assessments to identify inefficiencies and optimize resource allocation. \\r\\n* T**rack costs closely with AKS Cost Analysis add-on** \u2014 Gain detailed insights into how resources are consumed using the cost analysis add-on native to AKS.\\r\\n\\r\\nYou can also adjust your [VM instance options](https://azure.microsoft.com/pricing/reserved-vm-instances?ocid=buildia24_60days_blogs) for cost efficiency. For workloads with flexible timing, opt for your cloud provider\u2019s spot instances \u2014 spare compute capacity at significantly discounted prices compared to on-demand instances. For predictable workloads with fixed commitments, reserved instances provide further cost reductions.\\r\\n\\r\\nThese strategies help you make informed decisions, balancing cost-effectiveness with dynamic development requirements. \\r\\n\\r\\n### Summary\\r\\n\\r\\nUnderstanding and managing costs effectively is paramount to developing Intelligent Apps, requiring strategic planning and continuous optimization. Combining the right approach with tools like the AKS Cost Analysis add-on and Azure Pricing Calculator ensures that Intelligent App creation remains manageable and becomes a catalyst for innovation.\\r\\n\\r\\nAs you explore further, delve into [Microsoft Customer Stories](https://customers.microsoft.com/en-us/home?sq=aks&ff=&p=1&so=story_publish_date%20desc) for real-world insights. And mark your calendar for the [final episode](https://aka.ms/serverless-learn-live/ep4?ocid=buildia24_60days_blogs) of the **Serverless on Azure** learn live series."}]}')}}]);
"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[30554],{64861:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"building-a-multichannel-notification-system-1","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/building-a-multichannel-notification-system-1","source":"@site/blog-60daysofIA/2024-04-01/building-a-multichannel-notification-system-1.md","title":"6.4 Building a Multichannel Notification System (1)","description":"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.","date":"2024-04-01T09:00:00.000Z","formattedDate":"April 1, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":12.645,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-04-01T09:00","slug":"building-a-multichannel-notification-system-1","title":"6.4 Building a Multichannel Notification System (1)","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"nextItem":{"title":"6.3 Creating a Virtual Stylist Chatbot \u2014 Part 3: Deploying the App","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-3"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/building-a-multichannel-notification-system-1\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/building-a-multichannel-notification-system-1\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/building-a-multichannel-notification-system-1\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n## Building a Multichannel Notification System with Azure Functions and Azure Communication Services\u202f(1)\\r\\n\\r\\nIn the interconnected digital era, it\'s crucial for businesses and services to communicate effectively with their audience. A robust notification system that spans various communication channels can greatly enhance user engagement and satisfaction. This blog post outlines a step-by-step guide on building such a multichannel notification system with Azure Functions and Azure Communication Services.\\r\\n\\r\\nLeveraging serverless architecture and the reach of Azure Communication Services, your application can dynamically generate and send messages via SMS, Email, and WhatsApp. By incorporating OpenAI GPTs, the system can create content that is not only relevant and timely but personalized, making communication more impactful.\\r\\n\\r\\n![image of email composer assist from Chat GPT](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-1.gif)\\r\\n\\r\\nArchitecture Diagram\\r\\n\\r\\n![Architecture Diagram](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-2.png)\\r\\n\\r\\nHere are some practical scenarios where a multichannel notification system is valuable: \\r\\n\\r\\n  1. **Financial Alerts**: Banks and financial services can send fraud alerts, transaction confirmations, and account balance updates.\\r\\n  2. **Healthcare Reminders**: Clinics and pharmacies can notify patients about appointment schedules, vaccinations, or prescription refills.\\r\\n  3. **Security Verification**: Services requiring secure authentication can utilize two-factor authentication prompts sent via SMS or WhatsApp.\\r\\n  4. **Marketing and Promotions**: Retailers can craft and distribute targeted marketing messages and promotions, thereby driving customer engagement.\\r\\n  5. **Infrastructure Notifications**: Utility companies can alert customers about service disruptions, maintenance schedules, or conservation tips.\\r\\n  6. **E-commerce Updates**: Online retailers can inform customers about order confirmations, shipping details, and delivery tracking.\\r\\n\\r\\nThe foundation of this solution is [Azure Functions](https://docs.microsoft.com/azure/azure-functions/?ocid=buildia24_60days_blogs), a flexible, event-driven platform for running scalable applications with minimal overhead. We will utilize [Azure Communication Services](https://docs.microsoft.com/azure/communication-services/?ocid=buildia24_60days_blogs), which provides reliable APIs for Email, SMS, and WhatsApp messaging. To generate content, we use [OpenAI GPTs](https://openai.com/blog/introducing-gpts), which enables the creation of sophisticated, context-aware text that can be used in notifications.\\r\\n\\r\\nBy following this tutorial, you will gain the knowledge and practical experience necessary to implement a scalable multichannel notification platform that can serve a wide array of communication needs. Let\'s get started on your path to building a cutting-edge, serverless messaging system on Azure.\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nBefore we dive into building our multichannel notification system with Azure Functions and Azure Communication Services, you will need to ensure that the following tools and accounts set up: \\r\\n\\r\\n  1. **Azure Account**: You\'ll need a Microsoft Azure account to create and manage resources on Azure. If you haven\'t got one yet, you can [create a free account here](https://azure.microsoft.com/free/?ocid=buildia24_60days_blogs).\\r\\n  2. **Visual Studio Code**: We\'ll be using Visual Studio Code (VS Code) as our Integrated Development Environment (IDE) for writing and debugging our code. Download and install it from [here](https://code.visualstudio.com/).\\r\\n  3. **Azure Functions Extension for Visual Studio Code**: This extension provides you with a seamless experience for developing Azure Functions. It can be installed from the [VS Code marketplace](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions).\\r\\n  4. **C# Dev Kit**: Since we\'re writing our Azure Functions in C#, this extension is necessary for getting C# support in VS Code. You can install it from the [VS Code marketplace](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit).\\r\\n  5. **Azure CLI**: The Azure Command-Line Interface (CLI) will be used to create and manage Azure resources from the command line. For installation instructions, visit the Azure CLI installation [documentation page](https://docs.microsoft.com/cli/azure/install-azure-cli?ocid=buildia24_60days_blogs).\\r\\n  6. **Postman**: Although not strictly necessary, Postman is a handy tool for testing our HTTP-triggered Azure Functions without having to write a front-end application. You can download Postman from [getpostman.com](https://www.getpostman.com/).\\r\\n\\r\\nWith the prerequisites in place, you\'re ready to set up your development environment, which we will cover in the following section.\\r\\n\\r\\n:::info\\r\\nRegister for **[Episode 4](https://aka.ms/serverless-learn-live/ep4?ocid=buildia24_60days_blogs)**\u202fof the new hands-on live learning series with an SME **on\u202fIntelligent Apps with Serverless on Azure**.\\r\\n:::\\r\\n\\r\\n### Creating Resources\\r\\n\\r\\nTo get started with building a multichannel notification system, we\'ll need to create several resources within Azure. This section will walk you through setting up your Azure environment using the Azure CLI. Ensure that you have the Azure CLI installed on your machine and that you\'re logged into your Azure account.\\r\\n\\r\\n#### Azure Communication Services\\r\\n\\r\\n  1. Azure Communication Services (ACS) provides the backbone for our notification system, allowing us to send SMS, Email, and WhatsApp messages. The steps below create resources for all three communication channels. However, you can choose one or more depending upon your preference. **Log in to Azure**:\\r\\n\\r\\n`bash`\\r\\n  \\r\\n```\\r\\naz login \\r\\n```\\r\\n\\r\\n  2. **Create a Resource Group (if necessary)**: This groups all your resources in one collection.\\r\\n\\r\\n`bash`\\r\\n\\r\\n```\\r\\naz group create --name <YourResourceGroupName> --location <PreferredLocation>\\r\\n```\\r\\nReplace `<YourResourceGroupName>` with a name for your new resource group and `<PreferredLocation>` with the Azure region you prefer (e.g., eastus).\\r\\n\\r\\n  3. **Create ACS Resource**: This will be the main ACS resource where we manage communications capabilities.\\r\\n\\r\\n`bash`\\r\\n\\r\\n```\\r\\naz communication create --name <YourACSResourceName> --location Global --data-location UnitedStates --resource-group <YourResourceGroupName>\\r\\n```\\r\\n\\r\\nReplace `<YourACSResourceName>` with a unique name for your ACS resource and `<YourResourceGroupName>` with the name of your resource group.\\r\\n\\r\\nAfter creating the resource, retrieve the connection string as you will need it to connect your Azure Function to ACS. Copy the one marked as primary.\\r\\n\\r\\n`bash`\\r\\n\\r\\n```\\r\\naz communication list-key --name <YourACSResourceName> --resource-group <YourResourceGroupName>\\r\\n```\\r\\n\\r\\n#### Azure Communication Services for Email\\r\\n\\r\\nTo set up Azure Communication Services Email, you\'ll need to follow a few steps in the Azure Portal:\\r\\n\\r\\n  1. **Create the Email Communications Service resource using the portal**: Provision a new Email Communication Services resource in [Azure portal](https://portal.azure.com/) using the instructions [here](https://learn.microsoft.com/azure/communication-services/quickstarts/email/create-email-communication-resource?ocid=buildia24_60days_blogs). Make sure to select the same resource group as your ACS resource.\\r\\n\\r\\n![image of Email Communication Services in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-3.png)\\r\\n\\r\\n![image of Email Communication Services resource fields in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-4.png)\\r\\n\\r\\n  2. **Configure the Email Communications Service**: You will need to configure domains and sender authentication for email. Provision an [Azure Managed Domain](https://learn.microsoft.com/azure/communication-services/quickstarts/email/add-azure-managed-domains?ocid=buildia24_60days_blogs) or set up your [Custom Verified Domain](https://learn.microsoft.com/azure/communication-services/quickstarts/email/add-custom-verified-domains?ocid=buildia24_60days_blogs) depending on your use case.\\r\\n\\r\\n![image of Email Communication Services configurations in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-5.png)\\r\\n\\r\\n#### Azure Communication Services for SMS\\r\\n\\r\\nTo send SMS messages, you will need to acquire a phone number through ACS. The phone number has a cost associated with it. If you want to avoid that, continue with Email and WhatsApp only.\\r\\n\\r\\n  1. **Get a Phone Number**: Navigate to the **Phone Numbers** blade in your ACS resource on the [Azure portal](https://portal.azure.com/) and follow the steps to [get a phone number](https://learn.microsoft.com/azure/communication-services/quickstarts/telephony/get-phone-number?ocid=buildia24_60days_blogs) that\'s capable of sending and receiving SMS.\\r\\n\\r\\n![image of Email Communication Services phone number features in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-6.png)\\r\\n\\r\\n  2. **Note the Phone Number**: After acquiring a phone number, note it down as it will be used to send SMS messages from your Azure Function.\\r\\n\\r\\n![image of Email Communication Services resource once configured in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-7.png)\\r\\n\\r\\n#### WhatsApp for Business\\r\\n\\r\\nSending WhatsApp messages requires setting up a WhatsApp Business account. \\r\\n\\r\\n  1. **Set up a WhatsApp Business Account**: Follow the instructions for connecting a [WhatsApp business account](https://learn.microsoft.com/azure/communication-services/quickstarts/advanced-messaging/whatsapp/connect-whatsapp-business-account?ocid=buildia24_60days_blogs) with Azure Communication Services.\\r\\n  2. **Note the WhatsApp Configuration**: Once set up, make a note of the necessary configuration details such as the phone number and WhatsApp Business API credentials, as they will be needed in your Azure Function.\\r\\n\\r\\n![image of Email Communication Services configuration channels in Azure](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-8.png)\\r\\n\\r\\nBy following these steps, you will have created the necessary resources to build a multichannel notification system that can reach users through SMS, Email, and WhatsApp. Next, we\'ll proceed with setting up your Azure Function and integrating these services into it.\\r\\n\\r\\n### Setting Up Environment\\r\\n\\r\\nWith the prerequisites out of the way, let\'s prepare our environment to develop our multichannel notification system using Azure Functions and Azure Communication Services.\\r\\n\\r\\n#### Creating the Function App Project\\r\\n\\r\\nOpen Visual Studio Code and follow these steps to create a new Azure Functions project:\\r\\n\\r\\n  1. Click on the Azure icon in the Activity Bar on the side of Visual Studio Code to open the Azure Functions extension.\\r\\n  2. In the Azure Functions extension, click on the \'Create New Project\' icon, choose a directory for your project, and select \'Create New Project Here\'.\\r\\n\\r\\n![mage of a new Azure Functions project in Visual Studio Code](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-9.png)\\r\\n\\r\\n  3. Choose the language for your project. We will select C# for this tutorial.\\r\\n  4. Select the template for your first function. For this project, an HTTP-triggered function is a good starting point since we want to receive HTTP requests to send out notifications.\\r\\n  5. Provide a function name, such as `EmailTrigger`, and set the authorization level to anonymous or function, depending on your security preference.\\r\\n\\r\\nAfter you have completed these steps, your Azure Functions project will be set up with all the necessary files in the chosen directory.\\r\\n\\r\\n#### Installing the Necessary Packages\\r\\n\\r\\nNow it\u2019s time to add the packages necessary for integrating Azure Communication Services:\\r\\n\\r\\n  1. Open the integrated terminal in Visual Studio Code by clicking on \'Terminal\' in the top menu and then selecting \'New Terminal\'.\\r\\n  2. Add the Azure Communication Services packages to your project:\\r\\n\\r\\n`bash`\\r\\n```\\r\\ndotnet add package Azure.Communication.Email\\r\\ndotnet add package Azure.Communication.Sms\\r\\ndotnet add package Azure.Communication.Messages --prerelease\\r\\n```\\r\\n\\r\\n#### Setting Up Environment Variables\\r\\n\\r\\nYou should store configuration details like connection strings and phone numbers as environment variables instead of hardcoding them into your functions. To do so in Azure Functions, add them to the `local.settings.json` file, which is used for local development.\\r\\n\\r\\nEdit the `local.settings.json` file to include your Azure Communication Services (ACS) connection string and phone numbers:\\r\\n\\r\\n`json`\\r\\n```\\r\\n{ \\r\\n  \\"IsEncrypted\\": false,\\r\\n  \\"Values\\": {\\r\\n    \\"AzureWebJobsStorage\\": \\"\\",\\r\\n    \\"FUNCTIONS_WORKER_RUNTIME\\": \\"dotnet\\",\\r\\n    \\"COMMUNICATION_SERVICES_CONNECTION_STRING\\": \\"<acs_connection_string>\\",\\r\\n    \\"SENDER_PHONE_NUMBER\\": \\"<acs_sms_phone_number>\\",\\r\\n    \\"WHATSAPP_NUMBER\\": \\"<acs_whatsapp_number>\\", \\r\\n    \\"SENDER_EMAIL_ADDRESS\\": \\"<acs_email_address>\\"\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nBe sure to replace `<acs_connection_string>`, `<acs_sms_phone_number>`, `<acs_whatsapp_number>`, and `<acs_email_address>` with your actual Azure storage account connection string, Azure Communication Services connection string, SMS phone number, WhatsApp number, and sending email address. \\r\\n\\r\\nRemember not to commit the `local.settings.json` file to source control if it contains sensitive information. Configure similar settings in the Application Settings for your Azure Function when you deploy to Azure.\\r\\n\\r\\n### Coding the EmailTrigger\\r\\n\\r\\nCreating a functional `EmailTrigger` Azure Function involves starting from the default template provided by Azure Functions for C# and enhancing it with the necessary logic and services to handle email sending. In this section, we guide you through the steps to transform the default template into the finished `EmailTrigger` function.\\r\\n\\r\\n#### Step 1: Set Up the Function Template\\r\\n\\r\\nStart by using the default HTTP triggered function template provided by Visual Studio Code for creating an Azure Functions project. It will have the necessary usings, function name attribute, and a simple HTTP trigger that returns a welcome message. Select your project in the Workspace pane and click on the \'Create Function\' button in the Azure Functions extension. Choose \'HTTP trigger\' as the template and provide a name for the function, such as `EmailTrigger`. Set the authorization level to anonymous or function, depending on your security preference.\\r\\n\\r\\n![image of creating a new function in Visio Studio Code](../../static/img/60-days-of-ia/blogs/2024-04-01/6-4-10.png)\\r\\n\\r\\n#### Step 2: Add Azure Communication Services Email Reference\\r\\n\\r\\nAdd a reference to using Azure.Communication.Email then create a property in the EmailTrigger class to hold an instance of EmailClient and a property to hold the email sender address.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nprivate readonly EmailClient _emailClient;\\r\\nprivate string? sender = Environment.GetEnvironmentVariable(\\"SENDER_EMAIL_ADDRESS\\");\\r\\n```\\r\\n\\r\\n#### Step 3: Read Configuration and Initialize EmailClient\\r\\n\\r\\nWithin the `EmailTrigger` class constructor, read the Azure Communication Services connection string from the environment variables using `Environment.GetEnvironmentVariable()` method and initialize an instance of `EmailClient` with the connection string.\\r\\n\\r\\nMake sure to handle the possibility that the environment variable may be null and throw an appropriate exception if it is not set.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nstring? connectionString = Environment.GetEnvironmentVariable(\\"COMMUNICATION_SERVICES_CONNECTION_STRING\\");\\r\\nif (connectionString is null)\\r\\n{\\r\\n    throw new InvalidOperationException(\\"COMMUNICATION_SERVICES_CONNECTION_STRING environment variable is not set.\\");\\r\\n}\\r\\n_emailClient = new EmailClient(connectionString);\\r\\n```\\r\\n\\r\\n#### Step 4: Define the Request Model\\r\\n\\r\\nCreate a request model class `EmailRequest` inside the `EmailTrigger` class to represent the expected payload. This model includes the subject, HTML content, and recipient email address.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\npublic class EmailRequest \\r\\n\\r\\n{\\r\\n    public string Subject { get; set; } = string.Empty;\\r\\n    public string HtmlContent { get; set; } = string.Empty;\\r\\n    public string Recipient { get; set; } = string.Empty;\\r\\n}\\r\\n```\\r\\n\\r\\n#### Step 5: Parse the Request Body\\r\\n\\r\\nModify the `Run` function to be async since we\'ll be performing asynchronous operations.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\npublic async Task<IActionResult> Run([HttpTrigger(AuthorizationLevel.Anonymous, \\"post\\")] HttpRequest req)\\r\\n```\\r\\n\\r\\nUse `StreamReader` to read the request body and deserialize it into the `EmailRequest` object using `System.Text.Json.JsonSerializer`. \\r\\n\\r\\nHandle the case where the deserialization fails by returning a `BadRequestResult`.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nstring requestBody = await new StreamReader(req.Body).ReadToEndAsync();\\r\\nEmailRequest? data = JsonSerializer.Deserialize<EmailRequest>(requestBody, new JsonSerializerOptions() {\\r\\n                PropertyNamingPolicy = JsonNamingPolicy.CamelCase\\r\\n            });\\r\\nif (data is null)\\r\\n{\\r\\n    return new BadRequestResult();\\r\\n}\\r\\n```\\r\\n\\r\\n#### Step 6: Define the Sender and Send the Email\\r\\n\\r\\nInstantiate a sender email address string that will be passed to the `SendAsync` method of the EmailClient instance. Replace the static email \'DoNotReply@effaa622-a003-4676-b27e-6b9e7a783581.azurecomm.net\' with your configured sender address in the actual implementation. \\r\\n\\r\\nUse a try-catch block to send the email using the `SendAsync` method and catch any `RequestFailedException` to log any errors.\\r\\n\\r\\n`csharp`\\r\\n\\r\\n```\\r\\n_logger.LogInformation(\\"Sending email...\\");\\r\\nEmailSendOperation emailSendOperation = await _emailClient.SendAsync(\\r\\n    Azure.WaitUntil.Completed,\\r\\n    sender,\\r\\n    data.Recipient,\\r\\n    data.Subject,\\r\\n    data.HtmlContent\\r\\n); \\r\\n\\r\\n_logger.LogInformation($\\"Email Sent. Status = {emailSendOperation.Value.Status}\\");\\r\\n_logger.LogInformation($\\"Email operation id = {emailSendOperation.Id}\\");\\r\\n```\\r\\n\\r\\n#### Step 7: Return a Success Response\\r\\n\\r\\nOnce the email send operation is completed, return an `OkObjectResult` indicating the success of the operation.\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nreturn new OkObjectResult(\\"Email sent successfully!\\");\\r\\n}\\r\\n```\\r\\n\\r\\n#### Final Code\\r\\n\\r\\nAfter completing all the above steps, your `EmailTriggerAzure Function` should look as follows:\\r\\n\\r\\n`csharp`\\r\\n```\\r\\nusing System;\\r\\nusing System.IO;\\r\\nusing System.Text.Json;\\r\\nusing System.Threading.Tasks;\\r\\nusing Azure;\\r\\nusing Azure.Communication.Email;\\r\\nusing Microsoft.AspNetCore.Mvc;\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Extensions.Logging;\\r\\n\\r\\nnamespace ACSGPTFunctions\\r\\n{ \\r\\n    public class EmailTrigger\\r\\n    { \\r\\n        private readonly ILogger<EmailTrigger> _logger;\\r\\n        private readonly EmailClient _emailClient;\\r\\n\\r\\n        public EmailTrigger(ILogger<EmailTrigger> logger)\\r\\n        {\\r\\n            _logger = logger;\\r\\n            string? connectionString = Environment.GetEnvironmentVariable(\\"COMMUNICATION_SERVICES_CONNECTION_STRING\\");\\r\\n            if (connectionString is null)\\r\\n            {\\r\\n                throw new InvalidOperationException(\\"COMMUNICATION_SERVICES_CONNECTION_STRING environment variable is not set.\\");\\r\\n            }\\r\\n            _emailClient = new EmailClient(connectionString);\\r\\n        } \\r\\n\\r\\n        public class EmailRequest\\r\\n        {\\r\\n            public string Subject { get; set; } = string.Empty;\\r\\n            public string HtmlContent { get; set; } = string.Empty;\\r\\n            public string Recipient { get; set; } = string.Empty;\\r\\n        }\\r\\n\\r\\n        [Function(\\"EmailTrigger\\")]\\r\\n        public async Task<IActionResult> Run([HttpTrigger(AuthorizationLevel.Anonymous, \\"post\\")] HttpRequest req) \\r\\n        {\\r\\n            _logger.LogInformation(\\"Processing request.\\");\\r\\n\\r\\n            string requestBody = await new StreamReader(req.Body).ReadToEndAsync();\\r\\n            EmailRequest? data = JsonSerializer.Deserialize<EmailRequest>(requestBody, new JsonSerializerOptions() {\\r\\n                PropertyNamingPolicy = JsonNamingPolicy.CamelCase\\r\\n            });\\r\\n\\r\\n            if (data is null)\\r\\n            {\\r\\n                return new BadRequestResult();\\r\\n            }\\r\\n\\r\\n            var sender = \\"DoNotReply@effaa622-a003-4676-b27e-6b9e7a783581.azurecomm.net\\";\\r\\n\\r\\n            try \\r\\n            {\\r\\n                _logger.LogInformation(\\"Sending email...\\");\\r\\n                EmailSendOperation emailSendOperation = await _emailClient.SendAsync(\\r\\n                    Azure.WaitUntil.Completed,\\r\\n                    sender,\\r\\n                    data.Recipient,\\r\\n                    data.Subject,\\r\\n                    data.HtmlContent\\r\\n                );\u202f \\r\\n\\r\\n                _logger.LogInformation($\\"Email Sent. Status = {emailSendOperation.Value.Status}\\");\\r\\n                _logger.LogInformation($\\"Email operation id = {emailSendOperation.Id}\\");\\r\\n            }\\r\\n            catch (RequestFailedException ex)\\r\\n            {\\r\\n                _logger.LogInformation($\\"Email send operation failed with error code: {ex.ErrorCode}, message: {ex.Message}\\");\\r\\n                return new ObjectResult(new { error = ex.Message }) { StatusCode = 500 };\\r\\n            }\\r\\n\\r\\n            return new OkObjectResult(\\"Email sent successfully!\\");\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThis completed `EmailTriggerAzure` Function is now ready to be part of a multichannel notification system, handling the email communication channel.\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nContinue to the next part of this topic to further explore building, deploying and testing your intelligent app for a multichannel notification system."},{"id":"creating-a-virtual-stylist-chatbot-part-3","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-3","source":"@site/blog-60daysofIA/2024-03-28/creating-a-virtual-stylist-chatbot-part-3.md","title":"6.3 Creating a Virtual Stylist Chatbot \u2014 Part 3: Deploying the App","description":"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In the final article of this series, you\u2019ll deploy and test the Intelligent App.","date":"2024-03-28T09:00:00.000Z","formattedDate":"March 28, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.32,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-28T09:00","slug":"creating-a-virtual-stylist-chatbot-part-3","title":"6.3 Creating a Virtual Stylist Chatbot \u2014 Part 3: Deploying the App","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In the final article of this series, you\u2019ll deploy and test the Intelligent App.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"6.4 Building a Multichannel Notification System (1)","permalink":"/Cloud-Native/60DaysOfIA/building-a-multichannel-notification-system-1"},"nextItem":{"title":"6.2 Creating a Virtual Stylist Chatbot \u2014 Part 2: Adding a Chatbot Interface","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-2"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/creating-a-virtual-stylist-chatbot-part-3\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In the final article of this series, you\u2019ll deploy and test the Intelligent App.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-3\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In the final article of this series, you\u2019ll deploy and test the Intelligent App.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-3\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![A minimalist graphic features a t-shirt, pants, and robot head in a rounded square, connected by broken line to a smartphone that displays a chatbot conversation.](../../static/img/60-days-of-ia/blogs/2024-03-28/6-3-1.jpeg)\\r\\n\\r\\n## Creating a Virtual Stylist Chatbot\u2014Part 3: Deploying the App\\r\\n\\r\\nIn [part 1](https://azure.github.io/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-1) of this series, you used AI to analyze images of clothing and generate a text description of each piece. Then, in [part 2](https://azure.github.io/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-2), you designed the chatbot\u2019s interface.\\r\\n\\r\\nIn this third and final installment, you\u2019ll deploy the app as an Azure Static Web App using the Azure command-line interface (CLI). The Azure Static Web Apps service provides a hassle-free means of hosting static web apps with serverless APIs. It also features global distribution, custom domains, SSL certificates, authentication, authorization, and GitHub integration.\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nTo follow along, ensure you have:\\r\\n\\r\\n* The complete code from [part 2](https://azure.github.io/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-2)\\r\\n* The Azure CLI installed and signed in to your Azure account\\r\\n* A GitHub account, [an empty repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/quickstart-for-repositories) to push the app\u2019s code to, and a [personal access token](https://docs.github.com/en/enterprise-server@3.9/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-personal-access-token) granting read and write access to the repository\\r\\n\\r\\nFor a preview of the project, check out the [complete project code available on GitHub](https://github.com/rogerwinter/Microsoft-Creating-a-Virtual-Stylist-Chatbot/).\\r\\n\\r\\n### Pushing the App to GitHub\\r\\n\\r\\nYou can set up Azure Static Web Apps to deploy automatically every time you push a new commit to GitHub. Before proceeding, create a GitHub repository for the web app and push all its code to the repo.\\r\\n\\r\\n:::info\\r\\nRegister for [Episode 3](https://aka.ms/serverless-learn-live/ep3?ocid=buildia24_60days_blogs) of the new learning series on **Intelligent Apps with Serverless on Azure**. Join the community along with MVPs, and the Azure Product Group on how to leverage AI with Serverless on Azure technologies \u2013Azure Functions and Azure Container Apps \u2013 to build intelligent applications.\\r\\n:::\\r\\n\\r\\n### Creating an Azure Static Web Resource\\r\\n\\r\\nNext, you\u2019ll create an Azure Static Web App resource using the Azure CLI. The Azure Static Web App resource is the container for the app and its settings.\\r\\n\\r\\nTo create it, run the following command in your terminal:\\r\\n\\r\\n```\\r\\naz staticwebapp create \\\\\\r\\n  --name virtual-stylist-chat \\\\\\r\\n  --resource-group <your resource group> \\\\\\r\\n  --location westus2 \\\\\\r\\n  --source virtual-stylist-chat \\\\\\r\\n  --branch main \\\\\\r\\n  --app-location / \\\\\\r\\n  --output-location dist \\\\\\r\\n  --login-with-github\\r\\n```\\r\\n\\r\\nThis command will create an Azure Static Web App resource with the following parameters:\\r\\n\\r\\n* **`--name`**\u2014The name of the resource, which must be globally unique\\r\\n* **`--resource-group`**\u2014The name of the resource group to contain the resource\\r\\n* **`--location`** \u2014The location of the resource\\r\\n* **`--source`**\u2014The name of the GitHub repository that contains the app code\\r\\n* **`--branch`**\u2014The name of the GitHub branch that contains the app code\\r\\n* **`--app-location`**\u2014The location of the app code in the repository\\r\\n* **`--output-folder`**\u2014The folder where the app output is generated\\r\\n* **`--login-with-github`**\u2014The GitHub personal access token that grants access to the repository\\r\\n\\r\\nThe command creates a GitHub Actions workflow file in the repository that triggers the app build and deployment whenever a change is pushed to the branch. It also outputs some information about the resource, like this:\\r\\n\\r\\n```\\r\\n{ \\r\\n  \\"defaultHostname\\": \\"orange-beach-0c471f710.azurestaticapps.net\\",\\r\\n  \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/virtual-stylist-chat-rg/providers/Microsoft.Web/staticSites/virtual-stylist-chat\\",\\r\\n  \\"location\\": \\"West US 2\\",\\r\\n  \\"name\\": \\"virtual-stylist-chat\\",\\r\\n  \\"repositoryUrl\\": \\"https://github.com/username/virtual-stylist-chat\\",\\r\\n  \\"resourceGroup\\": \\"virtual-stylist-chat-rg\\",\\r\\n  \\"sku\\": \\"Free\\",\\r\\n  \\"type\\": \\"Microsoft.Web/staticSites\\",\\r\\n  \\"userId\\": \\"username\\",\\r\\n  \\"workflowFileUrl\\": \\"https://github.com/username/virtual-stylist-chat/blob/main/.github/workflows/azure-static-web-apps-virtual-stylist-chat.yml\\"\\r\\n}\\r\\n```\\r\\n\\r\\nYou\u2019ve now created an Azure Static Web App resource and a GitHub Actions workflow for the app.\\r\\n\\r\\nTo link the function app from part 1 as the back end for the Azure Static Web App, you use [`az staticwebapp backends link`](https://learn.microsoft.com/cli/azure/staticwebapp/backends?view=azure-cli-latest&ocid=buildia24_60days_blogs#az-staticwebapp-backends-link). This command links a pre-existing back end with a static web app, also known as \u201cBring your own API.\u201d You need to provide the function app\u2019s resource ID, the static web app\u2019s resource group, and the back-end region.\\r\\n\\r\\nLink the function app as the back end for the static web app by running the following:\\r\\n\\r\\n```\\r\\naz staticwebapp backends link \\\\\\r\\n  --backend-resource-id \\"/subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Web/sites/<function-app-name>\\" \\\\\\r\\n  --name virtual-stylist-chat \\\\\\r\\n  --resource-group <your-resource-group> \\\\\\r\\n  --backend-region westus\\r\\n```\\r\\n\\r\\n### Testing the App\\r\\n\\r\\nNow, you\u2019ll test the app by uploading some images of clothing items or outfits to see how the stylist bot responds and makes recommendations. You\u2019ll also witness how the app handles different types of inputs, such as images and text messages.\\r\\n\\r\\n#### Uploading an Image of a Fashion Item\\r\\n\\r\\nTo start, you\u2019ll upload an image of a blue denim jacket to see how the bot responds.\\r\\n\\r\\nClick **Upload** at the bottom of the chat window. Then, select the image file from your local machine. Alternatively, you can drag and drop the image file to the chat window.\\r\\n\\r\\nThe app will display the image as a chat message and send it to the back-end function. This function will analyze the image and generate a natural language response and recommendations using Azure Functions, Azure AI Services, and GPT-4 Vision using Azure OpenAI Service. It will then display the response and its recommendations as another chat message. Your result will look something like this:\\r\\n\\r\\n![The Virtual Stylist Chatbot sends a greeting message that invites the user to request fashion advice and recommendations. The user responds with an image of a red t-shirt and a request to find a matching outfit.](../../static/img/60-days-of-ia/blogs/2024-03-28/6-3-2.png)\\r\\n\\r\\nAs you can see, the stylist bot correctly identified the fashion item as a red t-shirt and provided some information and tips about it. It also suggested some images of other items to pair with red t-shirts, including blue jeans and a red hat:\\r\\n\\r\\n![The Virtual Stylist Chatbot returns a message with images of jeans and a red baseball cap](../../static/img/60-days-of-ia/blogs/2024-03-28/6-3-3.png)\\r\\n\\r\\nYou can click the images to view them full-size:\\r\\n\\r\\n![The full-size photo of the jeans includes a partially visible torso and arms.](../../static/img/60-days-of-ia/blogs/2024-03-28/6-3-4.png)\\r\\n\\r\\nIf you don\u2019t like the suggestions or just want to see more, you can reply with additional details or questions, and it will generate new suggestions based on the information you provide.\\r\\n\\r\\n:::info\\r\\nJoin the Azure Functions product group for an **[Ask The Expert](https://aka.ms/intelligent-apps/ate-functions?ocid=buildia24_60days_blogs)** session on how to focus on the pieces of code that matter most to you in AI application development, while Azure Functions handles the rest for you at extreme scale.\\r\\n:::\\r\\n\\r\\n### Conclusion\\r\\n\\r\\nIn this tutorial series, you learned how to create a virtual stylist chatbot app using Azure and OpenAI. You built the app\u2019s back end using Azure Functions, Azure AI, and GPT-4 Vision on Azure OpenAI Service. You then learned how to use these services to analyze images and generate natural language responses and recommendations based on the images. Next, you created the chatbot interface for our app using Vite, Vue, TypeScript, Tailwind CSS, and vue-advanced-chat.\\r\\n\\r\\nYou learned how to use these tools to build a web application that allows you conversationally interact with your stylist bot. Finally, you deployed the app as an Azure Static Web App using the Azure CLI.\\r\\n\\r\\nGet your hands on the newly released [Azure Functions Flex Consumption Plan](https://aka.ms/flexconsumption/signup?ocid=buildia24_60days_blogs) for private networking, instance size selection, concurrency control, and fast and large scale out features on a serverless compute model."},{"id":"creating-a-virtual-stylist-chatbot-part-2","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-2","source":"@site/blog-60daysofIA/2024-03-27/creating-a-virtual-stylist-chatbot-part-2.md","title":"6.2 Creating a Virtual Stylist Chatbot \u2014 Part 2: Adding a Chatbot Interface","description":"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this second installment, you\u2019ll design the chatbot\u2019s interface.","date":"2024-03-27T09:00:00.000Z","formattedDate":"March 27, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":13.865,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-27T09:00","slug":"creating-a-virtual-stylist-chatbot-part-2","title":"6.2 Creating a Virtual Stylist Chatbot \u2014 Part 2: Adding a Chatbot Interface","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this second installment, you\u2019ll design the chatbot\u2019s interface.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"6.3 Creating a Virtual Stylist Chatbot \u2014 Part 3: Deploying the App","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-3"},"nextItem":{"title":"6.1 Creating a Virtual Stylist Chatbot \u2014 Part 1: Analyzing Images with AI","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-1"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/creating-a-virtual-stylist-chatbot-part-2\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this second installment, you\u2019ll design the chatbot\u2019s interface.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-2\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this four-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this second installment, you\u2019ll design the chatbot\u2019s interface.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-2\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Graphical representation of a chatbot. The human user\'s chat bubble contains a t-shirt with a question mark, while the bot\'s chat bubble contains three dots to indicate it is responding.](../../static/img/60-days-of-ia/blogs/2024-03-27/6-2-1.jpeg)\\r\\n\\r\\n## Creating a Virtual Stylist Chatbot \u2014 Part 2: Adding a Chatbot Interface\\r\\n\\r\\nWelcome to part 2 of this tutorial series on creating a virtual stylist chatbot using Azure OpenAI Service.\u202f \\r\\n\\r\\nIn [part 1](https://azure.github.io/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-1), you built the chatbot app\u2019s back end using Azure Functions, Azure AI Services, and GPT-4 Vision with Azure OpenAI Service. That tutorial covered using these services to analyze an image of a fashion item or outfit and generate natural language responses and recommendations based on it.\\r\\n\\r\\nIn this second installment, you\u2019ll create a chatbot interface for your virtual stylist app using Vite, Vue, TypeScript, and vue-advanced-chat. You\u2019ll learn how to use these tools to build a web application that allows you to interact with your stylist bot conversationally.\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nBefore you begin, ensure you have:\\r\\n\\r\\n* An Azure subscription with access to [Azure OpenAI Service](https://azure.microsoft.com/products/ai-services/openai-service?ocid=buildia24_60days_blogs)\\r\\n* [Azure command-line interface (CLI)](https://learn.microsoft.com/cli/azure/?ocid=buildia24_60days_blogs) installed\\r\\n* [Azure Functions Core Tools](https://github.com/Azure/azure-functions-core-tools) installed\\r\\n* An Azure OpenAI Service resource with a GPT-4 Vision model deployed\\r\\n* The deployment name, endpoint, and API key for your OpenAI Service\\r\\n* The [Fashion Product Images dataset](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small) from Kaggle\\r\\n* [Node.js 20](https://nodejs.org/en/download/) or later installed on your local machine \\r\\n* A text editor that supports Vue and TypeScript. If you use Visual Studio Code, consider installing the [TypeScript Vue Plugin](https://marketplace.visualstudio.com/items?itemName=Vue.vscode-typescript-vue-plugin) to ensure the editor understands all the files you\u2019re about to create. \\r\\n\\r\\nFor a preview of this tutorial, check out the [project code available on GitHub](https://github.com/rogerwinter/Microsoft-Creating-a-Virtual-Stylist-Chatbot/tree/main/stylist-backend).\\r\\n\\r\\n\\r\\n### Creating a Chatbot Interface for Your Virtual Stylist\\r\\n\\r\\nIn this section, you\u2019ll create a chatbot interface for the virtual stylist app using Vue and vue-advanced-chat. You\u2019ll use Vue to create the main components of the app, including the header, the footer, the chat window, and the image upload button. You\u2019ll also use the vue-advanced-chat library to create the chat messages, chat input, and other chat options, using Tailwind CSS to style the app.\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n#### Setting Up the Project\\r\\n\\r\\nThe first step is creating a new Vue project using Vite. Vite is a fast and lightweight build tool that provides a smooth developer experience and supports features like hot module replacement, code splitting, and tree shaking.\\r\\n\\r\\nTo create a new Vue project with Vite, run the following command in your terminal:\\r\\n\\r\\n```\\r\\nnpm init vite@latest virtual-stylist-chat -- --template vue-ts\\r\\n```\\r\\n\\r\\nThis builds a new folder, `virtual-stylist-chat`, with the following structure:\\r\\n\\r\\n```\\r\\nvirtual-stylist-chat\\r\\n\u251c\u2500\u2500 index.html\\r\\n\u251c\u2500\u2500 package.json\\r\\n\u251c\u2500\u2500 public\\r\\n\u2502   \u2514\u2500\u2500 favicon.svg\\r\\n\u251c\u2500\u2500 src\\r\\n\u2502   \u251c\u2500\u2500 App.vue\\r\\n\u2502   \u251c\u2500\u2500 assets\\r\\n\u2502   \u2502   \u2514\u2500\u2500 logo.svg\\r\\n\u2502   \u251c\u2500\u2500 components\\r\\n\u2502   \u2502   \u2514\u2500\u2500 HelloWorld.vue\\r\\n\u2502   \u251c\u2500\u2500 main.ts\\r\\n\u2502   \u2514\u2500\u2500 shims-vue.d.ts\\r\\n\u2514\u2500\u2500 tsconfig.json\\r\\n```\\r\\n\\r\\nNext, add a few dependencies:\\r\\n\\r\\n* [vue-advanced-chat](https://github.com/advanced-chat/vue-advanced-chat), a feature-rich and highly customizable Vue chat component library that provides many out-of-the-box features for chat interfaces. These include images, videos, files, voice messages, emojis, link previews, typing indicators, reactions, markdown text formatting, online presence indicators, delivery and read receipts, theming and customization options, and responsive design.\\r\\n* [Tailwind CSS](https://tailwindcss.com/), [PostCSS](https://postcss.org/), and [autoprefixer](https://www.npmjs.com/package/autoprefixer) to simplify styling the app\\r\\n* [uuid](https://www.npmjs.com/package/uuid) to generate unique IDs for each message\\r\\n\\r\\nTo install the required packages, run the following command:\\r\\n\\r\\n```\\r\\nnpm install --save vue-advanced-chat tailwindcss@latest postcss@latest autoprefixer@latest uuid @types/uuid \\r\\n```\\r\\n\\r\\nThis command adds vue-advanced-chat, Tailwind, and PostCSS as dependencies in the `package.json` file.\\r\\n\\r\\nNow that you\u2019ve set up the project and installed the dependencies, check that it builds as expected by running `npm run dev`. The app should build and provide an address to view it in a web browser. Load it, and you should see the default welcome screen:\\r\\n\\r\\n![the Vite + Vue welcome page displays both logos and provides links to create-vue and Volar.](../../static/img/60-days-of-ia/blogs/2024-03-27/6-2-2.png)\\r\\n\\r\\nNext, generate the `tailwind.config.js` and `postcss.config.js` files using the following command:\\r\\n\\r\\n```\\r\\nnpx tailwindcss init -p\\r\\n```\\r\\n\\r\\nEdit the `tailwind.config.js` file and add the paths to your template files in the `content` property:\\r\\n\\r\\n```\\r\\n// tailwind.config.js\\r\\nexport default {\\r\\n  content: [\\"./index.html\\", \\"./src/**/*. {vue,js,ts,jsx,tsx}\\"],\\r\\n  theme: {\\r\\n    extend: {},\\r\\n  },\\r\\n  plugins: [],\\r\\n};\\r\\n```\\r\\n\\r\\nThen, replace the content of `style.css` file in the `src` folder with the following code to import Tailwind CSS using the `@tailwind` directives:\\r\\n\\r\\n```\\r\\n@tailwind base;\\r\\n@tailwind components;\\r\\n@tailwind utilities;\\r\\n```\\r\\n\\r\\nThen, import the `styles.css` file in the `main.ts` file and remove the unused import:\\r\\n\\r\\n```\\r\\nimport { createApp } from \\"vue\\";\\r\\nimport App from \\"./App.vue\\";\\r\\nimport \\"./styles.css\\"; // import Tailwind CSS\\r\\n\\r\\ncreateApp(App).mount(\\"#app\\");\\r\\n```\\r\\n\\r\\nFinally, copy the images from the [dataset](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small) you downloaded in the first part of this series. Using your preferred CLI or file manager, create a new folder called `Images` inside the project\u2019s `public` folder, and then copy all the images from the dataset\u2019s `images_compressed` folder to the `Images` folder. The stylist bot will use these images to make recommendations based on the image IDs it returns.\\r\\n\\r\\nThe result should look like this:\\r\\n\\r\\n```\\r\\nvirtual-stylist-chat\\r\\n\u251c\u2500\u2500 index.html\\r\\n\u251c\u2500\u2500 package.json\\r\\n\u251c\u2500\u2500 public\\r\\n\u2502   \u251c\u2500\u2500 favicon.svg\\r\\n\u2502   \u2514\u2500\u2500 images\\r\\n\u2502       \u251c\u2500\u2500 10001.jpg\\r\\n\u2502       \u251c\u2500\u2500 10002.jpg\\r\\n\u2502       \u251c\u2500\u2500 10003.jpg\\r\\n\u2502       \u251c\u2500\u2500 ...\\r\\n\u2502       \u251c\u2500\u2500 19998.jpg\\r\\n\u2502       \u251c\u2500\u2500 19999.jpg\\r\\n\u2502       \u2514\u2500\u2500 20000.jpg\\r\\n\u251c\u2500\u2500 src\\r\\n\u2502   \u251c\u2500\u2500 App.vue\\r\\n\u2502   \u251c\u2500\u2500 assets\\r\\n\u2502   \u2502   \u2514\u2500\u2500 logo.svg\\r\\n\u2502   \u251c\u2500\u2500 components\\r\\n\u2502   \u2502   \u2514\u2500\u2500 HelloWorld.vue\\r\\n\u2502   \u251c\u2500\u2500 main.ts\\r\\n\u2502   \u251c\u2500\u2500 styles.css\\r\\n\u2502   \u251c\u2500\u2500 tailwind.config.js\\r\\n\u2502   \u251c\u2500\u2500 postcss.config.js\\r\\n\u2502   \u2514\u2500\u2500 shims-vue.d.ts\\r\\n\u2514\u2500\u2500 tsconfig.json\\r\\n```\\r\\n\\r\\nNow, it\u2019s time to start coding the chatbot interface.\\r\\n\\r\\n#### Coding the Chatbot Interface\\r\\n\\r\\nIn this section, you\u2019ll prepare your virtual stylist app\u2019s chatbot interface. You\u2019ll use Vue to create the main components, including the header, the footer, the chat window, and the image upload button. Then, you\u2019ll use the vue-advanced-chat component to create the chat messages, input, and options.\\r\\n\\r\\nTo keep things simple, we\u2019ll link to the code of non-essential components like the header and footer. Since these aren\u2019t critical to how the app functions, feel free to copy and paste them into your codebase.\\r\\n\\r\\n##### Header and Footer\\r\\n\\r\\nStart by creating two files in the `src/components` folder: `Header.vue` and `Footer.vue`. Next, copy the code from the [header](https://github.com/contentlab-io/Microsoft-Creating-a-Virtual-Stylist-Chatbot/blob/main/stylist_frontend/src/components/Header.vue) and [footer](https://github.com/contentlab-io/Microsoft-Creating-a-Virtual-Stylist-Chatbot/blob/main/stylist_frontend/src/components/Footer.vue) files in the GitHub repository into the files you just created.\\r\\n\\r\\nThese files are simple Vue components that use HTML and CSS to create a stylish header and footer for the app. If you\u2019d like to customize them, replace the logo image link in the header with a link to an image of your own.\\r\\n\\r\\nNow, it\u2019s time to dive into the chat interface that makes this app work.\\r\\n\\r\\n##### Creating the Chat Window Component\\r\\n\\r\\nThe chat window component displays the messages between the user and the stylist bot. To start, create a new file called `ChatWindow.vue` inside the project\u2019s src/components folder. Then, add the following code to it:\\r\\n\\r\\n```\\r\\n<template>\\r\\n  <div class=\\"chat-window h-screen\\">\\r\\n    <vue-advanced-chat\\r\\n      .messages=\\"messages\\"\\r\\n      .options=\\"options\\"\\r\\n      .rooms=\\"[{ roomId: \'main\', roomName: \'Stylist Chat\', avatar: \'/images/logo.svg\', users: [currentUser]}]\\"\\r\\n      :rooms-list-opened=\\"false\\"\\r\\n      :rooms-loaded=\\"true\\"\\r\\n      :messages-loaded=\\"true\\"\\r\\n      :current-user-id=\\"currentUser._id\\"\\r\\n      accepted-files=\\".png, .jpg, .jpeg\\"\\r\\n      show-audio=\\"false\\"\\r\\n      @send-message=\\"onInputSubmit\\"\\r\\n      .message-actions=\\"[{\\r\\n        label: \'Send\',\\r\\n        action: (message: Message) => {\\r\\n          console.log(\'Send message \' + message.content);\\r\\n        },\\r\\n      }]\\"\\r\\n      v-bind=\\"{\\r\\n        \'current-user-id\': currentUser?._id || \'\',\\r\\n        \'room-info-enabled\': false,\\r\\n       }\\"\\r\\n\\r\\n       />\\r\\n  </div>\\r\\n</template>\\r\\n\\r\\n<script lang=\\"ts\\">\\r\\nimport { defineComponent, ref, Ref } from \\"vue\\";\\r\\nimport { VueAdvancedChat, Message, register, RoomUser } from \\"vue-advanced-chat\\";\\r\\nregister();\\r\\nimport { v4 as uuidv4 } from \\"uuid\\";\\r\\n\\r\\nfunction toTimeString(date: Date): string {\\r\\n  let month = date.toLocaleString(\'default\', { month: \'short\' });\\r\\n  return `${date.getFullYear()}-${month}-${date.getDate()} ${date.getHours()}:${date.getMinutes()}`;\\r\\n}\\r\\n\\r\\nexport default defineComponent({\\r\\n  name: \\"ChatWindow\\",\\r\\n  components: {\\r\\n    VueAdvancedChat,\\r\\n  },\\r\\n  setup() {\\r\\n    // Define the current user, the messages, and the options for the chat component\\r\\n    const currentUser: Ref<RoomUser> = ref({\\r\\n      _id: \\"user\\",\\r\\n      username: \\"User\\",\\r\\n      avatar: \\"\\",\\r\\n      status: { state: \\"online\\", lastChanged: new Date().toDateString()},\\r\\n    });\\r\\n    const messages: Ref<Array<Message>> = ref([]);\\r\\n    const options = ref({\\r\\n      enableVoiceMessages: false,\\r\\n      enableReactions: false,\\r\\n      enableSeenBy: false,\\r\\n      enableLinkPreview: false,\\r\\n      enableUploads: true,\\r\\n      enableAttachments: false,\\r\\n      enableReply: true,\\r\\n      enableEdit: false,\\r\\n      enableDelete: false,\\r\\n      enableGroup: false,\\r\\n      enableSearch: false,\\r\\n      enableOptions: false,\\r\\n      enableScrollToBottom: true,\\r\\n      enableScrollToTop: false,\\r\\n      enableLoadMore: false,\\r\\n      enableComposer: true,\\r\\n      enableInput: true,\\r\\n      enableSendButton: true,\\r\\n      enableEmojis: false,\\r\\n      enableRecording: false,\\r\\n      enableMarkdown: true,\\r\\n      enableTypingIndicator: true,\\r\\n      enableOnlinePresence: false,\\r\\n      enableCustomTheme: true,\\r\\n      enableRooms: false,\\r\\n      customTheme: {\\r\\n        primaryColor: \\"#333333\\",\\r\\n        secondaryColor: \\"#f0f0f0\\",\\r\\n        tertiaryColor: \\"#ffffff\\",\\r\\n        quaternaryColor: \\"#e0e0e0\\",\\r\\n        quinaryColor: \\"#999999\\",\\r\\n        senaryColor: \\"#666666\\",\\r\\n        septenaryColor: \\"#333333\\",\\r\\n        octonaryColor: \\"#f0f0f0\\",\\r\\n        nonaryColor: \\"#ffffff\\",\\r\\n        denaryColor: \\"#e0e0e0\\",\\r\\n      },\\r\\n    });\\r\\n\\r\\n    // Update the image preview in the chat message after it\'s uploaded\\r\\n    const updateMessageImage = (newMessage: Message, url: string) => {\\r\\n      const existingMessage = messages.value.find(m => m._id === newMessage._id);\\r\\n      // Update the URL of the first message file\\r\\n      const message = existingMessage || newMessage;\\r\\n\\r\\n      if(message && message.files && message.files.length > 0) {\\r\\n        message.files[0].url = url;\\r\\n        const existingMessages = messages.value.filter(m => m._id !== message._id);\\r\\n        //set a new message ID to prevent file from being overwritten\\r\\n        message._id = uuidv4();\\r\\n        messages.value = [...existingMessages, message];\\r\\n      }\\r\\n    }\\r\\n\\r\\n    const onInputSubmit = async (event: CustomEvent) => {\\r\\n      // Create a new message object with the content and the current user\\r\\n      console.log(\\"called!\\")\\r\\n      let content = event.detail[0].content;\\r\\n      let files = event.detail[0].files;\\r\\n      const newMessage: Message = {\\r\\n        // generate uuid\\r\\n        _id: uuidv4(),\\r\\n        content,\\r\\n        senderId: currentUser.value._id,\\r\\n        date: new Date().toLocaleString(\'default\', { year: \'numeric\', month: \'short\', day: \'numeric\' }),\\r\\n        timestamp: toTimeString(new Date()),\\r\\n      };\\r\\n\\r\\n      if(files) {\\r\\n        newMessage.files = [...files.map((file: any) => {\\r\\n          var messageFile = {\\r\\n            name: file.name,\\r\\n            size: file.size,\\r\\n            type: file.type,\\r\\n            url: file.url || file.localUrl, \\r\\n            extension: file.extension,\\r\\n            preview: file.localUrl,\\r\\n          }\\r\\n          const reader = new FileReader();\\r\\n          reader.readAsDataURL(file.blob);\\r\\n\\r\\n          reader.onload = () => { \\r\\n            // Get the base64-encoded string from the reader result \\r\\n            messageFile.url = reader.result as string;\\r\\n            // reload messages so UI updates\\r\\n            messages.value = [...messages.value];\\r\\n            updateMessageImage(newMessage, messageFile.url!);\\r\\n            callBackendFunction(content, reader.result as string);\\r\\n          };\\r\\n          return messageFile;\\r\\n        })];\\r\\n      } else {\\r\\n\\r\\n        // Push the new message to the messages array\\r\\n        messages.value = [...messages.value, newMessage];\\r\\n        // Call the backend function to get the response from the stylist bot\\r\\n        callBackendFunction(content, \\"\\");\\r\\n      }\\r\\n    };\\r\\n\\r\\n    const callBackendFunction = async (prompt: string, image: string) => {\\r\\n      // Get the previous prompts and responses from the messages array\\r\\n      const context = messages.value\\r\\n        .filter((message) => message.content || message.replyMessage)\\r\\n        .map((message) => ({\\r\\n          prompt: message.content,\\r\\n          response: message.replyMessage,\\r\\n        }));\\r\\n      // Create a JSON object with the prompt, the image, and the context\\r\\n      const data = {\\r\\n        prompt,\\r\\n        image,\\r\\n        context,\\r\\n      };\\r\\n      // Send a POST request to the backend function URL with the data\\r\\n      const response = await fetch(\\"<backend function URL>\\", {\\r\\n        method: \'POST\',\\r\\n        headers: {\\r\\n          \'Content-Type\': \'application/json\',\\r\\n        },\\r\\n        body: JSON.stringify(data),\\r\\n      });\\r\\n      // Get the response data from the fetch response\\r\\n      const responseData = await response.json();\\r\\n      // Create a new message object with the response data and the stylist bot\\r\\n      const newMessage: Message = {\\r\\n        _id: uuidv4(),\\r\\n        content: responseData.response,\\r\\n        files: responseData.images,\\r\\n        senderId: \\"stylist-bot\\",\\r\\n        date: new Date().toLocaleString(\'default\', { year: \'numeric\', month: \'short\', day: \'numeric\' }),\\r\\n        timestamp: toTimeString(new Date()),\\r\\n      };\\r\\n      // Push the new message to the messages array\\r\\n      messages.value = [...messages.value, newMessage];\\r\\n    };\\r\\n\\r\\n    // Return the current user, the messages, the options, and the event handlers\\r\\n    return {\\r\\n      currentUser,\\r\\n      messages,\\r\\n      options,\\r\\n      onInputSubmit,\\r\\n    };\\r\\n  },\\r\\n\\r\\n  mounted() {\\r\\n    // Add a welcome message from the stylist bot when the component is mounted\\r\\n    this.messages = [...this.messages, { _id: \\"stylist-bot\\", content: \\"Hello! I\'m your virtual stylist chatbot. You can ask me for fashion advice, recommendations, and more. You can also upload images of clothing items and accessories to get personalized suggestions. How can I help you today?\\", senderId: \\"stylist-bot\\", date: new Date().toTimeString()}];\\r\\n  },\\r\\n});\\r\\n\\r\\n<\/script>\\r\\n\\r\\n<style scoped>\\r\\n.chat-window {\\r\\n  @apply h-screen flex-1 overflow-y-auto;\\r\\n}\\r\\n</style> \\r\\n```\\r\\n\\r\\nThis code defines a chat window component that uses the vue-advanced-chat component to display the messages between the user and the stylist bot. It also defines some data and methods to handle the chat logic, such as the current user, messages, options, input submit event, file upload event, and the back-end function call.\\r\\n\\r\\n`currentUser` and `messages` are reactive objects that store information about the chat participant and chat history. The `currentUser` object represents the app user while the `messages` array contains the Message objects with the following properties:\\r\\n\\r\\n* `_id`\u2014A unique identifier for the message\\r\\n* `content`\u2014The text content of the message (optional)\\r\\n* `files`\u2014Contains any files attached to the image (optional)\\r\\n* `senderId`\u2014The ID of the message sender\\r\\n* `date`\u2014The date of the message\\r\\n* `timestamp`\u2014The time and date that appear with every message\\r\\n\\r\\nThe `options` object contains the configuration options for the vue-advanced-chat component. It allows you to enable or disable various features of the chat interface, including:\\r\\n\\r\\n* Voice messages\\r\\n* Reactions\\r\\n* Seen by\\r\\n* Link preview\\r\\n* Uploads and attachments\\r\\n* Reply and send button\\r\\n* Edit and delete\\r\\n* Group and search\\r\\n* Options\\r\\n* Scroll to bottom and scroll to top\\r\\n* Load more\\r\\n* Composer\\r\\n* Input\\r\\n* Emojis\\r\\n* Recording\\r\\n* Markdown\\r\\n* Typing indicator\\r\\n* Online presence/status\\r\\n* Custom theme\\r\\n\\r\\nYou can learn more about the options and their meanings in the [documentation](https://github.com/advanced-chat/vue-advanced-chat).\\r\\n\\r\\nThe `onInputSubmit` method is the event handler for the input submit event. It\u2019s triggered when the user types a text message and presses the **Enter** key or clicks the **Send** button. This method creates a new message object with the text content and the current user, then pushes it to the messages array.\\r\\n\\r\\nIf the message contains an attached image file, the function loads it into a base64-encoded string, which is what the back-end Azure function expects to receive. Finally, it calls the back-end function to prompt a response from the stylist bot. \\r\\n\\r\\nThe `callBackendFunction` method calls the back-end Azure function to retrieve the stylist bot\u2019s reply. It takes the prompt and the image as parameters and sends a POST request to the back-end function URL with the data and the options. The data object contains the prompt, image, and context.\\r\\n\\r\\nThe context is an array of objects that store the previous prompts and responses from the `messages` array. The options object contains the headers for the request, such as the content type. The `response` object contains the response data from the back-end function, including the response, images, and context. \\r\\n\\r\\nFinally, the function creates a new `message` object with the response data and the stylist bot\u2019s ID, and then adds it to the `messages` array.\\r\\n\\r\\n:::info\\r\\nRegister for [Episode 2](https://aka.ms/serverless-learn-live/ep2?ocid=buildia24_60days_blogs) of the new learning series on **Intelligent Apps with Serverless on Azure**. Join the community along with MVPs, and the Azure Product Group on how to leverage AI with Serverless on Azure technologies\u2014Azure Functions and Azure Container Apps\u2014to build intelligent applications.\\r\\n:::\\r\\n\\r\\n#### Integrating Components into the App Component\\r\\n\\r\\nIn this section, you\u2019ll integrate the components you just created into the `src/App.vue` file\u2014your main app component. You\u2019ll import the header, footer, chat window, and image upload button components and display them in a simple layout.\\r\\n\\r\\nTo start, open the `App.vue` file in the project\u2019s `src` folder and replace the existing code with the following:\\r\\n\\r\\n```\\r\\n<template>\\r\\n  <div class=\\"app\\">\\r\\n    <Header />\\r\\n    <div class=\\"main\\">\\r\\n      <ChatWindow ref=\\"chat\\" />\\r\\n      <ImageUploadButton :chat=\\"chat\\" />\\r\\n    </div>\\r\\n    <Footer />\\r\\n  </div>\\r\\n</template>\\r\\n\\r\\n<script lang=\\"ts\\">\\r\\nimport { defineComponent, ref } from \\"vue\\";\\r\\nimport Header from \\"./components/Header.vue\\";\\r\\nimport Footer from \\"./components/Footer.vue\\";\\r\\nimport ChatWindow from \\"./components/ChatWindow.vue\\";\\r\\n\\r\\nexport default defineComponent({\\r\\n  name: \\"App\\",\\r\\n  components: {\\r\\n    Header,\\r\\n    Footer,\\r\\n    ChatWindow\\r\\n  },\\r\\n  setup() {\\r\\n    // Define a ref for the chat component\\r\\n    const chat = ref(ChatWindow);\\r\\n    // Return the ref\\r\\n    return {\\r\\n      chat,\\r\\n    };\\r\\n  },\\r\\n});\\r\\n<\/script>\\r\\n<style>\\r\\n.app {\\r\\n  @apply min-h-screen flex flex-col;\\r\\n}\\r\\n\\r\\n.main {\\r\\n  @apply flex-1 flex flex-col;\\r\\n}\\r\\n</style>\\r\\n```\\r\\n\\r\\nThis code defines the app component that uses the header, footer, chat window, and image upload button components. It also defines a [`ref`](https://vuejs.org/guide/essentials/template-refs) for the chat component and passes it as a prop to the image upload button component. This action allows the image upload button component to access the chat component\u2019s methods, such as `onFileUpload`.\\r\\n\\r\\nWith that, you\u2019re ready to deploy!\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nPart 2 of this series equipped you with the necessary skills to create a dynamic chatbot interface for your virtual stylist app. By setting up your project, installing dependencies, and coding the chatbot interface, you laid the groundwork for the final deployment and testing phase. Now, you\u2019re ready to see your virtual stylist in action.\\r\\n\\r\\nJump to the third part of this series, where you\u2019ll deploy and test your Intelligent App."},{"id":"creating-a-virtual-stylist-chatbot-part-1","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-1","source":"@site/blog-60daysofIA/2024-03-26/creating-a-virtual-stylist-chatbot-part-1.md","title":"6.1 Creating a Virtual Stylist Chatbot \u2014 Part 1: Analyzing Images with AI","description":"In this three-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this first part, you\u2019ll analyze clothing images using AI to generate a text description of the piece, focusing on the clothing\u2019s characteristics. ","date":"2024-03-26T09:00:00.000Z","formattedDate":"March 26, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":13.335,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-26T09:00","slug":"creating-a-virtual-stylist-chatbot-part-1","title":"6.1 Creating a Virtual Stylist Chatbot \u2014 Part 1: Analyzing Images with AI","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this three-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this first part, you\u2019ll analyze clothing images using AI to generate a text description of the piece, focusing on the clothing\u2019s characteristics. ","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"6.2 Creating a Virtual Stylist Chatbot \u2014 Part 2: Adding a Chatbot Interface","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-2"},"nextItem":{"title":"5 The Role of Platform Engineering in Developing Intelligent Apps","permalink":"/Cloud-Native/60DaysOfIA/the-role-of-platform-engineering-in-developing-intelligent-apps"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/creating-a-virtual-stylist-chatbot-part-1\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this three-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this first part, you\u2019ll analyze clothing images using AI to generate a text description of the piece, focusing on the clothing\u2019s characteristics. \\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-1\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this three-part series, you\u2019ll build a virtual stylist chatbot that uses AI to analyze images and suggest clothing items. In this first part, you\u2019ll analyze clothing images using AI to generate a text description of the piece, focusing on the clothing\u2019s characteristics. \\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/creating-a-virtual-stylist-chatbot-part-1\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![virtual stylist chatbot that uses AI to analyze images and suggest clothing items](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-1.jpeg)\\r\\n\\r\\n## Creating a Virtual Stylist Chatbot \u2014 Part 1: Analyzing Images with AI\\r\\n\\r\\nEver wished you had a personal fashion consultant who could help you find the ideal outfit for any occasion? What if you could use artificial intelligence (AI) to create a virtual stylist chatbot that could analyze clothing in images and suggest the perfect match from a database of clothing options.\\r\\n\\r\\nThis assistant is an example of an intelligent app\u2014an application that leverages AI to enhance and personalize its user experience.\\r\\n\\r\\nIn this three-part series, you\u2019ll learn how to build your own AI stylist app. When you\u2019re done, you\u2019ll have an app that can understand the contents of user-uploaded images and recommends similar items from a fashion image dataset.\\r\\n\\r\\nThe first article of this series demonstrates how to create the app\u2019s core logic. It analyzes the clothing styles in the image and finds the closest match in the dataset using Azure AI Search, Azure OpenAI Service, and Azure Functions. In the later parts of the series, you\u2019ll add a chatbot interface to the app.\\r\\n\\r\\nLet\u2019s get started!\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nBefore you start, ensure you have:\\r\\n\\r\\n* Python 3.10 or later\\r\\n* An Azure subscription with access to [Azure OpenAI Service](https://azure.microsoft.com/products/ai-services/openai-service?ocid=buildia24_60days_blogs)\\r\\n* [Azure command-line interface (CLI)](https://learn.microsoft.com/cli/azure/?ocid=buildia24_60days_blogs) installed\\r\\n* [Azure Functions Core Tools](https://github.com/Azure/azure-functions-core-tools) installed\\r\\n* An Azure OpenAI Service resource with a GPT-4 Vision model deployed. Read the [resource deployment guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal&ocid=buildia24_60days_blogs) if you haven\u2019t yet deployed a model. Note that [GPT-4 Vision](https://learn.microsoft.com/azure/ai-services/openai/concepts/models?ocid=buildia24_60days_blogs#model-summary-table-and-region-availability) is only available in the Sweden Central and West US regions, so be sure to select either of those two.\\r\\n* The deployment name, endpoint, and API key for your OpenAI Service. See the \u201cRetrieve key and endpoint\u201d section in the [Azure OpenAI Service docs](https://learn.microsoft.com/azure/ai-services/openai/dall-e-quickstart?pivots=programming-language-python?ocid=buildia24_60days_blogs#retrieve-key-and-endpoint) for details on finding your model\u2019s endpoint URL and API key.\\r\\n* The [Fashion Product Images dataset](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small) from Kaggle. Download and unzip the dataset. You\u2019ll only need the CSV file in part 1, but keep all the images because you\u2019ll use them later in the series.\\r\\n* Familiarity with Python\\r\\n* [Flask installed](https://flask.palletsprojects.com/en/3.0.x/installation/)\\r\\n* [Visual Studio Code](https://code.visualstudio.com/Download) or another code editor of your choice\\r\\n\\r\\nFor a preview, refer to the complete code for [part 1 available on GitHub](https://github.com/rogerwinter/Microsoft-Creating-a-Virtual-Stylist-Chatbot/tree/main/stylist-backend).\\r\\n\\r\\n### Analyzing Clothing Styles with AI\\r\\n\\r\\nWith the prerequisites in place, it\u2019s time to create an app from scratch. It will use Azure AI Search, Azure Functions (in Python), and Azure OpenAI Service to do the following:\\r\\n\\r\\n* Accept an image uploaded from a web interface. It should be an image of a clothing item or a person wearing one or more pieces of clothing.\\r\\n* Analyze that image using Azure OpenAI GPT-4 Turbo with Vision to generate a text description of the piece. Focus on describing the characteristics of the clothing.\\r\\n* Use the text description of the clothing\u2019s characteristics to find its closest matches in the clothing dataset.\\r\\n* Return a suggestion from the dataset of which clothing items are the best matches.\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n#### Create the Search Index and Upload the Dataset\\r\\n\\r\\nFirst, you must create a search index and upload the dataset that contains the clothing options. You\u2019ll use Azure AI Search, which can automatically ingest and parse the CSV data supplied with the fashion image dataset.\\r\\n\\r\\nBegin by uploading the CSV data included in the [fashion dataset](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small) into Azure Blob Storage. Navigate to the Storage Accounts page to get started. To find it quickly, enter its name in the Azure Portal\u2019s search bar:\\r\\n\\r\\n![image of storage accounts search in Azure](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-2.png)\\r\\n\\r\\nWhen the page loads, choose an existing storage account if you already have one. If not, create a new one. Click the storage account\u2019s name to load its dashboard page. Then, click **Upload** to upload a new file:\\r\\n\\r\\n![image of upload storage accounts search in Azure](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-3.jpeg)\\r\\n\\r\\nNext, select the `styles.csv` file from the fashion dataset downloaded from Kaggle.\\r\\n\\r\\n![image of file upload](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-4.png)\\r\\n\\r\\nIf you have an existing storage container you\u2019d like to use, select it from the dropdown menu. Otherwise, click the link to create a new one. Either way, ensure the container is empty before proceeding. The `styles.csv` file you upload should be the only file in the container.\\r\\n\\r\\nNow, you\u2019re ready to create the AI Search service. Look it up using the Azure Portal search box:\\r\\n\\r\\n![image of searching for AI Search service in Azure Portal](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-5.png)\\r\\n\\r\\nWhen the AI Search page loads, click **+ Create** to create a new AI Search instance.\\r\\n\\r\\n![image of create in AI services](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-6.png)\\r\\n\\r\\nSelect the subscription and resource group you\u2019d like to use to create the search service. Then, enter a unique name of your choice \u2014 this demonstration uses \u201cstylist-search-service.\u201d\\r\\n\\r\\n![image of fields for creating a new search service in AI services](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-7.png)\\r\\n\\r\\nUse the defaults for all remaining settings and click **Create** to create the search service. This may take a few minutes. The Azure Portal will let you know when the service is ready.\\r\\n\\r\\nNow, it\u2019s time to index the data in the `styles.csv` file you uploaded to Blob Storage earlier. From the main page of your new search index, click **Import data**.\\r\\n\\r\\n![image of import data option for indexing data](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-8.jpeg)\\r\\n\\r\\nIn the first data import screen, select **Azure Blob Storage** as the data source and enter \u201cfashion-images\u201d as the data source name. Choose **Delimited text** as the parsing mode, and enter a comma as the delimiter character. For the connection string, click **Choose an existing connection** and select the storage container where you uploaded `styles.csv`. Delete the forward slash in the Blob folder input box. Azure will auto-populate the connection string.\\r\\n\\r\\n![image of fields for importing data](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-9.png)\\r\\n\\r\\nClick **Next** until Azure prompts you to customize the target index, and then update the field settings as follows:\\r\\n\\r\\n![image of field settings for importing data](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-10.png)\\r\\n\\r\\nClick **Next**. On the final screen, enter a name for the indexer and click **Submit**.\\r\\n\\r\\n![image of final screen when importing data](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-11.png)\\r\\n\\r\\nAzure will create a search index and then run the ingester to import the data. It should finish in under two minutes. When it does, you\u2019re done with search index creation.\\r\\n\\r\\n:::info\\r\\nRegister for the new learning series on **[Intelligent Apps with Serverless on Azure](https://aka.ms/serverless-learn-live?ocid=buildia24_60days_blogs)**. Join the community along with MVPs, and the Azure Product Group on how to leverage AI with Serverless on Azure technologies \u2013Azure Functions and Azure Container Apps \u2013 to build intelligent applications.\\r\\n:::\\r\\n\\r\\n#### Create the Azure Function\\r\\n\\r\\nThe next step is to create the Azure Function that will perform image analysis, matching logic, and recommendation generation. You\u2019ll use Python as the programming language and Flask as the web framework.\\r\\n\\r\\nTo create and deploy the Azure Functions app, use the Azure Functions CLI. Open a terminal and create a new directory to store your app. Then, run:\\r\\n\\r\\n```\\r\\nfunc init --python\\r\\n```\\r\\n\\r\\nThe app generator will run. Open the directory in Visual Studio Code or your text editor of choice. You should see several files:\\r\\n\\r\\n![the directory in Visual Studio Code](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-12.png)\\r\\n\\r\\nOpen `requirements.txt` and add the following:\\r\\n\\r\\n```\\r\\nazure-functions\\r\\nrequests\\r\\nazure-search-documents\\r\\n```\\r\\n\\r\\nThis change ensures Azure will install all the dependencies the function needs before trying to run it.\\r\\n\\r\\nNext, open `function_app.py` and replace its contents with the following:\\r\\n\\r\\n```\\r\\nimport base64\\r\\nimport os\\r\\nimport json\\r\\nimport requests\\r\\nimport azure.functions as func\\r\\nfrom azure.search.documents import SearchClient\\r\\nfrom azure.core.credentials import AzureKeyCredential\\r\\n\\r\\napp = func.FunctionApp()\\r\\n\\r\\n# Get the environment variables\\r\\nOPENAI_API_KEY = os.environ[\'OPENAI_API_KEY\']\\r\\nOPENAI_ENDPOINT = os.environ[\'OPENAI_ENDPOINT\']\\r\\nOPENAI_DEPLOYMENT_NAME = os.environ[\'OPENAI_DEPLOYMENT_NAME\']\\r\\nSEARCH_API_KEY = os.environ[\'SEARCH_API_KEY\']\\r\\nSEARCH_ENDPOINT = os.environ[\'SEARCH_ENDPOINT\']\\r\\nSEARCH_INDEX_NAME = os.environ[\'SEARCH_INDEX_NAME\']\\r\\n\\r\\n# Initialize the Azure OpenAI headers\\r\\nopenai_headers = {\\r\\n    \'Authorization\': \'Bearer {}\'.format(OPENAI_API_KEY),\\r\\n    \'Content-Type\': \'application/json\'\\r\\n}\\r\\n\\r\\n# Initialize the Azure Search client\\r\\nsearch_credentials = AzureKeyCredential(SEARCH_API_KEY)\\r\\nsearch_client = SearchClient(SEARCH_ENDPOINT, SEARCH_INDEX_NAME, search_credentials)\\r\\n\\r\\n@app.route(route=\\"stylist\\", methods=[\\"post\\"], auth_level=func.AuthLevel.FUNCTION)\\r\\ndef stylist(req: func.HttpRequest) -> func.HttpResponse:\\r\\n    # get image from request and convert to a base64 string\\r\\n    image = req.files[\\"image\\"]\\r\\n    image_bytes = image.read()\\r\\n    image_base64 = base64.b64encode(image_bytes).decode(\\"utf-8\\")\\r\\n\\r\\n    # Generate a text description from the image using Azure OpenAI\\r\\n    base_url = f\\"{OPENAI_ENDPOINT}openai/deployments/{OPENAI_DEPLOYMENT_NAME}\\"\\r\\n    endpoint = f\\"{base_url}/chat/completions?api-version=2023-12-01-preview\\"\\r\\n    data = {\\r\\n        \\"messages\\": [\\r\\n            { \\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\" },\\r\\n            { \\"role\\": \\"user\\", \\"content\\": [\\r\\n                {\\r\\n                    \\"type\\": \\"text\\",\\r\\n                    \\"text\\": \\"Describe the main fashion item in this picture. Make sure you include the type of item (e.g., Shirt, T-Shirt, Shorts, Pants, Dress, Purse, Clutch), the color of the item, and \'Men\' or \'Women\' if the fashion item appears to be specific to either of those genders.\\"\\r\\n                },\\r\\n                {\\r\\n                    \\"type\\": \\"image_url\\",\\r\\n                    \\"image_url\\": {\\r\\n                        \\"url\\": image_base64\\r\\n                    }\\r\\n                }\\r\\n            ] }\\r\\n        ],\\r\\n        \\"max_tokens\\": 2000\\r\\n    }\\r\\n\\r\\n    response = requests.post(endpoint, headers=openai_headers, data=json.dumps(data))\\r\\n    result = response.json()\\r\\n    image_description = result[\'text\']\\r\\n\\r\\n    # Find the closest match from the search index using Azure OpenAI\\r\\n    search_result = search_client.search(\\r\\n        search_text=image_description,\\r\\n        select=[\\"id\\", \\"productDisplayName\\"],\\r\\n        top=1\\r\\n    )\\r\\n    match_id = search_result[\\"id\\"]\\r\\n    match_name = search_result[\\"productDisplayName\\"]\\r\\n\\r\\n    # Generate a natural language recommendation based on the match result using Azure OpenAI\\r\\n    data = {\\r\\n        \\"messages\\": [\\r\\n            { \\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\" },\\r\\n            { \\"role\\": \\"user\\", \\"content\\": [\\r\\n                {\\r\\n                    \\"type\\": \\"text\\",\\r\\n                    \\"text\\": f\\"Please generate a natural language recommendation based on the matching item: {match_id}, {match_name}. For example: The best match for your clothing item is: Peter England Men Party Blue Jeans. This is a pair of jeans for men in blue color, suitable for casual occasions. You can pair it with a shirt or a t-shirt of your choice.\\"\\r\\n                }\\r\\n            ] }\\r\\n        ],\\r\\n        \\"max_tokens\\": 2000\\r\\n    }\\r\\n    response = requests.post(endpoint, headers=openai_headers, data=json.dumps(data))\\r\\n    result = response.json()\\r\\n    recommendation = result[\'text\']\\r\\n\\r\\n    # Return the recommendation as a JSON response\\r\\n    return func.HttpResponse(json.dumps({\\r\\n        \'image_id\': match_id,\\r\\n        \'recommendation\': recommendation\\r\\n    })) \\r\\n```\\r\\n\\r\\nLet\u2019s break down what\u2019s happening step by step.\\r\\n\\r\\nFirst, you set up the function app and Azure clients. This code:\\r\\n\\r\\n* Initializes an Azure Function with an HTTP trigger\\r\\n* Retrieves necessary API keys and endpoints from environment variables for OpenAI and Azure Search services\\r\\n* Sets up headers for interacting with Azure OpenAI Service and a client for using Azure Search\\r\\n\\r\\nThen, you define the `process_image` function. This function:\\r\\n\\r\\n* Is the application\u2019s code and executes on a specific request to the Flask app\\r\\n* Receives an image as part of the request\\r\\n\\r\\nNext, you generate text descriptions with Azure OpenAI. This code:\\r\\n\\r\\n* Constructs a request to Azure OpenAI Service\u2019s chat API to generate a description of the main fashion item in the image. The request includes the image and a prompt to describe the fashion item, including its type, color, and gender specificity.\\r\\n* Sends the request and extracts the generated description from the response\\r\\n\\r\\nAfter, you search for a matching product. This code:\\r\\n\\r\\n* Uses the Azure Search client to search for a product that matches the description generated by OpenAI. The search query uses the textual description and selects specific fields (`id`, `productDisplayName`) from the search index.\\r\\n* Extracts the ID and display name of the closest matching product from the search results\\r\\n\\r\\nThen, you generate a natural language recommendation. This code:\\r\\n\\r\\n* Constructs another request to Azure OpenAI Service\u2019s chat API to generate a natural language recommendation based on the matching product. The request includes the matching product\u2019s details and asks for a natural language recommendation.\\r\\n* Sends the request and extracts the recommendation from the response\\r\\n\\r\\nNext, the code returns the recommendation:\\r\\n\\r\\n* The function ends by returning a JSON response containing the matching product\u2019s ID and the natural language recommendation.\\r\\n* The ID matches the file name of an image in the dataset, so you can use it to load and display images in the web UI in parts 2 and 3 of this series.\\r\\n\\r\\nFinally, you define a main function:\\r\\n\\r\\n* This is the function Azure runs to boot the function app and prepare it to receive HTTP requests.\\r\\n\\r\\nThis app combines image processing, text generation, and search capabilities to provide fashion item recommendations. It demonstrates how to implement the entire back end of an intelligent application.\\r\\n\\r\\n#### Deploy the Azure Function\\r\\n\\r\\nThe final step is to deploy the Azure Function to the cloud so the web interface can access it. Start by using the Azure CLI to create a new function app:\\r\\n\\r\\n```\\r\\naz functionapp create --resource-group <RESOURCE_GROUP_NAME> --consumption-plan-location westus --runtime python --runtime-version 3.9 --functions-version 4 --name <APP_NAME> --os-type linux --storage-account <STORAGE_NAME> \\r\\n```\\r\\n\\r\\nYou can use the same resource group and storage account you used for the search service.\\r\\n\\r\\n**Note**: The app name must be unique, so you might need to try a few options to find one available.\\r\\n\\r\\nNext, set all the environment variables the app will need by running:\\r\\n\\r\\n```\\r\\naz functionapp config appsettings set \u2013name <APP_NAME> --resource-group <RESOURCE_GROUP> --settings\\r\\nOPENAI_API_KEY=<your Azure OpenAI key>\\r\\nOPENAI_ENDPOINT=<your Azure OpenAI endpoint>\\r\\nOPENAI_DEPLOYMENT_NAME=<your Azure OpenAI deployment name>\\r\\nSEARCH_API_KEY=<your Search API key>\\r\\nSEARCH_ENDPOINT=<your Search service endpoint>\\r\\nSEARCH_INDEX_NAME=<your Search index name>\\r\\n```\\r\\n\\r\\nIf you\u2019re unsure where to find any of these values, here\u2019s how to locate them:\\r\\n\\r\\n* For the OpenAI values, find and load the Azure OpenAI page by entering OpenAI in the Azure Portal search bar. Click the name of your Azure OpenAI service, and you\u2019ll see two menu options:\\r\\n\\r\\n  ![the directory in Visual Studio Code](../../static/img/60-days-of-ia/blogs/2024-03-26/6-1-13.png)\\r\\n\\r\\n  Click **Keys and Endpoint** to locate the required information, or click **Model deployments** to navigate to Azure OpenAI Studio and find the names of your model deployments.\\r\\n\\r\\n* For the search service, load your stylist search service\u2019s page in Azure Portal:\\r\\n\\r\\n  * On the **Overview** page, the **Url** value is your search endpoint.\\r\\n  * Click **Keys** in the menu to access your search service\u2019s keys.\\r\\n  * Click **Indexes** in the menu to see the name of your search service\u2019s index.\\r\\n\\r\\nNote that you\u2019re saving these as app settings for simplicity. In a product app, you should keep secrets like API keys safe by using [Azure Key Vault](https://azure.microsoft.com/products/key-vault?ocid=buildia24_60days_blogs).\\r\\n\\r\\nOnce you\u2019ve created the app and saved its settings, you can deploy your function by running the following command from the Azure Functions CLI:\\r\\n\\r\\n```\\r\\nfunc azure functionapp publish <APP_NAME>\\r\\n```\\r\\n\\r\\nThe CLI will begin deploying your app. When the deployment is complete, the interface will provide a URL to send HTTPS requests to the function app.\\r\\n\\r\\nNow, the back end of the stylist chatbot app is complete! You\u2019re ready to move on to creating the web interface for the app.\\r\\n\\r\\n:::info\\r\\nJoin the Azure Functions product group for an **[Ask The Expert](https://aka.ms/intelligent-apps/ate-functions?ocid=buildia24_60days_blogs)** session on how to focus on the pieces of code that matter most to you in AI application development, while Azure Functions handles the rest for you.\\r\\n:::\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nIn this article, you learned how to create a virtual stylist chatbot that can analyze clothing styles in an image and identify the best match from a dataset of clothing options \u2014 leveraging Azure Functions and Azure OpenAI Service to do so. You also learned how to use Azure AI Search feature to index, store, and retrieve entries from a search index. Next, you discovered how to use Azure OpenAI Service to generate natural language descriptions and recommendations based on the user\u2019s input image.\\r\\n\\r\\nIn the next part of this series, you\u2019ll learn how to add a chatbot interface to the app using React and an [Azure Static Web App](https://azure.microsoft.com/products/app-service/static?ocid=buildia24_60days_blogs)."},{"id":"the-role-of-platform-engineering-in-developing-intelligent-apps","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/the-role-of-platform-engineering-in-developing-intelligent-apps","source":"@site/blog-60daysofIA/2024-03-22/the-role-of-platform-engineering-in-developing-intelligent-apps.md","title":"5 The Role of Platform Engineering in Developing Intelligent Apps","description":"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.","date":"2024-03-22T09:00:00.000Z","formattedDate":"March 22, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":8.935,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-22T09:00","slug":"the-role-of-platform-engineering-in-developing-intelligent-apps","title":"5 The Role of Platform Engineering in Developing Intelligent Apps","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization","serverless","ai apps"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"6.1 Creating a Virtual Stylist Chatbot \u2014 Part 1: Analyzing Images with AI","permalink":"/Cloud-Native/60DaysOfIA/creating-a-virtual-stylist-chatbot-part-1"},"nextItem":{"title":"4.5 Deploying Your Copilot On Azure","permalink":"/Cloud-Native/60DaysOfIA/deploying-your-copilot-on-azure"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/the-role-of-platform-engineering-in-developing-intelligent-apps\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/the-role-of-platform-engineering-in-developing-intelligent-apps\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps, triumphing over traditional approaches.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/the-role-of-platform-engineering-in-developing-intelligent-apps\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![The Role of Platform Engineering in Developing Intelligent Apps](../../static/img/60-days-of-ia/blogs/2024-03-22/5-1.png)\\r\\n\\r\\n## The Role of Platform Engineering in Developing Intelligent Apps\\r\\n\\r\\nIntelligent Apps leverage advanced technologies like machine learning (ML), data analytics, and artificial intelligence (AI) to enhance decision-making, content generation, and user experiences. These apps incorporate AI and ML components to process data, derive insights, and adapt to user behavior to boost efficiency and personalization.\\r\\n\\r\\n[Platform engineering](https://learn.microsoft.com/platform-engineering/what-is-platform-engineering?ocid=buildia24_60days_blogs) is integral to building robust Intelligent Apps. It\u2019s the DevOps-inspired practice of designing, building, and maintaining the infrastructure and systems that underpin software applications. This includes strengthening security, maintaining compliance, controlling costs, and enhancing business value within a governed framework and via an internal developer platform.\\r\\n\\r\\nIntelligent Apps require careful planning and execution as they necessitate complex processes like data management, model optimization and training, algorithm selection, and development. Fortunately, platform engineering helps with these tasks.\\r\\n\\r\\nCoupled with Azure services, platform engineering creates a robust foundation for developing, deploying, and maintaining Intelligent Apps. With the help of platform engineering, you\u2019re free to focus on what you do best as a developer. Instead of worrying about the details of infrastructure, ensuring compliance, or navigating the maze of underlying technologies that support modern apps, you can put your efforts toward designing, implementing, and iterating on their Intelligent Apps.\\r\\n\\r\\nWith platform engineering practices, Azure\u2019s scalable infrastructure streamlines the development lifecycle, enabling rapid prototyping and iteration. Azure services\u2014including Azure\u2019s [AI portfolio](https://azure.microsoft.com/solutions/ai?ocid=buildia24_60days_blogs) and [Azure OpenAI Service](https://azure.microsoft.com/products/ai-services/openai-service?ocid=buildia24_60days_blogs)\u2014enhance app intelligence. Furthermore, [Azure\u2019s DevOps-supporting tools](https://azure.microsoft.com/products?ocid=buildia24_60days_blogs#devops) automate deployment and maintenance tasks, ensuring seamless updates and operational efficiency.\\r\\n\\r\\nLet\u2019s explore how Azure and platform engineering pave the way for the efficient development, deployment, and maintenance of Intelligent Apps.\\r\\n\\r\\n### How Platform Engineering Paves the Way for Intelligent Apps\\r\\n\\r\\nThe rise of cloud computing and microservices has laid the groundwork for platform engineering techniques.\\r\\n\\r\\nAs applications have become more advanced in their functionality, so too has the underlying ecosystem necessary to deploy, manage, and maintain them. This shift necessitated creating specialized platforms to manage deployment complexities using platform engineering processes. Cloud resources and Infrastructure as Code (IaC) further revolutionized platform engineering by automating infrastructure provisioning and management via code. This streamlined resource deployment, improved scalability, and boosted reliability across environments.\\r\\n\\r\\nWhile these technologies provide substantial benefits to developers, they require a careful strategy to be truly effective. This is where platform engineering truly shines. A well-engineered platform supports developer efforts through the concept of self-service with guardrails. It facilitates the autonomy you need in your workflows while simultaneously offering a set of organizational constraints that free you from unnecessary context switching, helping dissolve silos between teams.\\r\\n\\r\\nOften embracing an \u201ceverything as code\u201d philosophy, platform engineering ensures that everything from infrastructure to deployment is easily managed. The \u201cas code\u201d concept allows for infrastructure, policy, security, and all cloud configurations to be maintained like any other form of code using familiar tools and repositories. This approach offers a powerful method for achieving self-service autonomy, enabling you to make changes in a format you\u2019re already familiar with.\\r\\n\\r\\nPlatform engineering supports the entire lifecycle of Intelligent Apps, from conceptualization to deployment and scaling:\\r\\n\\r\\n* **Conceptualization and design**\u2014Platform engineering teams collaborate with app developers, data scientists, and stakeholders to grasp Intelligent Apps\u2019 requirements. They offer tech insights, aiding in tech selection and architecture design for scalability, reliability, and performance. Platform engineers help design data architecture, encompassing pipelines, storage, and processing frameworks. They also provide start-right templates that ensure you can start the development process correctly, adhering to policy, following best practices, and utilizing the most relevant technologies.\\r\\n* **Development and testing**\u2014Platform engineers configure development environments, giving you tools for efficient coding. They also establish continuous integration and continuous delivery (CI/CD) pipelines for automated processes and offer testing infrastructures. Tools like [Azure Pipelines](https://azure.microsoft.com/products/devops/pipelines/?ocid=buildia24_60days_blogs) and [Azure Test Plans](https://azure.microsoft.com/products/devops/test-plans/?ocid=buildia24_60days_blogs) help. By putting the right infrastructure in place\u2014and empowering you with enough self-service autonomy to utilize this infrastructure\u2014platform engineering allows for more focused and consistent development and testing cycles.\\r\\n* **Model training and optimization**\u2014Using technologies like distributed computing and GPU acceleration, platform engineers work with data scientists to establish scalable infrastructure for model training. They enhance training efficiency by adjusting hardware setups, refining data pipelines, and employing parallel processing methods. Additionally, they integrate model monitoring tools for performance tracking and retraining.\\r\\n* **Deployment and scaling**\u2014Platform engineers create automated deployment pipelines and infrastructure templates for deploying Intelligent Apps across various environments. They guarantee reliable, scalable processes, monitor performance, and use tools like Kubernetes for containerized workloads, ensuring scalability, resilience, and portability.\\r\\n* **Monitoring and maintenance**\u2014Platform engineers deploy monitoring and observability tools, like [Azure Monitor](https://azure.microsoft.com/products/monitor/?ocid=buildia24_60days_blogs), for real-time tracking of Intelligent Apps\u2019 health, performance, and usage. They set up alert systems and automated responses, ensuring proactive issue detection and minimizing downtime. Regular performance tuning, capacity planning, and security audits optimize infrastructure efficiency.\\r\\n\\r\\nPlatform engineering ensures scalability, improves reliability through optimized performance, and fosters efficiency by automating workflows\u2014aspects that make for more powerful, performant Intelligent Apps.\\r\\n\\r\\n:::info\\r\\nExplore the [Platform Engineering Guide](https://learn.microsoft.com/platform-engineering/?ocid=buildia24_60days_blogs) to learn how platform engineering teams can use building blocks from Microsoft and other vendors to create deeply personalized, optimized, and secure developer experiences.\\r\\n:::\\r\\n\\r\\n#### Platform Engineering in Action\\r\\n\\r\\nTo illustrate the impact of platform engineering on developing Intelligent Apps, let\u2019s compare the journey of developing one such app using a traditional software development methodology versus using an Azure-supported platform engineering approach.\\r\\n\\r\\n##### The Initial Phase\\r\\n\\r\\nIn the traditional software development approach, the initial phase typically involves requirements gathering followed by siloed development stages. Comparatively, a platform engineering approach involves embracing integrated planning and development stages. Teams collaborate from the beginning, leveraging Azure\u2019s cloud capabilities for streamlined workflows. Azure services, such as [Azure DevOps](https://azure.microsoft.com/products/devops/?ocid=buildia24_60days_blogs), facilitate seamless coordination between development, operations, and product teams, ensuring alignment with business objectives.\\r\\n\\r\\nFrom the outset, platform engineering supports your development efforts with a templated approach to creating new apps or services. Platform engineers often create start-right templates based on best practices, industry standards, or organizational guidelines to ensure consistency, efficiency, and quality from the project\u2019s conception. These templates may encompass IaC templates, service catalogs, or repositories containing pre-built components and libraries you can use to accelerate development.\\r\\n\\r\\nPlatform engineers help to define the core policies that govern resource provisioning and configuration. These policies might include restrictions on resource types, sizes, or regions, as well as rules for security, compliance, and cost management. With these guardrails in place, you can work without constantly looking over your shoulder to ensure you\u2019re adhering to policy and best practices.\\r\\n\\r\\n##### Development and Deployment Phases\\r\\n\\r\\nTraditionally, development progresses linearly with limited flexibility for adaptation. Deployment may encounter challenges due to disparate environments, leading to inconsistencies. Moreover, in traditional methods, siloing commonly occurs across design, development, testing, and deployment stages, hampering communication, slowing progress, creating inefficiencies, and hindering collaboration\u2014ultimately resulting in disjointed outcomes.\\r\\n\\r\\nBy empowering you with self-service platforms and powerful automation, a well-engineered platform expedites your development efforts. Self-service interfaces simplify provisioning infrastructure resources such as virtual machines, containers, databases, storage, and networking for use in these phases. You can request and provision the resources you need on-demand without waiting for manual intervention from infrastructure teams.\\r\\n\\r\\nLeveraging Azure services like [Azure Kubernetes Service](https://azure.microsoft.com/products/kubernetes-service?ocid=buildia24_60days_blogs) (AKS) and [Azure App Service](https://azure.microsoft.com/products/app-service?ocid=buildia24_60days_blogs) makes deployment automated and scalable, ensuring consistent performance across environments.\\r\\n\\r\\nAdditionally, Azure\u2019s AI-specific tools\u2014including [Azure Machine Learning](https://azure.microsoft.com/products/machine-learning/?ocid=buildia24_60days_blogs), [Data Science Virtual Machines](https://azure.microsoft.com/products/virtual-machines/data-science-virtual-machines/?ocid=buildia24_60days_blogs#overview) (DSVMs), and [Azure AI Language](https://azure.microsoft.com/products/ai-services/ai-language/?ocid=buildia24_60days_blogs)\u2014make developing and deploying robust Intelligent Apps straightforward.\\r\\n\\r\\n##### The Maintenance Phase\\r\\n\\r\\nDuring the maintenance phase, the benefits of platform engineering shine even brighter. Traditional methods often struggle with managing ongoing updates and addressing user feedback promptly. Moreover, post-deployment maintenance using traditional methodology requires dedicated resources for ongoing support and updates.\\r\\n\\r\\nPlatform engineering selects, deploys, and configures infrastructure monitoring tools that provide visibility into the underlying infrastructure components\u2019 health and performance. By letting you access these metrics directly, platform engineering arms you with the information you need to make informed decisions for maintenance and optimization.\\r\\n\\r\\nPlatform engineering enables teams to iterate rapidly based on real-time insights, leveraging Azure\u2019s analytics and monitoring tools to gather actionable data. For instance, [Azure Monitor](https://learn.microsoft.com/azure/azure-monitor/overview?ocid=buildia24_60days_blogs) and [Application Insights](https://learn.microsoft.com/azure/azure-monitor/app/app-insights-overview?ocid=buildia24_60days_blogs) enable proactive monitoring and efficient troubleshooting, minimizing downtime and optimizing performance. Additionally, Azure DevOps facilitates iterative improvements through feature flags and A/B testing, so teams can gather feedback and iterate quickly.\\r\\n\\r\\nFurthermore, Azure\u2019s AI-powered tools\u2014including [AI Anomaly Detector](https://azure.microsoft.com/products/ai-services/ai-anomaly-detector/?ocid=buildia24_60days_blogs) and [Azure AI Metrics Advisor](https://azure.microsoft.com/products/ai-services/ai-metrics-advisor/?ocid=buildia24_60days_blogs)\u2014support analytics and anomaly detection, allowing teams to address issues before they impact users.\\r\\n\\r\\n#### The Benefits of Platform Engineering\\r\\n\\r\\nLet\u2019s review some of the benefits of using platform engineering over traditional development methods for your Intelligent Apps:\\r\\n\\r\\n* **Reduced time to market**\u2014Platform engineering accelerates software development through reusable infrastructure components, automation tools, and standardized workflows. This contrasts with traditional methods, which prolong development cycles due to manual processes and lack of automation.\\r\\n* **Improved app quality**\u2014Platform engineering ensures consistency, repeatability, and reliability with standardized configurations, security policies, and automated testing. With traditional approaches, quality may suffer due to manual testing, ad-hoc configurations, and inconsistent environments.\\r\\n* **Scalability and resilience**\u2014Intelligent Apps designed with platform engineering in mind have resilient infrastructure that supports seamless scalability thanks to automated scaling and fault-tolerant architecture. Traditional, manual development methods can\u2019t compete.\\r\\n* **Enhanced ability to iterate based on user feedback**\u2014Traditional methods face the constraints of manual processes and lengthy deployment cycles. Comparatively, platform engineering facilitates rapid iteration and experimentation with flexible platform infrastructure.\\r\\n* **Operational efficiency**\u2014Platform engineering improves efficiency through automation, standardized processes, and centralized management. This contrasts with traditional methods, where operational tasks are more manual, leading to inefficiencies and increased costs.\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs)**\u202fto compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n### Conclusion\\r\\n\\r\\nAzure services and platform engineering have revolutionized the landscape of Intelligent Apps development, offering organizations unprecedented scalability, flexibility, and efficiency. And with Azure\u2019s robust suite of tools, you can deliver intelligent solutions that drive growth and enhance customer experiences.\\r\\n\\r\\nAs we look to the future, the potential of Intelligent Apps to provide substantial business value only continues to grow, promising even greater insights, automation, and competitive advantages. To learn more about the transformative power of Azure, join us at [Microsoft Build](https://build.microsoft.com/en-US/home?ocid=buildia24_60days_blogs)."},{"id":"deploying-your-copilot-on-azure","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/deploying-your-copilot-on-azure","source":"@site/blog-60daysofIA/2024-03-15/build-a-copilot-on-azure-code-first-with-prompt-flow.md","title":"4.5 Deploying Your Copilot On Azure","description":"You\'ve build a RAG-based copilot application on Azure AI with Prompt flow. Now it\'s time to deploy it, test it, and integrated it into your chat UI experience. Let\'s dive in!","date":"2024-03-15T09:00:00.000Z","formattedDate":"March 15, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":11.62,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-15T09:00","slug":"deploying-your-copilot-on-azure","title":"4.5 Deploying Your Copilot On Azure","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["azure","ai-studio","automation","accelerator"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"You\'ve build a RAG-based copilot application on Azure AI with Prompt flow. Now it\'s time to deploy it, test it, and integrated it into your chat UI experience. Let\'s dive in!","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"5 The Role of Platform Engineering in Developing Intelligent Apps","permalink":"/Cloud-Native/60DaysOfIA/the-role-of-platform-engineering-in-developing-intelligent-apps"},"nextItem":{"title":"4.4 Build a Copilot on Azure Code-First with Langchain","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-langchain"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/deploying-your-copilot-on-azure\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"You\'ve build a RAG-based copilot application on Azure AI with Prompt flow. Now it\'s time to deploy it, test it, and integrated it into your chat UI experience. Let\'s dive in!\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/deploying-your-copilot-on-azure\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"You\'ve build a RAG-based copilot application on Azure AI with Prompt flow. Now it\'s time to deploy it, test it, and integrated it into your chat UI experience. Let\'s dive in!\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/deploying-your-copilot-on-azure\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n\\r\\nWelcome to `Day 5\ufe0f\u20e3` of our journey **Building An AI App End-to-End On Azure!**. It\'s time to wrap-up the week with a look at two key topics - _deployment_ and _responsible AI_! Ready? Let\'s go!\\r\\n\\r\\n## What You\'ll Learn In This Post\\r\\n * Deploying the chat AI (Contoso Chat)\\r\\n * Deploying the chat UI (Contoso Web)\\r\\n * Automate Deployments (CI/CD)\\r\\n * Accelerate Solutions (Enterprise)\\r\\n * Evaluate & Mitigate Harms (Responsible AI)\\r\\n * Exercise: Explore training resources.   \\r\\n * Resources: [**Azure AI Studio Code-First Collection**](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs) \\r\\n\\r\\n<br/>\\r\\n\\r\\n![Deploy with Responsible AI](../../static/img/60-days-of-ia/blogs/2024-03-15/banner.png)\\r\\n\\r\\n---\\r\\n## 1. Revisiting Contoso Chat\\r\\n\\r\\nWe started the week by talking about LLM Ops, and identifying the three core phases of the end-to-end lifecycle for a generative AI application. In the previous posts, we\'ve mostly focused on the first two phases: **ideating** (building & validating a starter app) and **augmenting** (evaluating & iterating app for quality). In this post, we\'ll focus on phase 3: **operationalizing** the application to get it ready for real-world usage.\\r\\n\\r\\n![LLM Ops](../../static/img/60-days-of-ia/blogs/2024-03-15/llm-app-lifecycle.png)\\r\\n\\r\\nFirst, let\'s remind ourselves of the high-level architecture for a copilot application. Our solution has two components:\\r\\n - **Backend**: The _chat AI_ app that is deployed to provide a hosted API endpoint.\\r\\n - **Frontend**: The _chat UI_ app that is deployed to support user interactions with API.\\r\\n\\r\\nLet\'s look at what deployment means in each case:\\r\\n\\r\\n![Copilot Arch](../../static/img/60-days-of-ia/blogs/2024-03-15/copilot-architecture.png)\\r\\n\\r\\n## 2. Deploy your chat AI app\\r\\n\\r\\nIn our example, the chat AI is implemented by the [Contoso Chat](https://aka.ms/aitour/contoso-chat?ocid=buildia24_60days_blogs) sample. Deploying this chat AI solution involves [**three steps**](https://learn.microsoft.com/azure/ai-studio/concepts/deployments-overview?ocid=buildia24_60days_blogs).\\r\\n 1. Deploy the Models\\r\\n 1. Deploy the Flows\\r\\n 1. Deploy the Web App\\r\\n\\r\\nLet\'s look at the first two in this section, starting with **[model deployment](https://learn.microsoft.com/azure/ai-studio/concepts/deployments-overview?ocid=buildia24_60days_blogs#deploying-models)**. Azure AI Studio has a rich model catalog from providers including OpenAI, HuggingFace, Meta and Microsoft Research. Some models can be deployed _as a service_ (with a pay-as-you-go subscription) while others require _hosted, managed infra_ (with a standard Azure subscription). Our chat AI uses three models, all of which used the hosted, managed option.\\r\\n  - `gpt-35-turbo` - for chat completion (core function)\\r\\n  - `text-embedding-ada-002` - for embeddings (query vectorization)\\r\\n  - `gpt-4` - for chat evaluation (responsible AI)\\r\\n\\r\\nNext, let\'s talk about **[deploying flows](https://learn.microsoft.com/azure/ai-studio/how-to/flow-deploy?tabs=azure-studio?ocid=buildia24_60days_blogs)**. There are two kinds of flows we\'ll use in our chat AI - _completion flows_ (that we\'ll use for real-time inference) and _evaluation flows_ (that we\'ll use for quality assessment). Azure AI Studio provides [low-code deployment](https://learn.microsoft.com/azure/ai-studio/how-to/flow-deploy?tabs=azure-studio?ocid=buildia24_60days_blogs#create-an-online-deployment) via the UI and [code-first deployment](https://learn.microsoft.com/azure/ai-studio/how-to/flow-deploy?tabs=python?ocid=buildia24_60days_blogs#create-an-online-deployment) using the Azure AI SDK. In our Contoso Chat sample, we use the SDK to [upload the flow](https://github.com/Azure-Samples/contoso-chat/blob/main/deployment/push_and_deploy_pf.ipynb) to Azure, then deploy it using the UI as shown.\\r\\n  ![Deploy Contoso Chat](../../static/img/60-days-of-ia/blogs/2024-03-15/contoso-chat-deploy.png)\\r\\n\\r\\nFinally, let\'s talk about **[deploying web apps](https://learn.microsoft.com/azure/ai-studio/concepts/deployments-overview?ocid=buildia24_60days_blogs#deploying-web-apps)**. Here, the web app is a _chat UI_ that can invoke requests on the deployed chat AI and validate the functionality in production. There are three options to consider:\\r\\n1. **Built-in Testing UI**. When you deploy your flow via Azure AI Studio, you can visit the deployment details page and navigate to the _Test_ tab, to get a built-in testing sandbox as shown. This provides a quick way to test prompts with each new iteration, in a manual (interactive) way.\\r\\n  ![Deployment Testing](../../static/img/60-days-of-ia/blogs/2024-03-15/contoso-chat-test.png)\\r\\n1. **Deploy as Web App**. Azure AI Studio also provides a _Playground_ where you can deploy models directly (for chat completion) and _add your data (preview)_ (for grounding responses) using Azure AI Search and Blob Storage resources, to customize that chat experience. Then _deploy a new web app_ directly from that interface, to an Azure App Service resource.\\r\\n  ![Deploy as web app](https://learn.microsoft.com/azure/ai-studio/media/tutorials/chat-web-app/deploy-web-app.png?ocid=buildia24_60days_blogs)\\r\\n3. **Dedicated Web App**. This is the option we\'ll explore in the next section.\\r\\n\\r\\n## 3. Deploy your chat UI app\\r\\n\\r\\nThe Contoso Chat sample comes with a dedicated [Contoso Web](https://github.com/Azure-Samples/contoso-web) application that is implemented using the Next.js framework with support for static site generation. This provides a rich \\"Contoso Outdoors\\" website experience for users as shown below.\\r\\n\\r\\n![Contoso Web](../../static/img/60-days-of-ia/blogs/2024-03-15/app-contoso-chat-concept.png)\\r\\n\\r\\nTo use that application, simply [setup the endpoint variables](https://github.com/Azure-Samples/contoso-web?tab=readme-ov-file#setting-up-endpoints) for Contoso Chat and deploy the app to Azure App Service. Alternatively, you can use [this fork of the application](https://github.com/nitya/contoso-web/tree/main-codespaces-swa?tab=readme-ov-file) to explore a version that can be run in GitHub Codespaces (for development) and deployed to Azure Static Web Apps (for production) using GitHub Actions for automated deploys. Once deployed, you can click the _chat_ icon onscreen *bottom right) to see the chat dialog as shown in the screenshot above, and interact with the deployed Contoso chat AI.\\r\\n\\r\\n\\r\\n## 4. Automate your chat AI deployment\\r\\n\\r\\nThe [Contoso Chat sample](https://github.com/Azure-Samples/contoso-chat) is a **constantly-evolving** application sample that is updated regularly to reflect both the changes to Azure AI Studio (preview) and showcase new capabilities for end-to-end development workflows. You can currently explore two additional capabilities implemented in the codebase, to streamline your deployment process further.\\r\\n 1. **Using GitHub Actions**. The sample has instructions to [Deploy with GitHub Actions](https://github.com/Azure-Samples/contoso-chat?tab=readme-ov-file#9-deploy-with-github-actions) instead of the manual Azure AI Studio based deployment step we showed earlier. By setting up the actions workflow, you can automated deployments on every commit or PR, and get a baseline CI/CD pipeline for your chat AI, to build on later.\\r\\n 1. **Using Azure Developer CLI**. The sample was [just azd-enabled](https://github.com/Azure-Samples/contoso-chat/pull/74) recently, making it possible to use the [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/azd-templates?tabs=csharp?ocid=buildia24_60days_blogs) as a unified tool to accelerate the end-to-end process from _provisioning_ the resources to _deploying_ the solution. The [azd template](https://github.com/Azure-Samples/contoso-chat/blob/main/azure.yaml) adds support for _infrastructure-as-code_, allowing your application to have a consistent and repeatable deployment blueprint for all users. You can also [browse the azd template gallery](https://azure.github.io/awesome-azd/?tags=ai&tags=chatgpt) for other _ChatGPT_ style application examples.\\r\\n\\r\\nNote that the Contoso Chat sample is a _demo application_ sample that is designed to showcase the capabilities of Azure AI Studio and Azure AI services. It is not a production-ready application, and should be used primarily as a learning tool and starting point for your own development. \\r\\n\\r\\n---\\r\\n\\r\\n## 5. Enterprise Architecture Options\\r\\n\\r\\nThe objective of this series was to familiarize you with the Azure AI Studio (preview) platform and the capabilities it provides for building generative AI applications. And to give you a sense of how to build, run, test and deploy, your chat AI application for real-world use. But the platform is still in preview (and evolving rapidly). So what are your options if you want to build and deploy generative AI solutions at enterprise scale **today**? How can you design it using a well-architected cloud framework with **cloud-native technologies** like [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?ocid=buildia24_60days_blogs) or [Azure Kubernetes Service](https://learn.microsoft.com/azure/aks/intro-kubernetes?ocid=buildia24_60days_blogs)? \\r\\n\\r\\nHere are some open-source samples and guidance you can explore to start with:\\r\\n1. [ChatGPT + Enterprise data with Azure Open AI and AI Search (Python)](https://github.com/Azure-Samples/azure-search-openai-demo/) - open-source sample that uses Azure App Service, Azure Open AI, Azure AI Search and Azure Blob Storage, for an enterprise-grade solution grounded in your (documents) data.\\r\\n1. [ChatGPT + Enterprise data with Azure Open AI and AI Search (.NET)](https://github.com/Azure-Samples/azure-search-openai-demo-csharp) - open-source sample chat AI for a fictitious company called \\"Contoso Electronics\\" using the application architecture shown below. [This blog post](https://devblogs.microsoft.com/dotnet/transform-business-smart-dotnet-apps-azure-chatgpt/?ocid=buildia24_60days_blogs) provides more details.\\r\\n  ![Chat GPT Enterprise](../../static/img/60-days-of-ia/blogs/2024-03-15/chatgpt-enterprise.png)\\r\\n1. [Chat with your data Solution Accelerator](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator) - uses Azure App Service, Azure Open AI, Azure AI Search and Azure Blob Storage, for an end-to-end baseline RAG sample that goes beyond the [Azure OpenAI Service On Your Data](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/on-your-data-is-now-generally-available-in-azure-openai-service/ba-p/4059514?ocid=buildia24_60days_blogs) feature (GA in Feb 2024).\'\'\\r\\n1. [Built a private ChatGPT style app with enterprise-ready architecture](https://techcommunity.microsoft.com/t5/microsoft-mechanics-blog/build-your-own-private-chatgpt-style-app-with-enterprise-ready/ba-p/4069529?ocid=buildia24_60days_blogs) - a blog post from the Microsoft Mechanics team that uses an [open-source chat UI sample](https://aka.ms/GitHubChatBotUI?ocid=buildia24_60days_blogs) and discusses how to [enhance the chat experience with Azure AI Studio](https://www.youtube.com/watch?v=IKcuod-JFYU&t=252s) and streamline setup by [using Azure Landing Zones](https://www.youtube.com/watch?v=IKcuod-JFYU&t=404s).\\r\\n\\r\\nWe covered a lot today - but there\'s one last thing we should talk about before we wrap up. **Responsible AI**.\\r\\n\\r\\n---\\r\\n\\r\\n## 6. Responsible AI In Practice\\r\\n\\r\\n### 6.1 Principles of Responsible AI\\r\\n\\r\\nBy [one definition](https://learn.microsoft.com/azure/machine-learning/concept-responsible-ai?view=azureml-api-2?ocid=buildia24_60days_blogs), Responsible AI is _approach to developing, assessing, and deploying AI systems in a safe, trustworthy, and ethical way_. The [Responsible AI standard](https://www.microsoft.com/ai/principles-and-approach?ocid=buildia24_60days_blogs) was developed by Microsoft as a framework for building AI systems, using 6 principles to guide our design thinking.\\r\\n\\r\\n![Responsible AI Standard](https://learn.microsoft.com/en-us/azure/machine-learning/media/concept-responsible-ai/concept-responsible-ml.png?view=azureml-api-2?ocid=buildia24_60days_blogs)\\r\\n\\r\\n|Principle|Description|\\r\\n|---|---|\\r\\n|Fairness|How might an AI system allocate opportunities, resources, or information in ways that are fair to the humans who use it?|\\r\\n|Reliability & Safety|How might the system function well for people across different use conditions and contexts, including ones it was not originally intended for?|\\r\\n|Privacy & Security|How might the system be designed to support privacy and security?.|\\r\\n|Inclusiveness|How might the system be designed to be inclusive of people of all abilities?|\\r\\n|Transparency|How might people misunderstand, misuse, or incorrectly estimate the capabilities of the system?|\\r\\n|Accountability|How can we create oversight so that humans can be accountable and in control?|\\r\\n\\r\\n### 6.2 Implications for Generative AI\\r\\n\\r\\nThe [Fundamentals of Responsible Generative AI](https://learn.microsoft.com/en-us/training/modules/responsible-generative-ai/?ocid=buildia24_60days_blogs) describes core guidelines for building  generative AI solutions _responsibly_ as a 4-step process:\\r\\n1. **Identify** potential harms relevant to your solution.\\r\\n1. **Measure** presence of these harms in outputs generated by your solution.\\r\\n1. **Mitigate** harms at multiple layers to minimize impact, and ensure transparent communication about potential risks to users.\\r\\n1. **Operate** your solution responsibly by defining and following a deployment and operational readiness plan.\\r\\n\\r\\n### 6.3 Identify Potential Harms\\r\\n\\r\\nThe first step of the process is to identify potential harms in your application domain using a 4-step process:\\r\\n 1. Identify potential harms (offensive, unethical, fabrication) that may occur in generated content.\\r\\n 1. Assess likelihood of each occurrence, and severity of impact.\\r\\n 1. Test and verify if harms occur, and under what conditions.\\r\\n 1. Document and communicate potential harms to stakeholders.\\r\\n\\r\\n![4 steps](https://learn.microsoft.com/en-us/training/wwl-data-ai/responsible-generative-ai/media/identify-harms.png)\\r\\n\\r\\n\\r\\n### 6.4 Measure Presence of Harms\\r\\n\\r\\n[Evaluation of generative AI applications](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-approach-gen-ai?ocid=buildia24_60days_blogs) is the process of measuring the presence of identified harms in the generated output. Think of it as a 3-step process:\\r\\n\\r\\n 1. Prepare a diverse selection of input prompts that may result in the potential harms documented.\\r\\n 1. Submit prompts to your AI application and retrieve generated output\\r\\n 1. **Evaluate** those responses using pre-defined criteria.\\r\\n\\r\\nAzure AI Studio provides many features and pathways to support evaluation. Start with _manual evaluation_ (small set of inputs, interactive) to ensure coverage and consistency. Then scale to _automated evaluation_ (larger set of inputs, flows) for increased coverage and operationalization.\\r\\n\\r\\n![Evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/media/evaluations/evaluation-monitor-flow.png)\\r\\n\\r\\nBut what _metrics_ can we use to quantify the quality of generated output? Quantifying accuracy is now complicated _because we don\'t have access to a ground truth or deterministic answer_ that can serve as a baseline. Instead, we can use [AI-assisted metrics](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in?ocid=buildia24_60days_blogs) - where we _instruct_ another LLM to score your generated output **for quality and safety** using the guidelines and criteria you provide.\\r\\n- **Quality** is measured using metrics like _relevance, coherence and fluency_.\\r\\n- **Safety** is measured using metrics like _groundedness and content harms_.\\r\\n\\r\\nIn our _Contoso Chat_ app sample, we [show examples](https://github.com/Azure-Samples/contoso-chat/blob/main/eval/evaluate-chat-prompt-flow.ipynb) of local evaluation (with single and multiple metrics) and batch runs (for automated evaluation in the cloud). Here\'s an exmaple of what the output from the local evaluation looks like:\\r\\n\\r\\n![Local Eval](../../static/img/60-days-of-ia/blogs/2024-03-15/eval-local.png)\\r\\n\\r\\n\\r\\n### 6.5 Content Safety for Mitigation\\r\\n\\r\\nOne of the most effective ways to mitigate harmful responses from generative AI models in Azure OpenAI is to use [Content Filtering](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/content-filtering?ocid=buildia24_60days_blogs) powered by the [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview?ocid=buildia24_60days_blogs) service. The service works by running the user input (prompt) and the generated output (completion) through _an ensemble of classification models_ that are trained to detect, and act on, identified caegories of harmful content. \\r\\n\\r\\nAzure AI Studio provides a default content safety filter, and allows you to create custom content filters with more tailored configurations if you opt-in to that capability first. These filters can then be _applied_ to a model or app deployment to ensure that inputs and outputs are gated to meet your content safety requirements.\\r\\n\\r\\n![Create Filter](https://learn.microsoft.com/en-us/azure/ai-studio/media/content-safety/content-filter/configure-threshold.png#lightbox)\\r\\n\\r\\nThe screenshot shows the [different content filtering categories](https://learn.microsoft.com/azure/ai-studio/concepts/content-filtering?ocid=buildia24_60days_blogs#content-filtering-categories-and-configurability) and the level of configurability each provides. This allows us to identify and mitigate different categories of issues (Violence, Hate, Sexual and Self-harm) by **automatically detecting** these in both user prompts (input) and model completions (output). An additional filter (optional) lets you enable filters for more advanced usage scenarios including _jailbreaks, protected content or code_ as [described here](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/content-filtering?ocid=buildia24_60days_blogs#more-filters-for-generative-ai-scenarios).\\r\\n\\r\\n![Content Filter](https://learn.microsoft.com/en-us/azure/ai-studio/media/content-safety/content-filter/additional-models.png)\\r\\n\\r\\nOnce the filters are applied, the deployment can be opened up in the Playground, or using an integrated web app, to validate that the filters work. Check out [this #MSIgnite session](https://ignite.microsoft.com/en-US/sessions/5db0e51a-d8b1-4234-b149-31671a633ffc?source=sessions?ocid=buildia24_60days_blogs) from the Responsible AI team for strategies and examples for responsible AI practices with prompt engineering and retrieval augmented generation patterns in context.\\r\\n\\r\\n## 7. Exercise: \\r\\n\\r\\nWe covered a lot today - and that also brings us to the end of our journey into Azure AI in this series. Want to get hands-on experience with some of these concepts? Here are some suggestions:\\r\\n\\r\\n1. Walk through the [Contoso Chat](https://aka.ms/aitour/contoso-chat?ocid=buildia24_60days_blogs) sample end-to-end, and get familiar with the Azure AI Studio platform and the LLM Ops workflow for generative AI solutions.\\r\\n1. Explore the [Responsible AI Developer Hub](https://aka.ms/rai-hub/website?ocid=buildia24_60days_blogs) and try out the Content Safety and Prompt flow Evaluation workshops to get familiar with the Responsible AI principles and practices for generative AI.\\r\\n\\r\\n## 8. Resources\\r\\n\\r\\nWe covered a lot this week!! But your learning journey with Generative AI development and Azure AI is just beginning. Want to keep going? Here are three resources to help you:\\r\\n\\r\\n1. [Azure AI Studio for Developers](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs)\\r\\n1. [Responsible AI For Developers](https://aka.ms/rai-hub/collection?ocid=buildia24_60days_blogs)\\r\\n1. [Contoso Chat Sample](https://aka.ms/aitour/contoso-chat?ocid=buildia24_60days_blogs)"},{"id":"build-a-copilot-on-azure-code-first-with-langchain","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-langchain","source":"@site/blog-60daysofIA/2024-03-14/build-a-copilot-on-azure-code-first-with-prompt-flow.md","title":"4.4 Build a Copilot on Azure Code-First with Langchain","description":"This project uses the AI Search service to create a vector store for a custom department store data.  To enable the user to ask questions our data in a conversational format, we\'ll using Langchain to connect our prompt template with our Azure Open AI LLM.","date":"2024-03-14T09:00:00.000Z","formattedDate":"March 14, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.445,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-14T09:00","slug":"build-a-copilot-on-azure-code-first-with-langchain","title":"4.4 Build a Copilot on Azure Code-First with Langchain","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["AzureAlStudio","copilot","RAG","Langchain"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"This project uses the AI Search service to create a vector store for a custom department store data.  To enable the user to ask questions our data in a conversational format, we\'ll using Langchain to connect our prompt template with our Azure Open AI LLM.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4.5 Deploying Your Copilot On Azure","permalink":"/Cloud-Native/60DaysOfIA/deploying-your-copilot-on-azure"},"nextItem":{"title":"4.3 Build a Copilot on Azure Code-First with Prompt Flow","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-prompt-flow"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/build-a-copilot-on-azure-code-first-with-langchain\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"This project uses the AI Search service to create a vector store for a custom department store data.  To enable the user to ask questions our data in a conversational format, we\'ll using Langchain to connect our prompt template with our Azure Open AI LLM.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-on-azure-code-first-with-langchain\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"This project uses the AI Search service to create a vector store for a custom department store data.  To enable the user to ask questions our data in a conversational format, we\'ll using Langchain to connect our prompt template with our Azure Open AI LLM.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-on-azure-code-first-with-langchain\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n\\r\\n**Welcome to Day 4\ufe0f\u20e3 of the Azure AI week on #60Days Of IA**\\r\\nIn the previous post, we learned about how to get started with the Azure AI SDK and Prompt Flow to build a Copilot. In today\'s post we\'ll be covering `building a copilot with custom code and data using Langchain`.\\r\\n\\r\\n\\r\\n\\r\\n## What You\'ll Learn Today\\r\\n * Quickstart Sample: Using Langchain to build a copilot.\\r\\n * What is \\"Langchain\\" ? \\r\\n * Build the Copilot\\r\\n * Evaluate the Copilot\\r\\n * Deploy the Copilot\\r\\n * **Challenge**: [Try this quickstart sample](https://github.com/Azure-Samples/aistudio-python-langchain-sample/tree/main)\\r\\n * **Resources**: To learn more\\r\\n    - [Azure AI Studio](https://aka.ms/azureaistudio?ocid=buildia24_60days_blogs) - UI to explore, build & manage AI solutions.\\r\\n    - [Azure AI Studio Docs](https://learn.microsoft.com/azure/ai-studio?ocid=buildia24_60days_blogs) - Azure AI Studio documentation.\\r\\n    - [Azure AI Services](https://learn.microsoft.com/azure/ai-services/what-are-ai-services?ocid=buildia24_60days_blogs) - Azure AI Services documentation.\\r\\n    - [Training: Using vector search in Azure Cognitive Search](https://learn.microsoft.com/training/modules/improve-search-results-vector-search?ocid=buildia24_60days_blogs) \\r\\n    - [Tutorial: Deploy a web app for chat on your data](https://learn.microsoft.com/azure/ai-studio/tutorials/deploy-chat-web-app?ocid=buildia24_60days_blogs) \\r\\n\\r\\n<br/>\\r\\n\\r\\n\x3c!-- FIXME: banner image --\x3e\\r\\n![Build a Copilot on Azure Code-First with Langchain](../../static/img/60-days-of-ia/blogs/2024-03-14/BIA-4.png)\\r\\n\\r\\n---\\r\\n\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\x3c!--  AUTHORS: WRITE BLOG POST CONTENT HERE --\x3e\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\\r\\n## 1 | Introduction \\r\\n\\r\\nThis project use the AI Search service to create a vector store for a custom department store data.  We will be using Azure Open AI\'s text-embedding-ada-002 deployment for embedding the data in vectors. The vector representation of your data is stored in [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search?ocid=buildia24_60days_blogs) (formerly known as \\"Azure Cognitive Search\\").  \\r\\n\\r\\nTo enable the user to ask questions our data in a conversational format, we\'ll using Langchain to connect our prompt template with our Azure Open AI LLM.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/rag-pattern.png)\\r\\n\\r\\nWe\'ll use Retrieval Augmented Generation (RAG), a pattern used in AI which uses an LLM to generate answers with your own data. In addition, we\'ll  construct prompt template to provide the scope of our dataset, as well as the context to the submit questions. Lastly, we\'ll maintain the state of the conversation by store the chat history in the prompt.\\r\\n\\r\\n**Custom Data:** The sample data that we\'ll be using in this project is a department store dataset.  The dataset contains a list of customers, orders, products and their descriptions, and their prices.  We\'ll be using this dataset to create a copilot that can answer questions about the products in the dataset.\\r\\n\\r\\n## What is Langchain?\\r\\n\\r\\nLangchain is a framework for developing applications powered by language models. It enables you to connect a language model such as Azure OpenAI to a prompt template including: prompt instructions, chat history, context of the chat conversation, few shot examples, content to ground its response in, etc.).  This helps facilitate end-users to interact with the application to ask questions and language models to generate responses in a conversational format.\\r\\n\\r\\nIn this exercise, we\'ll be using ConversationalRetrievalChain, which is a subclass of langchain that handles chats that are based on retrieving data from documents or vector datasources. We will use it to connect the Azure OpenAI model, retriever, prompt template and chat memory in order to search the AI Search database to retrieve the most relevant response. To activate the instance you need an LLM model (ex. gpt-35-turbo) to retrieve response, the prompt template rules, and chat history. \\r\\n\\r\\n## 2 | Pre-Requisites\\r\\n\\r\\nCompleting the [tutorial](https://github.com/Azure-Samples/aistudio-python-langchain-sample/tree/main) requires the following:\\r\\n\\r\\n1. An Azure subscription - [Create one for free](https://azure.microsoft.com/free/cognitive-services?ocid=buildia24_60days_blogs)\\r\\n2. Access to Azure OpenAI in the Azure Subscription - [Request access here](https://aka.ms/oai/access?ocid=buildia24_60days_blogs)\\r\\n3. Custom data to ground the copilot - [Sample product-info data is provided](https://github.com/Azure-Samples/aistudio-python-langchain-sample/tree/main/data/3-product-info)\\r\\n4. A GitHub account - [Create one for free](https://github.com/signup)\\r\\n5. Access to GitHub Codespaces - [Free quota should be sufficient](https://docs.github.com/en/billing/managing-billing-for-github-codespaces/about-billing-for-github-codespaces#monthly-included-storage-and-core-hours-for-personal-accounts)\\r\\n\\r\\nThe tutorial uses Azure AI Studio which is currently in public preview.\\r\\n\\r\\n - Read [the documentation](https://learn.microsoft.com/azure/ai-studio/reference/region-support#azure-public-regions?ocid=buildia24_60days_blogs) to learn about regional availability of Azure AI Studio (preview)\\r\\n - Read the [Azure AI Studio FAQ](https://learn.microsoft.com/azure/ai-studio/faq?ocid=buildia24_60days_blogs) for answers to some commonly-asked questions.\\r\\n\\r\\n\\r\\n## Open the copilot with Jupiter notebook\\r\\n\\r\\nWe\'ll be using Python SDK to create our copilot for the Contoso outdoor/camping gear AI Chat application.\\r\\n\\r\\nLet\'s begin by opening the `copilot_langchain.ipynb` notebook in the visual studio code (VS code) editor.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/3-select-kernel.png)\\r\\n\\r\\nIn VS code, click on **Select Kernel**. Then under Python Environments, select the **Python 3.10.13** environment you just created\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/3-python-env.png)\\r\\n\\r\\n\\r\\n## Connect to azure resources\\r\\n\\r\\nIn order to access the resources you created in your project in AI studio, we\'ll use the python SDK to authenticate and connect to them.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/conn-to-azure.png)\\r\\n\\r\\n\\r\\nTo find the most relevant results from the vector database, we\'ll be using Langchain\'s retriever to search content from Azure AI Search. \\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/langchain-retriever.png)\\r\\n\\r\\n\\r\\n## Create Prompt Template\\r\\n\\r\\nPrompt engineering is an integral part of providing good user experience and relevant answers.  To achieve that you\'ll need to define a prompt template that includes system prompt rules, restrictions, chat history, input questions and context of conversation.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/prompt-template.png)\\r\\n\\r\\n\\r\\n## Add Langchain to connect Azure OpenAI and Prompt template\\r\\n\\r\\nTo process the search results and apply the system rules, you need it initialize the LLM.  In our case, we\'ll using AzureChatOpenAI class to specify the GPT-35-Turbo model deployment and settings we need for the chat.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/init-azure-openai.png)\\r\\n\\r\\nAll the dialogue that the end-user has with the chat needs retained to maintain the context of the conversation.  That\'s why we are using the ConversationMemoryBuffer class to store the chat history.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/store-convo.png)\\r\\n\\r\\nTo search the AI search database, we\'ll use a subclass of langchain to connect the Azure OpenAI, datasource retriever, prompt template and memory together.  When an instance of the langchain is invoke with an user input prompt, the retriever is used to search your data in AI Search.  The Azure OpenAI uses the prompt rules to process the response back to the user.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/config-langchain.png)\\r\\n\\r\\n\\r\\n## Run the copilot with Jupiter notebook\\r\\n\\r\\nTo run a single question & answer through the sample copilot:\\r\\n\\r\\nClick on **Run All** to run the notebook.\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/4-run-all.png)\\r\\n\\r\\n\\r\\n## Validate your copilot by asking a question about your custom data.\\r\\n\\r\\nEnter a question about the outdoor/camping gear and clothing products. For example:\\r\\n\\r\\n```shell\\r\\nWhich of your sleeping bags are polyester?\\r\\n```\\r\\n\\r\\n![](../../static/img/60-days-of-ia/blogs/2024-03-14/5-question.png)\\r\\n\\r\\n\\r\\n`The CozyNights Sleeping Bag (item_number: 7) and the MountainDream Sleeping Bag (item_number: 14) are both made of polyester.`\\r\\n\\r\\nTry asking another question. For example:\\r\\n\\r\\n```shell\\r\\nwhich tent is the most waterproof?\\r\\n```\\r\\n\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nIn this exercise you learned how to use Azure AI Search to create and load your data into a vector store.  Next, you learned how to use prompt engineering by constructing *System Prompt* with instructions on how to engage with the user, the scope of the subject area to enforce grounding which prevents the LLM from providing responses that are not relevent to your data.  You should now be able to build AI applications using Lanchain to connect Azure OpenAI, your prompts, chat history, context and retriever of your data source.  You\'ve now gained the knowledge on how to use the Retrieval Augmented Generation (RAG) pattern in AI which uses LLMs to generate answers with your own data."},{"id":"build-a-copilot-on-azure-code-first-with-prompt-flow","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-prompt-flow","source":"@site/blog-60daysofIA/2024-03-13/build-a-copilot-on-azure-code-first-with-prompt-flow.md","title":"4.3 Build a Copilot on Azure Code-First with Prompt Flow","description":"This blog walks you through how you can build, evaluate, and test a custom copilot implementation using Prompt Flow and Azure AI SDK.","date":"2024-03-13T09:00:00.000Z","formattedDate":"March 13, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.105,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-13T09:00","slug":"build-a-copilot-on-azure-code-first-with-prompt-flow","title":"4.3 Build a Copilot on Azure Code-First with Prompt Flow","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["promptflow","azure ai studio","ai cli","RAG"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"This blog walks you through how you can build, evaluate, and test a custom copilot implementation using Prompt Flow and Azure AI SDK.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4.4 Build a Copilot on Azure Code-First with Langchain","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-langchain"},"nextItem":{"title":"4.2 Build A Copilot Code-First with the Azure AI Python SDK","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/build-a-copilot-on-azure-code-first-with-prompt-flow\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"This blog walks you through how you can build, evaluate, and test a custom copilot implementation using Prompt Flow and Azure AI SDK.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-on-azure-code-first-with-prompt-flow\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"This blog walks you through how you can build, evaluate, and test a custom copilot implementation using Prompt Flow and Azure AI SDK.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-on-azure-code-first-with-prompt-flow\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n\\r\\n**Welcome to Day 3\ufe0f\u20e3 of the Azure AI week on #60Days Of IA**\\r\\nIn the previous post, we learned about how to get started with the Azure AI SDK and using it to build a Copilot. In today\'s post we\'ll be covering `building a copilot with custom code and data using PromptFlow`.\\r\\n\\r\\n\\r\\n## What You\'ll Learn Today\\r\\n * Quickstart Sample: Using PromptFlow to build a copilot.\\r\\n * What is \\"Prompt Flow\\" ? \\r\\n * Build the Copilot\\r\\n * Evaluate and Test your flow\\r\\n * Deploy the Copilot\\r\\n * Challenge: [Try this Quickstart sample](https://github.com/Azure-Samples/aistudio-python-promptflow-sample)\\r\\n * Resources: [To learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/prompt-flow?ocid=buildia24_60days_blog)\\r\\n\\r\\n<br/>\\r\\n\\r\\n\x3c!-- FIXME: banner image --\x3e\\r\\n![Build a Copilot on Azure Code-First with Promptflow](../../static/img/60-days-of-ia/blogs/2024-03-13/BIA-3.png)\\r\\n\\r\\n---\\r\\n\\r\\n---\\r\\n\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\x3c!--  AUTHORS: WRITE BLOG POST CONTENT HERE --\x3e\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\\r\\n## 1 | Learning Objectives\\r\\n\\r\\nThis [quickstart tutorial](https://github.com/Azure-Samples/aistudio-python-promptflow-sample) walks you through the steps of creating a copilot app for the enterprise using custom Python code and Prompt Flow to ground the copilot responses in your company data and APIs. The sample is meant to provide a starting point that you can further customize to add additional intelligence or capabilities. By the end of this tutorial, you should be able to\\r\\n1. Describe Prompt Flow and its components\\r\\n1. Build a copilot code-first, using Python and Prompt Flow\\r\\n1. Run the copilot locally, and test it with a question\\r\\n1. Evaluate the copilot locally, and understand metrics\\r\\n1. Deploy the copilot to Azure, and get an endpoint for integrations\\r\\n\\r\\nOnce you\'ve completed the tutorial, try to customize it further for your application requirements, or to explore other platform capabilities. **This is not a production sample** so make sure you validate responses and evaluate the suitability of this sample for use in your application context.\\r\\n\\r\\n## What is Prompt Flow? \\r\\n\\r\\nPrompt Flow is a tool that simplifies the process of building a fully-fledged AI Solution. It helps you prototype, experiment, iterate, test and deploy your AI Applications. Some of the tasks you can achieve with promptflow include:\\r\\n\\r\\n* Create executable flows linking LLM prompts and Python tools through a graph\\r\\n* Debug, share, and iterate through your flows with ease\\r\\n* Create prompt variants and evaluate their performance through testing\\r\\n* Deploy a real-time endpoint that unlocks the power of LLMs for your application.\\r\\n\\r\\n## 2 | Pre-Requisites\\r\\n\\r\\nCompleting the [tutorial](https://github.com/Azure-Samples/aistudio-python-promptflow-sample) requires the following:\\r\\n\\r\\n1. An Azure subscription - [Create one for free](https://azure.microsoft.com/free/cognitive-services?ocid=buildia24_60days_blog)\\r\\n2. Access to Azure OpenAI in the Azure Subscription - [Request access here](https://aka.ms/oai/access?ocid=buildia24_60days_blog)\\r\\n3. Custom data to ground the copilot - [Sample product-info data is provided](https://github.com/Azure-Samples/aistudio-python-promptflow-sample/tree/main/data/3-product-info)\\r\\n4. A GitHub account - [Create one for free](https://github.com/signup)\\r\\n5. Access to GitHub Codespaces - [Free quota should be sufficient](https://docs.github.com/en/billing/managing-billing-for-github-codespaces/about-billing-for-github-codespaces#monthly-included-storage-and-core-hours-for-personal-accounts)\\r\\n\\r\\nThe tutorial uses Azure AI Studio which is currently in public preview.\\r\\n\\r\\n - Read [the documentation](https://learn.microsoft.com/azure/ai-studio/reference/region-support#azure-public-regions?ocid=buildia24_60days_blog) to learn about regional availability of Azure AI Studio (preview)\\r\\n - Read the [Azure AI Studio FAQ](https://learn.microsoft.com/azure/ai-studio/faq?ocid=buildia24_60days_blog?ocid=buildia24_60days_blog) for answers to some commonly-asked questions.\\r\\n\\r\\n## Components of our Prompt Flow\\r\\n\\r\\n* **Flows:** LLM apps essentially involve a series of calls to external services. For instance, our application connects to AI Search, Embeddings Model, and GPT-35-turbo LLM. A flow in PromptFlow are merely...... There are two types of flows:\\r\\n    * **Standard flow:** This is a flow for you to develop you LLM application.\\r\\n    * **Chat flow:** This is similiar to standard flow but the difference is you can define the `chat_history`, `chat_input` and `chat_output` for our flow, enhancing the flow for conversations. \\r\\n    * **Evaluation Flow:** this flow allows you to test and evaluate the quality of your LLM application. It runs on the output of yout flow and computes metrics that can be used to determine whether the flow performs well.\\r\\n\\r\\n![example of our chat flow](../../static/img/60-days-of-ia/blogs/2024-03-13/flow.png)\\r\\n\\r\\n> The flow is defined in `src/copilot_proptflow/flow.dag.yaml`, where you will find all the inputs, nodes and outputs. \\r\\n\\r\\n* **Tools:** these are the fundamental building blocks (nodes) of a flow. The three basic tools are:\\r\\n    * **LLM:** allows you to customize your prompts and leverage LLMs to achieve specific goals\\r\\n    * **Python:** enables you to write custom Python functions to perform various tasks\\r\\n    * **Prompt:** allows you to prepare a prompt as a string for more complex use cases.\\r\\n\\r\\n![LLM Response .jinja2](../../static/img/60-days-of-ia/blogs/2024-03-13/llm_reponse_jinja2.png)\\r\\n\\r\\n*LLM Response .jinja2 file*\\r\\n\\r\\n![customer lookup .py](../../static/img/60-days-of-ia/blogs/2024-03-13/customer_lookup_py.png)\\r\\n\\r\\n*Customer lookup .py file*\\r\\n\\r\\nThe `source code` in `.py` or `.jinja2` defines tools used by the flow.\\r\\n\\r\\n* **Connections** these are for storing information about how you can access external services such as LLM endpoints, API keys, databases, and custom connections e.g. Azure Cosomos DB. You can add your connection as follows using `Prompt flow: Create connection` command:\\r\\n![Prompt flow: Create connection command](../../static/img/60-days-of-ia/blogs/2024-03-13/create_connection.png)\\r\\n\\r\\nOnce created, you can then update your new connection in you flow:\\r\\n\\r\\n![adding new connection to your flow](../../static/img/60-days-of-ia/blogs/2024-03-13/connections_editor.png)\\r\\n\\r\\n* **Variants:** they are used to tune your prompts, for instance utilizing different variants for prompts to evaluate how your model responds to different inputs to get the most suitable combination for your application.\\r\\n* **Running our code:** You can run by clicking run on the visual editor.\\r\\n\\r\\n![screenshot showing the flow output](../../static/img/60-days-of-ia/blogs/2024-03-13/code_output.png)\\r\\n\\r\\n## Test and evaluate our PromptFlow\\r\\n\\r\\nOnce you have built your flow, you need to evaluate the quality of your LLM app response to see if it is performing up to expectations. Some of the metrics you can include in your evaluation are:\\r\\n\\r\\n* Groundedness: how well does the generated responses align with the source data?\\r\\n* Relevance: to what extent is the model\'s generated responses directly related to the questions/input?\\r\\n* Coherence: to what extent does the generated response sound natural, fluent and human like?\\r\\n* Fluency: how grammatically proficient is the output generated by the AI?\\r\\n\\r\\nDuring local evaluation you can explore one metric e.g. groundedness or multiple metrics to evaluate your application. We will evaluate Groundedness by using the evaluation flow as shown below:\\r\\n\\r\\n![Screenshot showing evaluation of the groundeness of our flow](../../static/img/60-days-of-ia/blogs/2024-03-13/groundedness_flow.png)\\r\\n\\r\\n## Exercise\\r\\n\\r\\nWe have covered the building blocks of PromptFlow and how you can ground your data and build your AI Application. Next, once you are satisfied with the performance of your model, you can go ahead and deploy your application. You can do this using either *Azure AI Studio* or *Azure AI Python SDK.*\\r\\n\\r\\n> \ud83d\ude80 **EXERCISE**\\r\\n>\\r\\n> Deploy the PromptFlow either using the Azure AI Studio UI or using the Azure AI SDK\\r\\n\\r\\n## Resources\\r\\n\\r\\n* [AI Studio Prompt Flow Quickstart Sample:](https://github.com/Azure-Samples/aistudio-python-promptflow-sample) Code-first approach to building, running, evaluating, and deploying, a **prompt flow based** Copilot application _using your own data_.\\r\\n* [AI Tour Workshop 4:](https://aka.ms/aitour/contoso-chat/workshop) Comprehensive step-by-step instructions for building the Contoso Chat production RAG app with Prompt Flow and Azure AI Studio\\r\\n* [Azure AI Studio - Documentation:](https://learn.microsoft.com/en-us/azure/ai-studio/?ocid=buildia24_60days_blog) Build cutting-edge, market-ready, responsible applications for your organization with AI"},{"id":"build-a-copilot-code-first-with-the-azure-ai-python-sdk","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk","source":"@site/blog-60daysofIA/2024-03-12/build-a-copilot-code-first-with-the-azure-ai-python-sdk.md","title":"4.2 Build A Copilot Code-First with the Azure AI Python SDK","description":"Have a generative AI application you want to build, but don\'t know where to start? In this blog post, we introduce the Azure AI Studio Python Quickstart Sample, explain the end-to-end development workflow, then show you how you can get started customizing it, to explore your own application requirements.","date":"2024-03-12T09:00:00.000Z","formattedDate":"March 12, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":11.26,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-12T09:00","slug":"build-a-copilot-code-first-with-the-azure-ai-python-sdk","title":"4.2 Build A Copilot Code-First with the Azure AI Python SDK","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["azure","azureai","copilot","aisdk"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"Have a generative AI application you want to build, but don\'t know where to start? In this blog post, we introduce the Azure AI Studio Python Quickstart Sample, explain the end-to-end development workflow, then show you how you can get started customizing it, to explore your own application requirements.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4.3 Build a Copilot on Azure Code-First with Prompt Flow","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-prompt-flow"},"nextItem":{"title":"4.1 Build Contoso Chat End-to-End","permalink":"/Cloud-Native/60DaysOfIA/build-contoso-chat-end-to-end"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/build-a-copilot-code-first-with-the-azure-ai-python-sdk\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Have a generative AI application you want to build, but don\'t know where to start? In this blog post, we introduce the Azure AI Studio Python Quickstart Sample, explain the end-to-end development workflow, then show you how you can get started customizing it, to explore your own application requirements.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"Have a generative AI application you want to build, but don\'t know where to start? In this blog post, we introduce the Azure AI Studio Python Quickstart Sample, explain the end-to-end development workflow, then show you how you can get started customizing it, to explore your own application requirements.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n**Welcome to Day 2\ufe0f\u20e3 of the Azure AI week on #60Days Of IA** \\r\\n\\r\\nLet\'s recap what we learned so far. In our _kickoff_ post we set the stage by describing our application scenario (Contoso Chat), the paradigm shift for generative AI apps (LLM Ops) and the unified platform for streamlining development (Azure AI Studio). In the next post we walked through the signature [Contoso Chat](https://aka.ms/aitour/contoso-chat) application sample to understand how we can implement that scenario using Azure AI Studio and Prompt flow - from building the chat function, to evaluating it, deploying it to a hosted endpoint, then testing that API in a chat client.\\r\\n\\r\\nBut what if you want to get started building your own application scenario? Over the next three posts, we\'ll look at _starter samples_ that will get you from ideation (define chat function) to operationalization (deploy chat API) using different tools and frameworks to simplify orchestration.\\r\\n\\r\\nReady? Let\'s go!\\r\\n\\r\\n## What You\'ll Learn Today\\r\\n * What is the copilot architecture?\\r\\n * What is the Azure AI SDK?\\r\\n * What is the Quickstart sample?\\r\\n * How can I customize and extend this for my scenario?\\r\\n * **Challenge:** Fork [this quickstart](https://github.com/Azure-Samples/aistudio-python-quickstart-sample) and build it, then extend it with your data.\\r\\n * **Resources:** Bookmark [this collection](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs) for training & documentation.\\r\\n\\r\\n<br/>\\r\\n\\r\\n\\r\\n![Build a Copilot on Azure Code-First with Azure AI SDK](../../static/img/60-days-of-ia/blogs/2024-03-12/banner.png)\\r\\n\\r\\n---\\r\\n\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\x3c!--  AUTHORS: WRITE BLOG POST CONTENT HERE --\x3e\\r\\n\x3c!-- ************************************** --\x3e\\r\\n\\r\\n\\r\\n## 1 | Learning Objectives\\r\\n\\r\\nThe [copilot ai-sdk quickstart](https://github.com/Azure-Samples/aistudio-python-quickstart-sample) is a Python-based starter sample for a code-first approach to building a copilot experience on the Azure AI platform. Since this is the foundational sample, we\'ll use it to explore some of the details of the implementation and set the stage for you to explore customizing it further for your application requirements.\\r\\n\\r\\nBy the end of this tutorial you should be able to:\\r\\n\\r\\n1. Explain the functional components of the copilot architecture\\r\\n1. Explain the Azure resources required to implement a copilot\\r\\n1. Explain the core functionality provided by the Azure AI SDK\\r\\n1. Build, run, evaluate, and deploy, a basic copilot with Azure AI Studio.\\r\\n1. Explore the Azure AI curated VS Code environment to customize the sample\\r\\n\\r\\nKeep in mind that this is a _quickstart sample_ and is **not meant for production use**. We encourage you to extend and customize the sample to understand the platform capabilities and end-to-end development workflow. Make sure to validate the responses yourself and evaluate its suitability for your application needs in context.\\r\\n\\r\\n## 2| Copilot Architecture\\r\\n\\r\\nLet\'s first revisit the high-level application architecture for our copilot and familiarize ourselves with the core functional components. Our goal is to **build the chat function** component and deploy it to get a hosted **Copilot API** endpoint that we can integrate into front-end applications to provide a conversational chatbot capability grounded in our data.\\r\\n![Copilot architecture](../../static/img/60-days-of-ia/blogs/2024-03-12/copilot-architecture.png)\\r\\n\\r\\nLet\'s review what we will need to implement this architecture:\\r\\n\\r\\n1. **Model Deployments** - we need deployed models for chat and embeddings.\\r\\n1. **Search Index** - we need a search index populated with our product data.\\r\\n1. **Azure Resources** - we need to setup and configure our Azure AI project.\\r\\n1. **App Evaluation** - we need to evaluate copilot quality for responsible AI.\\r\\n1. **App Deployment** - we need to deploy the copilot for a hosted API endpoint.\\r\\n\\r\\nThe [copilot ai-sdk quickstart](https://github.com/Azure-Samples/aistudio-python-quickstart-sample) provides a starter codebase that implements this chat function using the Retrieval Augmented Generation (RAG) pattern with custom data. The implementation makes use of Azure AI Studio and the [Azure AI SDK (Python)](https://aka.ms/aistudio/docs/sdk?ocid=buildia24_60days_blogs) for a code-first approach. Since these technologies are currently in preview, we expect the sample to keep evolving quickly and **recommend following the README-based tutorial there** for the latest instructions.\\r\\n\\r\\n## 3 | Azure AI SDK\\r\\n\\r\\nBefore we dive into the sample, let\'s take a moment to learn about the [Azure AI SDK for Python (preview)](https://learn.microsoft.com/python/api/overview/azure/ai?view=azure-python-preview?ocid=buildia24_60days_blogs). The SDK consists of two packages:\\r\\n - [azure-ai-generative](https://pypi.org/project/azure-ai-generative/) - which provides the functionality needed for building, evaluating and deploying Generative AI applications. This has extra packages (index, evaluate, promptflow) you can use for enhanced local development capabilities - or optionally, remove if unused.\\r\\n - [azure-ai-resources](https://pypi.org/project/azure-ai-resources/) - which provides the functionality for connecting to, and managing, your Azure AI projects and resources. Use this for control plane operations to create and manage data, indexes, models and deployments.\\r\\n\\r\\nThe generative package makes use of the resources package to [create an `AIClient` instance](https://learn.microsoft.com/azure/ai-studio/how-to/sdk-generative-overview#connecting-to-projects?ocid=buildia24_60days_blogs) that can be used for connecting to the Azure AI project resources.\\r\\n\\r\\n```python\\r\\nfrom azure.ai.resources.client import AIClient\\r\\nfrom azure.identity import DefaultAzureCredential\\r\\n\\r\\nai_client = AIClient(\\r\\n    credential=DefaultAzureCredential(),\\r\\n    subscription_id=\'subscription_id\',\\r\\n    resource_group_name=\'resource_group\',\\r\\n    project_name=\'project_name\'\\r\\n)\\r\\n```\\r\\n\\r\\nOnce connected, you can use the generative package to build an index, run a local evaluation, or deploy chat functions and prompt flows, using the imports shown:\\r\\n\\r\\n```python\\r\\nfrom azure.ai.generative.index import build_index\\r\\nfrom azure.ai.generative.evaluate import evaluate\\r\\nfrom azure.ai.resources.entities.deployment import Deployment\\r\\n```\\r\\n\\r\\nTo get started, you will need to [install the SDK](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/sdk-install?ocid=buildia24_60days_blogs) in your local development environment. When you use the quickstart sample with GitHub Codespaces or the Azure AI curated VS Code environment, the SDK comes pre-installed and ready to use. \\r\\n\\r\\n## 4 | Using the Quickstart Sample\\r\\n\\r\\nThe [copilot ai-sdk quickstart](https://github.com/Azure-Samples/aistudio-python-quickstart-sample) provides a comprehensive **README.md** document that describes the step-by-step process for building, running, evaluating, and deploying, a starter copilot sample.\\r\\n\\r\\n### 4.1 | Pre-Requisites\\r\\n\\r\\nTo get started, you will need an active Azure subscription and have access to the Azure OpenAI service to create and deploy the required models for chat completion, chat evaluation and embedddings. You will also need a GitHub account. \\r\\n\\r\\n### 4.2 | Setup Dev Environment\\r\\n\\r\\nThe fastest way to get started exploring the sample is to fork the repo to your personal profile, then launch GitHub Codespaces by navigating to the \\"Codespaces\\" tab under the \\"Code\\" dropdown and creating a new codespace. Active codespaces are listed as shown below. \\r\\n\\r\\n![Launch](../../static/img/60-days-of-ia/blogs/2024-03-12/01-launch-codespaces.png)\\r\\n\\r\\nOnce the Codespace is ready, you will see the Visual Studio Code editor view in your browser tab. Open the **README.md** in the editor, then follow the instructions to complete the tutorial.\\r\\n\\r\\n![Run](../../static/img/60-days-of-ia/blogs/2024-03-12/03-running-codespaces.png)\\r\\n\\r\\n### 4.3 | Initialize Azure AI Resources\\r\\n\\r\\nTo build the copilot, we need to provision the Azure resources listed below. \\r\\n - An [Azure AI hub resource](https://learn.microsoft.com/azure/ai-studio/concepts/ai-resources?ocid=buildia24_60days_blogs) to provide a working _team_ environment and manage resource access, billing and more.\\r\\n - An [Azure AI project resource](https://learn.microsoft.com/azure/ai-studio/how-to/create-projects?ocid=buildia24_60days_blogs) to organize the data, models, and deployments for _an application_, and save its state for future use.\\r\\n - An [Azure AI Search resource](https://learn.microsoft.com/en-us/azure/search/?ocid=buildia24_60days_blogs) to host the search index for our product data.\\r\\n - An [Azure OpenAI resource](https://learn.microsoft.com/azure/openai?ocid=buildia24_60days_blogs) to deploy the models for chat completion, chat evaluation and embeddings.\\r\\n\\r\\nFor now, we will be creating these resources from the [Azure AI Studio UI](https://ai.azure.com?ocid=buildia24_60days_blogs) and [Azure Portal](https://portal.azure.com?ocid=buildia24_60days_blogs) UI in the browser. However, we expect future support for a command-line (CLI) based approach for efficiency and automation. Refer to the sample README for the step-by-step guidance.\\r\\n\\r\\n### 4.4 | Initialize Azure Configuration\\r\\n\\r\\nOnce we\'ve created the Azure resources, we need to configure our Visual Studio Code environment to connect to the cloud.  The repo comes with a `config.sample.json` that shows you the properties that need to be configured. The easiest way to set these is to download the `config.json` file from your Azure AI project resource and place it in the root folder. This information is then used to initialize the`AIClient` in the code, to support interactions with those resources, as explained earlier.\\r\\n\\r\\n```json\\r\\n{\\r\\n    \\"subscription_id\\": \\"your_subscription_id\\",\\r\\n    \\"resource_group\\": \\"your_resource_group\\",\\r\\n    \\"project_name\\": \\"your_project_name\\"\\r\\n}\\r\\n```\\r\\n\\r\\n### 4.5 | Configure Environment Variables\\r\\n\\r\\nThe codebase comes with a sample `.env.sample` file that shows the environment variables you will need to configure, to run the sample. Copy this to `.env` then replace the placeholder strings with the values from the respective Azure resources you provisioned earlier. These environment variables will be used by the Azure AI SDK, to connect to relevant services (by endpoint) with required authentication (key) when implementing the chat function.\\r\\n\\r\\n```bash\\r\\nAZURE_SUBSCRIPTION_ID=replace_with_azure_subscription_id\\r\\nOPENAI_API_TYPE=azure\\r\\nOPENAI_API_KEY=replace_with_openai_key\\r\\nOPENAI_API_BASE=replace_with_openai_base\\r\\nOPENAI_API_VERSION=replace_with_openai_version\\r\\nAZURE_AI_SEARCH_ENDPOINT=replace_with_aisearch_target\\r\\nAZURE_AI_SEARCH_KEY=replace_with_aisearch_key\\r\\nAZURE_AI_SEARCH_INDEX_NAME=replace_with_aisearch_index_name\\r\\nAZURE_OPENAI_CHAT_MODEL=gpt-35-turbo-16k\\r\\nAZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo-16k-0613\\r\\nAZURE_OPENAI_EVALUATION_MODEL=gpt-35-turbo-16k\\r\\nAZURE_OPENAI_EVALUATION_DEPLOYMENT=\\"gpt-35-turbo-16k-0613\\"\\r\\nAZURE_OPENAI_EMBEDDING_MODEL=text-embedding-ada-002\\r\\nAZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-ada-embedding-002-2\\r\\n```\\r\\n\\r\\n### 4.6 | Explore Custom Data\\r\\n\\r\\nAt this point, the base system configuration is done and we just need to populate the data (for the search index) and then run, evaluate, and iterate, the chat function till the response quality is acceptable. Let\'s take a minute to explore the codebase `data/` folder to see the sample data we provide in the starter. We only use the product catalog data (to build the index) in _this_ sample but you can explore usage of the other data types for advanced features or integrations later.\\r\\n\\r\\n| Data Folder | Data Description |\\r\\n| --- | --- |\\r\\n| `data/0-misc` | General information - e.g., customer policies for org. |\\r\\n| `data/1-customer-info`| Customer purchase records  - for 13 fictional customers |\\r\\n| `data/2-chat-history`| Customer conversation history - for a subset of customers |\\r\\n| `data/3-product-info` | Product catalog data - for 20 items in 7 categories |\\r\\n| `data/4-scores` | Test data - for use in evaluations  |\\r\\n| `data/5-prompt-templates` | Example templates - for different contexts |\\r\\n\\r\\n### 4.7 | Explore The Codebase\\r\\n\\r\\nHere are the main files you need to be aware of:\\r\\n\\r\\n| File | Description |\\r\\n| --- | --- |\\r\\n| `src/run.py` | The main entry point for executing core operations |\\r\\n| `src/streaming_utils.py` | Functions for use in interactive conversation |\\r\\n| `src/copilot_aisdk/chat.py` | The chat function implementation. |\\r\\n| `src/system-message.jinja2` | The prompt template with system context (assistant) |\\r\\n\\r\\nYou can now execute the various steps of the end-to-end workflow as follows:\\r\\n- `python src/run.py --build-index` - to build the search index\\r\\n- `python src/run.py --question \\"which tent is the most waterproof?\\"` - to test the chat function\\r\\n- `python src/run.py --evaluate` - to evaluate the chat function\\r\\n- `python src/run.py --deploy` - to deploy the chat function\\r\\n- `python src/run.py --invoke` - to test the deployed chat API endpoint\\r\\n\\r\\nNote that the exact syntax and parameters used in these commands may evolve over time - so check the README in the sample for the latest instructions.\\r\\n\\r\\n### 4.8 | Explore The Chat Function\\r\\n\\r\\nLet\'s briefly talk about the custom code for the copilot, found in the `src/chat.py` file. \\r\\n- The main entry point is the `chat_completion` function that takes a list of messages representing the conversation history.\\r\\n- The `get_documents` function extracts the last message (\\"user question\\") and uses it to retrieve relevant search results using the OpenAI embeddings model and the Azure AI Search client respectively, in a _retrieval augmented generation_ (RAG) pattern.\\r\\n- The `chat_completion` function then takes the returned response and crafts an enhanced prompt (with the system context template, initial user message, and returned search results) and sends the request to the OpenAI chat model for completion.\\r\\n- The returned response is then returned to the user either interactively, or by adding it to the conversation thread (in stream mode).\\r\\n\\r\\n## 5 | Operationalization\\r\\n\\r\\nThe starter sample provides a simple sequence of command-line operations to build, run, evaluate, deploy, and test, the chat function. However, in a real-world scenario, you would integrate the deployed app with a front-end chat UI (like the Contoso Outdoors website) - and use the Azure AI Studio platform to further evaluate the chat function (batch runs), configure content filters (content safety), and monitor usage (performance) for iterative improvement. We\'ll discuss some of these tools and practices in the final post of this series.\\r\\n\\r\\n\\r\\n## 6 | Customizing the Sample\\r\\n\\r\\nThe quickstart sample is a great starting point for exploring your own application scenarios using your own data. Note that the sample is not designed for production use - you will need to do your own validation and evaluation of responses to determine if the chat function is suitable for your application needs. \\r\\n\\r\\nHowever, this is a great time to introduce you to the _cloud development environment_ provided by the [Azure AI curated Visual Studio Code environment](https://learn.microsoft.com/azure/ai-studio/how-to/develop-in-vscode?ocid=buildia24_60days_blogs).This allows you to open your fork of the sample directly from Azure AI Studio, creating a compute instance with a development environment that has the Azure AI SDK and other dependencies pre-installed. Watch this video from the Azure AI Studio team to see how that works - then replicate the process to jumpstart your application exploration journey. \\r\\n\\r\\n<iframe width=\\"600\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/UbJg7RNLi7E\\" title=\\"Build generative AI applications using custom code with Azure AI\\" frameborder=\\"0\\" allowfullscreen></iframe>\\r\\n\\r\\n## Resources\\r\\n\\r\\nWe\'ve referenced a number of links and samples in this post. Bookmark the [_Azure AI Studio: Code-First Collection_](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs) and revisit it regularly for an updated list of resources for code-first development of generative AI applications on Azure."},{"id":"build-contoso-chat-end-to-end","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/build-contoso-chat-end-to-end","source":"@site/blog-60daysofIA/2024-03-11/build-contoso-chat-end-to-end.md","title":"4.1 Build Contoso Chat End-to-End","description":"Building generative AI applications poses new challenges for streamlining end-to-end application development - from prompt engineering, to LLM Ops. In this post we introduce Contoso Chat, a sample application for building a copilot with your data - using the Azure AI platform with prompt flow.","date":"2024-03-11T09:01:00.000Z","formattedDate":"March 11, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":9.93,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-11T09:01","slug":"build-contoso-chat-end-to-end","title":"4.1 Build Contoso Chat End-to-End","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["promptflow","azure","aistudio","generativeai","e2e","llmops"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"Building generative AI applications poses new challenges for streamlining end-to-end application development - from prompt engineering, to LLM Ops. In this post we introduce Contoso Chat, a sample application for building a copilot with your data - using the Azure AI platform with prompt flow.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4.2 Build A Copilot Code-First with the Azure AI Python SDK","permalink":"/Cloud-Native/60DaysOfIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk"},"nextItem":{"title":"4. Fuel Your Intelligent Apps with Azure AI","permalink":"/Cloud-Native/60DaysOfIA/fuel-your-intelligent-apps-with-azure-ai"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/build-contoso-chat-end-to-end\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Building generative AI applications poses new challenges for streamlining end-to-end application development - from prompt engineering, to LLM Ops. In this post we introduce Contoso Chat, a sample application for building a copilot with your data - using the Azure AI platform with prompt flow.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-contoso-chat-end-to-end\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"Building generative AI applications poses new challenges for streamlining end-to-end application development - from prompt engineering, to LLM Ops. In this post we introduce Contoso Chat, a sample application for building a copilot with your data - using the Azure AI platform with prompt flow.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/build-contoso-chat-end-to-end\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n**Welcome to Day 1\ufe0f\u20e3 of Azure AI week on ##60Days Of IA** \\r\\n\\r\\nIn today\'s post, we\'ll introduce you to the [Contoso Chat](https://aka.ms/aitour/contoso-chat) sample - a comprehensive end-to-end reference sample that walks you through the journey of building the customer support AI application we talked about in our kickoff post yesterday. By the end of this tutorial, you will be able to:\\r\\n - explain how to build a copilot app end-to-end on Azure AI\\r\\n - explain what Retrieval Augmented Generation does for copilot apps\\r\\n - explain what prompt flow is and how it streamlines your workflow\\r\\n - describe the Azure AI platform and Azure AI SDK capabilities\\r\\n\\r\\n_Ready? Let\'s go!_\\r\\n\\r\\n## What You\'ll Learn Today\\r\\n * **Contoso Chat Sample**: Building a copilot with Azure AI and Prompt flow\\r\\n * **Retrieval Augmented Generation**: Design pattern for using custom data\\r\\n * **Prompt flow**: Open-source tooling for orchestrating end-to-end workflow\\r\\n * **Azure resources**: Provisioning Azure for the Contoso Chat AI project\\r\\n * **Hands-on lab**: Step-by-step tutorial to build & deploy Contoso Chat\\r\\n * **Exercise**: [_Fork the sample_](https://aka.ms/aitour/contoso-chat) then work through the hands-on tutorial.\\r\\n * **Resources**: [_Explore this collection_](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs) for samples, docs and training resources.\\r\\n\\r\\n<br/>\\r\\n\\r\\n![Build Contoso Chat - from prompt-engineering to LLM Ops](../../static/img/60-days-of-ia/blogs/2024-03-11/banner.png)\\r\\n\\r\\n---\\r\\n\\r\\n## Contoso Chat Sample\\r\\n\\r\\nThe [Contoso Chat](https://aka.ms/aitour/contoso-chat) sample provides a comprehensive end-to-end reference example for using Azure AI Studio and Prompt flow, to build a copilot application end-to-end. The sample implements a _customer support chat AI_ experience - allowing customers on the Contoso Outdoors website to ask questions about related products and receive relevant responses based on their query and purchase history. The illustrated guide below gives you a high-level overview of the steps involved in building the application - from provisioning Azure resources to deploying and using the chat AI endpoint. To learn more about the application scenario, refer to our [kickoff post](https://azure.github.io/Cloud-Native/60DaysOfIA/fuel-your-intelligent-apps-with-azure-ai) for this week.\\r\\n\\r\\n![Sketchnote](../../static/img/60-days-of-ia/blogs/2024-03-11/contoso-chat-sketchnote.png)\\r\\n\\r\\n## RAG Design Pattern\\r\\n\\r\\nOur first step is to define the application architecture for Contoso Chat. We know we want to have our copilot _grounded in our data_ so that customer queries return responses that reflect the product catalog or customer purchase history.\\r\\n\\r\\nThe challenge is that Large Language Models (LLM) are trained on massive datasets so the default responses may not be _relevant_ or _accurate_ with respect to your data. This is where prompt engineering and design patterns like Retrieval Augmented Generation (RAG) come in. RAG is a design pattern that uses an information _retrieval_ component to get data relevant to the user prompt, then _augments_ the prompt with that context before sending it to the LLM, as illustrated below.\\r\\n\\r\\n![RAG](../../static/img/60-days-of-ia/blogs/2024-03-11/rag.png)\\r\\n\\r\\nWe can break down the workflow into the following steps:\\r\\n 1. User asks a question (\\"User prompt\\")\\r\\n 1. The question is sent to an information retrieval component (\\"AI Search\\")\\r\\n 1. This vectorizes the query (\\"Embedding Model\\")\\r\\n 1. And uses the vector to retrieve relevant results (\\"Product Index\\")\\r\\n 1. Results are used to augment User prompt (\\"Model prompt\\")\\r\\n 1. The enhanced prompt is sent to the LLM (\\"Chat completion\\")\\r\\n\\r\\nThe answer is then returned to the user, who now sees a response that is more relevant to the products in your catalog, and personalized to their purchase history. Note that this basic copilot workflow requires us to deploy two large language models:\\r\\n 1. Text-Embedding model (e.g., `text-embedding-ada-002`) that vectories the user query \\r\\n 1. Text-Generation model (e.g., `gpt-35-turbo`) that generates the final response\\r\\n\\r\\n## Prompt flow Orchestration\\r\\n\\r\\nImplementing the RAG pattern requires a number of interactions between the language model deployments and the data sources used (e.g., search index for products, cusomer database for purchase history), and _coordination_ of intermediate steps before the final response can be delivered. This is where frameworks like Prompt flow, LangChain and Semantic kernel come in.\\r\\n\\r\\n\\r\\nThe Contoso Chat sample makes extensive use of Prompt flow - an [open-source project](https://github.com/microsoft/promptflow) on GitHub, with its own SDK and VS Code extension. Prompt flow provides a comprehensive solution that simplifies the process of prototyping, experimenting, iterating, and deploying your AI applications. It is [recommended for use as a feature within Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/prompt-flow?ocid=buildia24_60days_blogs), making it a natural first choice for building our Contoso Chat application. The figure shows a high-level architecture diagram showcasing the Azure components used with Prompt flow as the orchestration layer.\\r\\n\\r\\n![Prompt Flow Architecture](../../static/img/60-days-of-ia/blogs/2024-03-11/contoso-chat-flow.png)\\r\\n\\r\\nWith Prompt flow, your application is defined as a a directed acyclic graph of _nodes_ (`flow.dag.yaml`) that connect _input_ (prompt) and final _output_ (response) - with intermediate nodes implemented as Python _functions_ (tools) that process or transform the data flowing through them. The Prompt flow extension in VS Code provides a rich _visual editor_ capability as shown below, making it easy to define, debug, run, and test, your application in a local development environment. _This view also helps us see how the RAG pattern is implemented in practice, in our copilot_.\\r\\n\\r\\n![Contoso Chat Flow](../../static/img/60-days-of-ia/blogs/2024-03-11//promptflow-visual.png)\\r\\n\\r\\n## Azure Provisioning\\r\\n\\r\\nThe Contoso Chat sample comes with a [`provision.sh`](https://github.com/Azure-Samples/contoso-chat/blob/main/provision.sh) script that will pre-provision many of the Azure resources for you, for use in the development workflow. To get started with the implementation, follow the instructions in the [README](https://github.com/Azure-Samples/contoso-chat/blob/main/README.md) file in the repo by doing the following:\\r\\n 1. [Fork the sample](https://github.com/Azure-Samples/contoso-chat/fork) to your own GitHub account\\r\\n 2. [Setup development environment](https://github.com/Azure-Samples/contoso-chat/blob/main/README.md#3-development-environment) using GitHub Codespaces\\r\\n 3. [Authenticate](https://github.com/Azure-Samples/contoso-chat/tree/main#41-authenticate-with-azure) with your Azure subscription\\r\\n 4. [Run the Provisioning script](https://github.com/Azure-Samples/contoso-chat/tree/main#42-run-provisioning-script) and verify your setup is complete\\r\\n\\r\\nAt this point, you should have an Azure resource group created for your project with the following resources created for your application. Note that in order to complete this step, you must have a valid Azure subscription that has been given access to the relevant Azure OpenAI services. You must also have available quota for model deployments in the specific regions that we use in the provisioning script.\\r\\n\\r\\n![Provisioning Azure](../../static/img/60-days-of-ia/blogs/2024-03-11//provision-azure.png)\\r\\n\\r\\n## Hands-on Lab\\r\\n\\r\\nYou can now complete the step-by-step tutorial in the [README](https://github.com/Azure-Samples/contoso-chat/blob/main/README.md) to build, evaluate and deploy the application. Let\'s quickly review the main steps involved in the end-to-end workflow.\\r\\n\\r\\n| Stage | Description |\\r\\n|:---|:---|\\r\\n| 1. Build a Copilot. | Get familiar with the application codebase. Check out the `data/` folder to see the data we will be using for customer order (history) and product catalog (index). |\\r\\n| 2. Provision Azure. | Run the `./provision.sh` script or manually provision the required resources. This should setup an Azure AI hub (manage), an Azure AI project (build), an Azure Cosmos DB resource  (customer data) and an Azure AI Search resource (product index). Verify you have a `config.json` created (for local Azure configuration) and an `.env` file (for relevant keys and endpoints for access). |\\r\\n| 3. Add Models & Data. | The provisioning script does the model deployments - but review them now. Make sure you have a chat completion model (gpt-35-turbo), a chat evaluation model (gpt-4) and a text-embeddings model (text-embedding-ada-02). Use the provided notebooks to populate the data in Azure Cosmos DB and Azure AI Search. |\\r\\n| 4. Add Connections | The `devcontainer` configuration ensures you have the Prompt flow extension installed in VS Code, and the `pf` too for command-line, by default. Use the provided notebooks to setup _connection configurations_ from prompt flow to key services (Azure OpenAI, Azure AI Search, Azure Cosmos DB) for use in related notes of the prompt flow graph. Use the `pf` tool to validate these were setup correctly (on VS Code). The provision script may have setup some of these for you in the cloud (Azure) for use in later stages (deploy) - take a minute to verify and correct these as described in README. |\\r\\n| 5. Build Prompt Flow| You are all set to run the prompt flow with your data in Azure.  Explore the components of the prompt flow. Click the _stylized P_ icon in the sidebar to see the Prompt Flow extension activity menu. Open the `contoso-chat/flow.dag.yaml` file in VS Code, then click the _Visual Editor_ option to see the view shown in the earlier screeshot above. Run it to validate it works - then explore the nodes, outputs and code.|\\r\\n| 6. Evaluate Prompt Flow| You can complete a local evaluation by opening the relevant notebook and running it _cell-by-cell_. Review the code in each cell of the notebook, then analyze the output to understand what the relevant metrics are telling you about the quality of the basic flow. The _batch run_ step takes a while and requires Azure connection setup so consider that an optional step. Switch periodically to the _Azure AI Studio_ website view to see how the relevant Azure AI project pages are updated to show the status of various activities or configurations. |\\r\\n| 7. Deploy Prompt Flow| Deploying the prompt flow is a 2-step process. First, we need to upload the flow (code, assets) to Azure AI Studio. Do this using the provided notebook, or you can try to do this manually using the _import_ option in Azure AI Studio under the _Prompt Flow_ section. Once uploaded, you need to select a runtime (\\"automatic\\") and start it to get a compute instance provisioned to execute your flow. Use that to _test_ that your flow was imported successfully. Then click the _Deploy_ option to deploy the flow. This will take a while - refresh the _Deployments_ page to get updates. Once deployment is successful, use the built-in testing feature to try a simple question against the hosted API endpoint. **Congratulations** Your chat AI endpoint is ready for use! |\\r\\n| 8. Summary & Clean up | This was a lot. Note that almost every step of this process can be achieved using code (SDK), command-line (CLI) or UI (Studio website) so explore the documentation. _Note that Azure AI Studio is in preview_ so the features are constantly evolving and things may break unexpectedly - send feedback if so! Finally, don\'t forget to **delete your codespaces and your Azure resources for this lab** to avoid unnecessary charges. And watch the sample repo for updates on workshop content and exercises to extend this further. |\\r\\n| | |\\r\\n\\r\\nCompleting this workshop can take 60-90 minutes based on your level of familiarity with the tools. In the _next_ blog post, we\'ll dive a bit deeper into the process with specific focus on the **Azure AI SDK** to understand _how_ you can implement core steps of the workflow from your Python application. And, in the _final_ post of this week, we\'ll return to the Contoso Chat sample to explore deployment and evaluation in more detail - with additional guidance for ensuring responsible AI usage in your generative AI applications.\\r\\n\\r\\n\\r\\n## Exercise\\r\\n\\r\\nCongratulations! You made it to the end of this whirlwind tour of the Contoso Chat sample. Now it\'s time for you to do the hard work of building this yoursel!! Start by [_forking the sample_](https://aka.ms/aitour/contoso-chat) - then follow the step-by-step instructions in the README.\\r\\n\\r\\n\\r\\n## Resources\\r\\n\\r\\nWe\'ve referenced a number of links and samples in this post. Bookmark the [_Azure AI Studio: Code-First Collection_](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs) and revisit it regularly for an updated list of resources for code-first development of generative AI applications on Azure."},{"id":"fuel-your-intelligent-apps-with-azure-ai","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/fuel-your-intelligent-apps-with-azure-ai","source":"@site/blog-60daysofIA/2024-03-11/fuel-your-intelligent-apps-with-azure-ai.md","title":"4. Fuel Your Intelligent Apps with Azure AI","description":"In this kickoff post, we\'ll set the stage for the week of posts by describing the application scenario (motivation) and introducing core terminology (LLM Ops), developer tools (Azure AI Studio, frameworks) and design patterns (RAG) to help you jumpstart your journey building and deploying generative AI solutions in the enterprise. By the end of this week, you should have a good understanding of how to build a copilot app end-to-end on the Azure AI platform, how to deploy it for integration with real-world applications, and how to incorporate responsible AI principles into your development workflow.","date":"2024-03-11T09:00:00.000Z","formattedDate":"March 11, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":9.09,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-11T09:00","slug":"fuel-your-intelligent-apps-with-azure-ai","title":"4. Fuel Your Intelligent Apps with Azure AI","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["promptflow","azure","aistudio","generativeai","e2e","llmops"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this kickoff post, we\'ll set the stage for the week of posts by describing the application scenario (motivation) and introducing core terminology (LLM Ops), developer tools (Azure AI Studio, frameworks) and design patterns (RAG) to help you jumpstart your journey building and deploying generative AI solutions in the enterprise. By the end of this week, you should have a good understanding of how to build a copilot app end-to-end on the Azure AI platform, how to deploy it for integration with real-world applications, and how to incorporate responsible AI principles into your development workflow.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4.1 Build Contoso Chat End-to-End","permalink":"/Cloud-Native/60DaysOfIA/build-contoso-chat-end-to-end"},"nextItem":{"title":"3.3 Dynamic Repricing of Products Using Intelligent Apps Part 3: Graphing and Displaying Price Forecasts in a Web Interface","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-3"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/fuel-your-intelligent-apps-with-azure-ai\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this kickoff post, we\'ll set the stage for the week of posts by describing the application scenario (motivation) and introducing core terminology (LLM Ops), developer tools (Azure AI Studio, frameworks) and design patterns (RAG) to help you jumpstart your journey building and deploying generative AI solutions in the enterprise. By the end of this week, you should have a good understanding of how to build a copilot app end-to-end on the Azure AI platform, how to deploy it for integration with real-world applications, and how to incorporate responsible AI principles into your development workflow.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/fuel-your-intelligent-apps-with-azure-ai\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this kickoff post, we\'ll set the stage for the week of posts by describing the application scenario (motivation) and introducing core terminology (LLM Ops), developer tools (Azure AI Studio, frameworks) and design patterns (RAG) to help you jumpstart your journey building and deploying generative AI solutions in the enterprise. By the end of this week, you should have a good understanding of how to build a copilot app end-to-end on the Azure AI platform, how to deploy it for integration with real-world applications, and how to incorporate responsible AI principles into your development workflow.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/fuel-your-intelligent-apps-with-azure-ai\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n# Kicking Off Azure AI Week!\\r\\n\\r\\nWelcome to the `Azure AI` week on **#60Days Of IA**. Over the next 5 days, we\'ll share a series of blog posts that give you a comprehensive look at the tools and end-to-end development workflow reequired to build intelligent applications [code-first on the Azure AI platform](https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/a-code-first-experience-for-building-a-copilot-with-azure-ai/ba-p/4058659?ocid=buildia24_60days_blogs). \\r\\n\\r\\nIn this kickoff post, we\'ll set the stage for the week of posts by describing the application scenario (motivation) and introducing core terminology (LLM Ops), developer tools (Azure AI Studio, frameworks) and design patterns (RAG) to help you jumpstart your journey building and deploying generative AI solutions in the enterprise. By the end of this week, you should have a good understanding of how to build a copilot app end-to-end on the Azure AI platform, how to deploy it for integration with real-world applications, and how to incorporate responsible AI principles into your development workflow.\\r\\n\\r\\nReady? Let\'s get started!\\r\\n\\r\\n## What We\'ll Cover Today\\r\\n * **Application Scenario |** What is Contoso Chat?\\r\\n * **Paradigm Shift |** What is LLM Ops?\\r\\n * **Unified Platform |** What is Azure AI Studio?\\r\\n * **Copilot Experience |** What is the development workflow?\\r\\n * **The Week Ahead |** What will we cover?\\r\\n * **Resources:** [Explore the Code-First Azure AI Collection](https://aka.ms/ai-studio/collection?ocid=buildia24_60days_blogs)\\r\\n\\r\\n---\\r\\n\\r\\n![Roadmap](../../static/img/60-days-of-ia/blogs/2024-03-11/banner.png)\\r\\n\\r\\n<br/>\\r\\n\\r\\nGenerative AI applications are transforming the user experience and accelerating adoption of AI tools and solutions in the enterprise. But as developers, we face new challenges in building solutions **end-to-end** - from prompt engineering to LLM Ops. We need new tools, frameworks, and guidance to help us navigate and streamline a fast-growing ecosystem. \\r\\n\\r\\nIn [a recent blog post](https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/a-code-first-experience-for-building-a-copilot-with-azure-ai/ba-p/4058659?ocid=buildia24_60days_blogs) we described how the Azure AI platform is addressing these challanges with a _code-first experience for building a copilot application end-to-end_ with your data and APIs. This week, we unpack that post in more detail - walking you through a end-to-end application sample, and several _quickstart_ options, to get you started on your own generative AI solutions.\\r\\n\\r\\nTo kick things off, let\'s set the stage by describing a common generative AI application scenario (\\"Contoso Chat\\") and introduce core terminology, tools and processes that we will be using throughout the week, on our development journey.\\r\\n\\r\\n## 1 | The Application Scenario\\r\\n\\r\\nSay hello to _Contoso Outdoor Company_ - an online retailer of outdoor adventuring equipment with a loyal and growing customer base. Your website has a rich catalog of items organized into categories like _tents_, _backpacks_, _hiking boots_ and more. Customers visit the site looking to find the best gear for their next adventure, and often have questions about the products, or how they might fit with their previous purchases.\\r\\n\\r\\n![Contoso Outdoors site](../../static/img/60-days-of-ia/blogs/2024-03-11/app-contoso-outdoors.png)\\r\\n\\r\\nThe company has a customer support line, but it is getting overwhelmed with calls and you don\'t have the resources to meet the demand. You hear about generative AI applications and decide to build a _customer support chat AI_ agent that knows your catalog and customers. You can then integrate it into the site as shown, to improve customer satisfaction and drive follow-up actions.\\r\\n\\r\\n![Contoso Chat concept](../../static/img/60-days-of-ia/blogs/2024-03-11/app-contoso-chat-concept.png)\\r\\n\\r\\nYou identify three requirements for your chat AI application:\\r\\n - **Custom Data**. The application responses must prioritize your catalog data.\\r\\n - **Responsible AI**. The application must follow responsible AI principles.\\r\\n - **LLM Ops**. The end-to-end development workflow must be operationalizable.\\r\\n\\r\\n## 2 | The Paradigm Shift\\r\\n\\r\\nBuilding generative AI applications requires a different mindset from traditional ML applications. The latter are trained on finite custom data, deploying an endpoint that makes _predictions_. By contrast, generative AI applications are trained on massive amounts of data, using large language models (LLM) and natural language processing (NLP) to _generate_ new content.\\r\\n\\r\\nThe focus now moves from **MLOps** (workflow for building ML apps) to **LLMOps** (workflow for building generative AI apps) - starting with _prompt engineering_, a process where we refine the inputs to the LLM (\\"prompts\\") through a process of trial-and-error (build-run-evaluate) till the responses meet our quality, cost and performance requirements. The generative AI application lifecycle now looks more like this:\\r\\n\\r\\n![LLM App Lifecyle](../../static/img/60-days-of-ia/blogs/2024-03-11/llm-app-lifecycle.png)\\r\\n\\r\\n1. **Ideation Phase**: Start by building the basic AI application (copilot) for your scenario. At this stage, you define the architectural elements (AI resources, design patterns) and language models (chat completion, chat evaluation, text embeddings) that you will need to build-run-evaluate the basic experience. And have sample data to test against.\\r\\n2. **Augmentation Phase**: Iteratively refine the quality and performance of your application by _engineering_ the prompts, _tuning_ the models, and _evaluating_ the responses with sample data (smal) and batch runs (large). Use relevant metrics (groundedness, coherence, relevance, fluency) to guide decisions on what to change, and when to stop iterating.\\r\\n3. **Operationalization Phase:** Now, you\'re ready to deploy the application to a production environment so that the endpoint can be accessed by others, for integrating into user-facing experiences. This is also a good time to review the entire workflow for responsible AI practices, and explore automation and monitoring solutions for efficiency and performance.\\r\\n\\r\\n## 3 | The Azure AI Platform\\r\\n\\r\\nImplementing this end-to-end workflow and managing the various phases of the application lifecycle can be challenging for developers. Azure AI Studio addresses these challenges with a [**unified platform**](https://ai.azure.com?ocid=buildia24_60days_blogs) for building generative AI applications and custom copilot experiences. \\r\\n\\r\\nUse the platform to **explore** language models from Microsoft and the broader community, and experiment with them in a built-in playground. Then **build** your \\r\\nAI project by seamlessly integrating with deployed models and built-in AI services - and **manage** your AI resources (for compute, access, billing and more) from the unified UI. \\r\\n\\r\\n![Azure AI Studio](../../static/img/60-days-of-ia/blogs/2024-03-11/azure-ai.png)\\r\\n\\r\\nAs a developer, you have both low-code and code-first options for engaging with the platform. Use the [Azure AI Studio UI](https://ai.azure.com?ocid=buildia24_60days_blogs) for a browser-based low-code experience, and the [Azure AI SDK](https://learn.microsoft.com/azure/ai-studio/how-to/sdk-generative-overview?ocid=buildia24_60days_blogs) for a Python-based code-first experience. In our posts this week, we\'ll focus on the code-first experience, and show you how to build a copilot app on Azure AI using the Python SDK and popular frameworks.\\r\\n\\r\\n\\r\\n## 4 | The Copilot Experience\\r\\n\\r\\nSo how do we get started on the end-to-end development journey using the Azure AI platform? Let\'s start by defining what we mean by a _copilot_ experience for enterprise-grade generative AI applications. A copilot is:\\r\\n - a generative AI application that uses large language models (LLM) and natural language processing (NLP) \\r\\n - to assist customers in completing complex cognitive tasks **using your data** \\r\\n - typically using conversational \u201cchat\u201d interactions (request-reponse)\\r\\n\\r\\nThe copilot (generative AI application) is deployed in the cloud to expose an interaction endpoint (API) that can be integrated into customer-facing experiences (e.g,, web or mobile apps) for real-world use. For our specific application scenario, the implementation will involve two components:\\r\\n - Contoso Chat (copilot API) as the backend component with the chat AI\\r\\n - Contoso Outdoors (web App) as the frontend component with the chat UI\\r\\n\\r\\n![Azure Copilot](../../static/img/60-days-of-ia/blogs/2024-03-11/copilot-architecture.png) \\r\\n\\r\\nThe figure shows the high-level application architecture for [building generative AI applications using custom code with Azure AI](https://www.youtube.com/watch?v=UbJg7RNLi7E), where the **App** represents the front-end component and the blue box encloses the components of the **Copilot** implementation exposed through the managed online endpoint (API). The copilot experience now involves the following steps:\\r\\n - The user (customer) asks a question from the chat UI (web app)\\r\\n - The web app sends the question to the chat API (copilot endpoint)\\r\\n - The chat API invokes our custom Python code (chat function) which:\\r\\n    - converts the user question (prompt) into a machine-friendly format (vector)\\r\\n    - uses the vectorized prompt to find matches in our custom data (search index)\\r\\n    - combines the user question with custom results for an enhanced prompt\\r\\n    - sends this prompt to the chat model to get the completion (answer)\\r\\n - The chat API now returns the answer as a response to the chat UI request\\r\\n\\r\\nTo build this workflow requires us to complete the following steps:\\r\\n 1. Provision the necessary resources on Azure\\r\\n 1. Create the search index using our custom data\\r\\n 1. Deploy chat and embedding models for use by the chat function\\r\\n 1. Configure connections between chat function and models, resources\\r\\n 1. Write the code to _orchestrate_ the steps for the chat function\\r\\n 1. Deploy our chat function to expose the API endpoint online\\r\\n 1. Integrate the API endpoint with our front-end application for usage\\r\\n\\r\\nFrom an LLM Ops perspective, we also need to consider two additional steps:\\r\\n 1. Evaluation of the chat function using sample data - to assess quality\\r\\n 1. Automation of the workflow steps - for iteration and operationalization\\r\\n\\r\\nThis is a non-trivial set of requirements for building, running, evaluating, and deploying a generative AI application. Thankfully, the Azure AI platform and related ecosystem of tools and services, helps streamline the process for developers - allowing us to focus on our chat function logic and user experience.\\r\\n\\r\\n## 5 | The Week Ahead!\\r\\n\\r\\nIn the upcoming week, we\'ll dive into the implementation details of these processes in the context of a signature reference sample (Contoso Chat) and as quickstart templates that showcase usage with popular frameworks. Here\'s what we\'ll cover:\\r\\n- [**Day 1:**](https://azure.github.io/Cloud-Native/60DaysOfIA/build-contoso-chat-end-to-end) Build the Contoso Chat app on Azure AI (end-to-end reference sample)\\r\\n- [**Day 2:**](https://azure.github.io/Cloud-Native/60DaysOfIA/build-a-copilot-code-first-with-the-azure-ai-python-sdk) Build a Copilot app on Azure AI with the Python SDK (quickstart)\\r\\n- [**Day 3:**](https://azure.github.io/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-prompt-flow) Build a Copilot app on Azure AI with promptflow (framework)\\r\\n- [**Day 4:**](https://azure.github.io/Cloud-Native/60DaysOfIA/build-a-copilot-on-azure-code-first-with-langchain) Build a Copilot app on Azure AI with LangChain (framework)\\r\\n- [**Day 5:**](https://azure.github.io/Cloud-Native/60DaysOfIA/deploying-your-copilot-on-azure) Deploy your Copilot app responsibly on Azure AI (advanced topics)"},{"id":"dynamic-repricing-of-products-using-intelligent-apps-part-3","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-3","source":"@site/blog-60daysofIA/2024-03-12/dynamic-repricing-of-products-using-intelligent-apps-part-3.md","title":"3.3 Dynamic Repricing of Products Using Intelligent Apps Part 3: Graphing and Displaying Price Forecasts in a Web Interface","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","date":"2024-03-08T09:05:00.000Z","formattedDate":"March 8, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":8.03,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-08T09:05","slug":"dynamic-repricing-of-products-using-intelligent-apps-part-3","title":"3.3 Dynamic Repricing of Products Using Intelligent Apps Part 3: Graphing and Displaying Price Forecasts in a Web Interface","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"4. Fuel Your Intelligent Apps with Azure AI","permalink":"/Cloud-Native/60DaysOfIA/fuel-your-intelligent-apps-with-azure-ai"},"nextItem":{"title":"3.2 Dynamic Repricing of Products Using Intelligent Apps Part 2","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-2"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/dynamic-repricing-of-products-using-intelligent-apps-part-3\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-3\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-3\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Dynamic Repricing of Products Using Intelligent Apps: Graphing and Displaying Price Forecasts in a Web Interface](../../static/img/60-days-of-ia/blogs/2024-03-12/3-3-1.png)\\r\\n\\r\\n*This three-part series demonstrates how to use Azure Cosmos DB to build an Intelligent App that uses historical pricing and product data to forecast future price fluctuations for specific products. In the final article of the series, you\u2019ll build a web interface to graph and display the Intelligent App\u2019s price forecasts.*\\r\\n\\r\\n## Dynamic Repricing of Products Using Intelligent Apps Part 3: Graphing and Displaying Price Forecasts in a Web Interface\\r\\n\\r\\nIn [\u200b\u200bPart 1](https://azure.github.io/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1) of this series, you set up an Azure Cosmos DB database and populated the database with pricing data. Then, in \u200b[\u200bPart 2](https://azure.github.io/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-2), you successfully set up an Azure Machine Learning model and deployed it as a web service.\\r\\n\\r\\nIn this final article of the series, you\u2019ll create a web application using Flask that interacts with the Azure Machine Learning endpoint to retrieve predictions and display them using a simple graph.\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nBefore proceeding, ensure you have the following:\u202f \\r\\n\\r\\n- [Python](https://www.python.org/downloads/) version 3.10 or greater\\r\\n- Flask (`pip install flask`)\\r\\n- Requests (`pip install requests`)\\r\\n- Matplotlib (`pip install matplotlib`)\\r\\n- Access to the Azure Machine Learning endpoint created in Part 2\\r\\n- [Docker](https://docs.docker.com/get-docker/), including the [Docker command-line interface](https://docs.docker.com/engine/reference/commandline/cli/) (CLI), installed. You\u2019ll use this to build a container image to run the web app on Azure Kubernetes Service (AKS).\\r\\n- The [Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli?ocid=buildia24_60days_blogs) installed. You\u2019ll use this for deployment to AKS.\\r\\n\\r\\nFor a preview of the completed Intelligent App, take a look at the [\u200bproject code](https://aka.ms/intelligent-apps/60daysofIA/3.3projectcode).\\r\\n\\r\\n:::info\\r\\nComplete the **[Data Skills Challenge](https://aka.ms/intelligent-apps/data-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n### Building the Web Interface\\r\\n\\r\\nIt only takes a few steps to create a simple web app that queries the Azure Machine Learning endpoint, retrieves predictions, and displays the resulting prediction in a graph. Let\u2019s dive in!\\r\\n\\r\\nStart by creating a new folder for your web application. Then, create these files and folders in it:\\r\\n\\r\\n```\\r\\n/your-flask-app\\r\\n\u202f\u202f\u202f /templates\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f index.html\\r\\n\u202f\u202f\u202f app.py\\r\\n```\\r\\n\\r\\nThe `app.py` file is the backbone of the Flask application. So, add the following code to it:\\r\\n\\r\\n```\\r\\nfrom flask import Flask, render_template, request\\r\\nimport requests\\r\\nimport json\\r\\nimport matplotlib.pyplot as plt\\r\\nimport io\\r\\nimport base64\\r\\nfrom datetime import datetime, timedelta\\r\\n```\\r\\n\\r\\n```\\r\\napp = Flask(__name__)\\r\\n```\\r\\n\\r\\n```\\r\\n# Replace with your actual Azure ML endpoint and key\\r\\nscoring_uri = \'<your_azure_ml_endpoint>\'\\r\\napi_key = \'<your_api_key>\'\u202f # Replace with your actual key if needed\\r\\n```\\r\\n```\\r\\ndef generate_future_dates(start_date, periods=3, freq=\'M\'):\\r\\n\u202f\u202f\u202f # Generate future dates for the next \'periods\' months\\r\\n\u202f\u202f\u202f future_dates = [(start_date + timedelta(days=30 * i)).strftime(\'%Y%m\') for i in range(1, periods + 1)]\\r\\n\u202f\u202f\u202f return future_dates\\r\\n```\\r\\n\\r\\n```\\r\\ndef get_predictions(dates):\\r\\n\u202f\u202f\u202f # Prepare the data in JSON format\\r\\n\u202f\u202f\u202f data = {\\"data\\": [[date] for date in dates]}\\r\\n\u202f\u202f\u202f headers = {\'Content-Type\': \'application/json\'}\\r\\n\u202f\u202f\u202f if api_key:\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f headers[\'Authorization\'] = f\'Bearer {api_key}\'\\r\\n\\r\\n\u202f\u202f\u202f # Send the request to the Azure ML endpoint\\r\\n\u202f\u202f\u202f response = requests.post(scoring_uri, json=data, headers=headers)\\r\\n\u202f\u202f\u202f if response.status_code == 200:\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f return response.json()\\r\\n\u202f\u202f\u202f else:\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f raise Exception(f\\"Failed to fetch prediction: {response.text}\\")\\r\\n```\\r\\n\\r\\n```\\r\\n@app.route(\'/\', methods=[\'GET\', \'POST\'])\\r\\ndef index():\\r\\n\u202f\u202f\u202f graph_url = None\\r\\n\u202f\u202f\u202f if request.method == \'POST\':\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f start_date = datetime.utcnow()\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f future_dates = generate_future_dates(start_date)\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f predictions = get_predictions(future_dates)\\r\\n```\\r\\n\\r\\n```\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f # Plotting the predictions\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.figure(figsize=(10, 5))\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.plot(future_dates, predictions, marker=\'o\', linestyle=\'-\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.title(\'Future Price Predictions for Jackets\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.xlabel(\'Date (YYYYMM)\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.ylabel(\'Predicted Price\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.grid(True)\\r\\n\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f # Save plot to a BytesIO buffer\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f img = io.BytesIO()\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.savefig(img, format=\'png\', bbox_inches=\'tight\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f img.seek(0)\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f graph_url = base64.b64encode(img.getvalue()).decode(\'utf8\')\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f plt.close()\\r\\n```\\r\\n```\\r\\n\u202f\u202f\u202f return render_template(\'index.html\', graph_url=graph_url)\\r\\n```\\r\\n```\\r\\nif __name__ == \'__main__\':\\r\\n\u202f\u202f\u202f app.run(debug=True)\\r\\n```\\r\\n\\r\\nThis simple Flask app accepts incoming requests and queries the Azure Machine Learning endpoint for the next few months of price forecasts for jackets. When it receives the predictions, it generates a graph using `matplotlib`, encoding it with base64 so it can display it in the HTML template. In a larger app, you could save the image to disk and then load it in the web page instead of base64 encoding it\u2014but we\u2019ve skipped that here to keep things simple.\\r\\n\\r\\nNext, create an `index.html` file in the templates directory. Add the following code for the user interface:\\r\\n\\r\\n```\\r\\n<!DOCTYPE html>\\r\\n<html lang=\\"en\\">\\r\\n<head>\\r\\n\u202f\u202f\u202f <meta charset=\\"UTF-8\\">\\r\\n\u202f\u202f\u202f <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1.0\\">\\r\\n\u202f\u202f\u202f <title>Price Forecast Visualization</title>\\r\\n\u202f\u202f\u202f \x3c!-- Load Tailwind CSS from CDN --\x3e\\r\\n\u202f\u202f\u202f <link href=\\"https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css\\" rel=\\"stylesheet\\">\\r\\n</head>\\r\\n<body class=\\"bg-gray-100 flex flex-col justify-center items-center min-h-screen\\">\\r\\n\u202f\u202f\u202f <div class=\\"w-full bg-blue-800 text-white\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f <div class=\\"container mx-auto py-4\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f <h1 class=\\"text-center text-xl md:text-3xl font-bold\\">Price Forecast for Jackets</h1>\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f </div>\\r\\n\u202f\u202f\u202f </div>\\r\\n\\r\\n\u202f\u202f\u202f <div class=\\"mt-8 mb-4\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f <form method=\\"post\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f <button type=\\"submit\\" class=\\"bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f Get Future Price Predictions\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f </button>\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f </form>\\r\\n\u202f\u202f\u202f </div>\\r\\n\\r\\n\u202f\u202f\u202f {% if graph_url %}\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f <div class=\\"shadow-xl bg-white rounded-lg p-8\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f <h2 class=\\"text-lg md:text-xl font-semibold mb-4 text-center\\">Price Prediction Graph</h2>\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f <div class=\\"flex justify-center\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f <img src=\\"data:image/png;base64,{{ graph_url }}\\" alt=\\"Price Prediction Graph\\" class=\\"max-w-full h-auto rounded-lg\\">\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f\u202f </div>\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f </div>\\r\\n\u202f\u202f\u202f {% endif %}\\r\\n</body>\\r\\n</html>\\r\\n```\\r\\n\\r\\nTo run your Flask app, navigate to the directory containing your `app.py` file and execute the following command:\\r\\n\\r\\n```\\r\\nflask run\\r\\n```\\r\\n\\r\\nYour web application should now be accessible at `http://127.0.0.1:5000`. Users can input feature data, submit it, and see both the predicted price and a simple graph comparing the current and predicted prices.\\r\\n\\r\\n:::info\\r\\nCheck out the **[Azure Cosmos DB Ask The Expert](https://aka.ms/intelligent-apps/ate-cosmos?ocid=buildia24_60days_blogs)** session to learn how to build RAG solutions, manage chat history by seamlessly connecting with *Azure OpenAI*, as well as explore the power of *Azure Cosmos DB\'s copilot*.\\r\\n\\r\\nThe experts also cover how to seamlessly integrate your operational and transactional data with AI frameworks and sdks like Semantic Kernel, Langchain, and LlamaIndex.\\r\\n:::\\r\\n\\r\\n### Deploying to Azure Kubernetes Service (AKS)\\r\\n\\r\\nRunning locally is great, but in production, you\u2019ll probably want to deploy to the cloud. Fortunately, Azure makes this easy. Let\u2019s review how to deploy your Flask app using AKS.\\r\\n\\r\\nFirst, you need to containerize the Flask app and push it to an Azure Container Registry. Then, you\u2019ll create an AKS cluster and deploy the container image to it.\\r\\n\\r\\n#### Create a Dockerfile\\r\\n\\r\\nStart by creating a file named `Dockerfile` in the Flask app\u2019s root folder. Add the following contents:\\r\\n\\r\\n```\\r\\nFROM python:3.11-slim\\r\\n```\\r\\n```\\r\\nWORKDIR /usr/src/app\\r\\n```\\r\\n```\\r\\nRUN pip install --no-cache-dir Flask\\r\\n```\\r\\n```\\r\\nCOPY . .\\r\\n```\\r\\n```\\r\\nCMD [\\"flask\\", \\"run\\"]\\r\\n```\\r\\n\\r\\n#### Create a Container Registry\\r\\n\\r\\nNext, create a container registry to store the container image. Use the Azure CLI to create a new resource group if you don\u2019t already have one you\u2019d like to use:\\r\\n\\r\\n```\\r\\naz group create --name my-container-resources --location eastus\\r\\n```\\r\\n\\r\\nThen, create a container registry in the resource group:\\r\\n\\r\\n```\\r\\naz acr create --resource-group my-container-resources --name my-registry --sku Basic\\r\\n```\\r\\n\\r\\nYou\u2019re now ready to build the container and push it to the registry.\\r\\n\\r\\n#### Build and Push the Container Image\\r\\n\\r\\nBuild the container image using the following command:\\r\\n\\r\\n```\\r\\ndocker build -t my-app-image .\\r\\n```\\r\\n\\r\\nThen, push the image to your container registry:\\r\\n\\r\\n```\\r\\ndocker push my-registry.azurecr.io/my-app-image\\r\\n```\\r\\n\\r\\n#### Create an AKS Cluster\\r\\n\\r\\nNow, it\u2019s time to create an AKS cluster. Run the following:\\r\\n\\r\\n```\\r\\naz aks create --name my-aks-cluster --resource-group my-resource-group --node-count 3 --node-vm-size Standard_B2s --location eastus\\r\\n```\\r\\n\\r\\nIt may take a few minutes for Azure to spin up your cluster. Once it\u2019s ready, you can deploy the Flask app.\\r\\n\\r\\n#### Deploy the Application to AKS\\r\\n\\r\\nCreate a Kubernetes deployment file named `deployment.yaml` in the project\u2019s root folder with the following contents. Update the image field to match the name of your registry and container image.\\r\\n\\r\\n```\\r\\napiVersion: apps/v1\\r\\nkind: Deployment\\r\\nmetadata:\\r\\n\u202f name: my-app-deployment\\r\\nspec:\\r\\n\u202f replicas: 1\\r\\n\u202f selector:\\r\\n\u202f\u202f\u202f matchLabels:\\r\\n\u202f\u202f\u202f\u202f\u202f app: my-app\\r\\n\u202f template:\\r\\n\u202f\u202f\u202f metadata:\\r\\n\u202f\u202f\u202f\u202f\u202f labels:\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f app: my-app\\r\\n\u202f\u202f\u202f spec:\\r\\n\u202f\u202f\u202f\u202f\u202f containers:\\r\\n\u202f\u202f\u202f\u202f\u202f - name: my-app\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f image: my-registry.azurecr.io/my-app-image\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f ports:\\r\\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f - containerPort: 5000\\r\\n```\\r\\n\\r\\nFinally, deploy the application to the AKS cluster using the following command:\\r\\n\\r\\n```\\r\\nkubectl apply -f deployment.yaml\\r\\n```\\r\\n#### Verify the Deployment\\r\\n\\r\\nOnce deployed, verify that the application is running using the following command:\\r\\n\\r\\n```\\r\\nkubectl get pods\\r\\n```\\r\\n\\r\\nYou should see a pod named `my-app` in the `Running` state.\\r\\n\\r\\nTo access the application, port-forward the service using the following command:\u202f\\r\\n\\r\\n```\\r\\nkubectl port-forward svc/my-app-service 5000:5000\\r\\n```\\r\\n\\r\\nFinally, navigate to `http://localhost:5000` in a web browser to verify the application is running.\\r\\n\\r\\n### Conclusion\\r\\n\\r\\nIn the final part of this series, you learned how to create a simple Flask web app that interacts with the Azure Machine Learning endpoint to provide real-time price predictions and visualize them. By integrating cloud-based artificial intelligence (AI) models with a web interface like this, businesses can dynamically adjust their pricing\u2014helping them remain competitive and stand out from the rest.\\r\\n\\r\\nIf you like what you\u2019ve seen in this series, try the **[Intelligent Apps Cloud Skill Challenge](https://aka.ms/intelligent-apps/csc)**. You can also register for **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days** at the premier conference for cloud-native technologies, *KubeCon EU 2024*."},{"id":"dynamic-repricing-of-products-using-intelligent-apps-part-2","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-2","source":"@site/blog-60daysofIA/2024-03-08/dynamic-repricing-of-products-using-intelligent-apps-part-2.md","title":"3.2 Dynamic Repricing of Products Using Intelligent Apps Part 2","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","date":"2024-03-08T09:01:00.000Z","formattedDate":"March 8, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.785,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-08T09:01","slug":"dynamic-repricing-of-products-using-intelligent-apps-part-2","title":"3.2 Dynamic Repricing of Products Using Intelligent Apps Part 2","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"3.3 Dynamic Repricing of Products Using Intelligent Apps Part 3: Graphing and Displaying Price Forecasts in a Web Interface","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-3"},"nextItem":{"title":"3.1 Dynamic Repricing of Products Using Intelligent Apps Part 1","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/dynamic-repricing-of-products-using-intelligent-apps-part-2\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-2\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-2\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Dynamic Repricing of Products Using Intelligent Apps Part 2: Price Forecasting with AI/ML](../../static/img/60-days-of-ia/blogs/2024-03-08/3-2-1.jpeg)\\r\\n\\r\\n*This three-part series demonstrates how to use Azure Cosmos DB to build an Intelligent App that uses historical pricing and product data to forecast future price fluctuations for specific products. In this installment, you\u2019ll use artificial intelligence and machine learning to build the price forecasting model.*\\r\\n\\r\\n## Dynamic Repricing of Products Using Intelligent Apps Part 2: Price Forecasting with AI/ML\\r\\n\\r\\n[In Part 1 of this series](https://azure.github.io/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1), you set up and populated an [Azure Cosmos DB](https://azure.microsoft.com/free/cosmos-db?ocid=buildia24_60days_blogs) database, laying the groundwork for your Intelligent Application. You also imported your data to a Cosmos DB instance.\\r\\n\\r\\nIn this second article, you\u2019ll use this data alongside Azure\u2019s machine learning (ML) and artificial intelligence (AI) capabilities to build a model that analyzes pricing trends and predicts future prices for a fictional e-commerce business.\\r\\n\\r\\n### Analyzing Price Trends to Predict Future Prices\\r\\n\\r\\nThe ability to forecast pricing is a game-changer. With the power of foresight, businesses can preemptively adjust their pricing strategies in line with market expectations.\\r\\n\\r\\nIn this tutorial, we\u2019ll give you a step-by-step guide to generating a predictive ML model for an e-commerce business, using Azure\u2019s suite of ML tools.\\r\\n\\r\\n#### Prerequisites\\r\\n\\r\\nBefore you begin, make sure you have the following:\\r\\n\\r\\n- An active [Azure Account](https://azure.microsoft.com/free/?ocid=buildia24_60days_blogs)\\r\\n- A Cosmos DB instance with the [pricing data](https://www.kaggle.com/datasets/sujaykapadnis/price-quote-data/data) you set up in Part 1\\r\\n- Access to [Azure Machine Learning Studio](https://studio.azureml.net/)\\r\\n- An [Azure Machine Learning workspace](https://learn.microsoft.com/azure/machine-learning/tutorial-azure-ml-in-a-day?view=azureml-api-2&ocid=buildia24_60days_blogs)\\r\\n- A [Jupyter notebook set up](https://learn.microsoft.com/azure/machine-learning/quickstart-create-resources?view=azureml-api-2#create-a-new-notebook&ocid=buildia24_60days_blogs) in your workspace\\r\\n- Familiarity with [Azure Machine Learning](https://azure.microsoft.com/products/machine-learning?ocid=buildia24_60days_blogs) concepts\\r\\n- Basic Python programming knowledge and understanding of ML concepts\\r\\n\\r\\n**Note**: You should add and run all code in this article into your Jupyter Notebook in the order in which it appears.\\r\\n\\r\\n:::info\\r\\nCheck out the Azure **[Cosmos DB Ask The Expert](https://aka.ms/intelligent-apps/ate-cosmos?ocid=buildia24_60days_blogs)** session to learn how to build RAG solutions, manage chat history by seamlessly connecting with *Azure OpenAI*, as well as explore the power of *Azure Cosmos DB\'s copilot*. The experts will also cover how to seamlessly integrate your operational and transactional data with AI frameworks and sdks like Semantic Kernel, Langchain, and LlamaIndex. \\r\\n:::\\r\\n\\r\\n#### Extract Historical Pricing Data from Cosmos DB\\r\\n\\r\\nStart by extracting historical pricing data from Cosmos DB, where you stored it in Part 1. For this tutorial, you\u2019ll extract items with names ending in `JACKET`. Because the dataset is relatively small, a simple `like` query will do. However, when working with larger data sets, you should consider additional upfront data cleaning and categorizing, to ensure you can query your database efficiently.\\r\\n\\r\\nRun the code below to extract the data:\\r\\n\\r\\n```\\r\\nfrom azure.cosmos import CosmosClient, exceptions\\r\\nimport pandas as pd\\r\\n```\\r\\n```\\r\\n# Initialize a Cosmos client\\r\\nendpoint = \\"your_cosmos_db_endpoint\\"\\r\\nkey = \'your_cosmos_db_key\'\\r\\nclient = CosmosClient(endpoint, key)\\r\\n```\\r\\n```\\r\\n# Connect to the database and container\\r\\ndatabase_name = \'your_database_name\'\\r\\ncontainer_name = \'your_container_name\'\\r\\ndatabase = client.get_database_client(database_name)\\r\\ncontainer = database.get_container_client(container_name)\\r\\n```\\r\\n```\\r\\n# Query these items using the SQL query syntax\\r\\nquery = \\"SELECT * FROM c where ITEM_DESC like \'%JACKET\'\\"\\r\\nitems = list(container.query_items(query=query, enable_cross_partition_query=True))\\r\\n```\\r\\n```\\r\\n# Convert the query result to a DataFrame\\r\\npricing_data = pd.DataFrame(items)\\r\\n```\\r\\n\\r\\n#### Preprocess Data and Split into Training and Testing\\r\\n\\r\\nBefore feeding the data into an ML model, preprocess it and split it into training and testing sets using the code below:\\r\\n\\r\\n```\\r\\nfrom sklearn.model_selection import train_test_split\\r\\n```\\r\\n```\\r\\n# Assume the DataFrame `pricing_data` has columns: \'quote_date\', \'price\', \'price_relative\', \'item_id\', etc.\\r\\n```\\r\\n```\\r\\n# Convert \'quote_date\' from string to datetime for proper chronological splitting\\r\\npricing_data[\'QUOTE_DATE\'] = pd.to_datetime(pricing_data[\'QUOTE_DATE\'], format=\'%Y%m\')\\r\\n```\\r\\n```\\r\\n# Selecting the features and target for the model\\r\\nX = pricing_data[[\'QUOTE_DATE\', \'ITEM_ID\', \'PRICE_RELATIVE\',\'STRATUM_WEIGHT\', \'SHOP_WEIGHT\']]\\r\\ny = pricing_data[\'price\']\\r\\n```\\r\\n```\\r\\n# Split the data into training and testing sets\\r\\n# We\'ll use a chronological split rather than a random split to maintain the time series integrity\\r\\nsplit_date = pd.Timestamp(\'YYYY-MM-DD\')  # replace with the appropriate date\\r\\ntrain = pricing_data.loc[pricing_data[\'QUOTE_DATE\'] <= split_date]\\r\\ntest = pricing_data.loc[pricing_data[\'QUOTE_DATE\'] > split_date]\\r\\n```\\r\\n```\\r\\nX_train, y_train = train[[\'ITEM_ID\', \'PRICE_RELATIVE\', \'STRATUM_WEIGHT\', \'SHOP_WIGHT\']], train[\'PRICE\']\\r\\nX_test, y_test = test[[\'ITEM_ID\', \'PRICE_RELATIVE\', \'STRATUM_WEIGHT\', \'SHOP_WEIGHT\']], test[\'PRICE\']\\r\\n```\\r\\n\\r\\n#### Train a Forecasting Model Using Azure Machine Learning\\r\\n\\r\\nNext, you\u2019ll build and train the forecasting model using Azure Machine Learning. Note that in the code below, you\u2019re using a local compute target, which works on simple datasets like the one used for this tutorial. However, Azure Machine Learning offers more powerful compute targets for more complex models.\\r\\n\\r\\n```\\r\\nfrom azureml.core import Workspace, Experiment, Environment\\r\\nfrom azureml.train.automl import AutoMLConfig\\r\\n```\\r\\n```\\r\\n# Connect to your Azure ML workspace\\r\\nws = Workspace.from_config()\\r\\n```\\r\\n```\\r\\n# Define your experiment\\r\\nexperiment_name = \'price_forecasting_experiment\'\\r\\nexperiment = Experiment(ws, experiment_name)\\r\\n```\\r\\n```\\r\\n# Configure the automated ML job \\r\\n\\r\\nautoml_config = AutoMLConfig(\\r\\n    task=\'forecasting\',\\r\\n    primary_metric=\'normalized_root_mean_squared_error\',\\r\\n    experiment_timeout_minutes=30,\\r\\n    training_data=train,\\r\\n    label_column_name=\'PRICE\',\\r\\n    n_cross_validations=5,\\r\\n    enable_early_stopping=True,\\r\\n    verbosity=logging.INFO,\\r\\n    compute_target=\'local\'\\r\\n) \\r\\n```\\r\\n```\\r\\n# Submit the experiment\\r\\nrun = experiment.submit(automl_config, show_output=True)\\r\\n```\\r\\n\\r\\n#### Evaluate and Integrate the Model\\r\\n\\r\\nNext, check the results of the model by running the following:\\r\\n\\r\\n```\\r\\nfrom azureml.widgets import RunDetails\\r\\n```\\r\\n```\\r\\n# Show run details while running\\r\\nRunDetails(run).show()\\r\\n```\\r\\n```\\r\\n# Wait for the run to complete\\r\\nrun.wait_for_completion()\\r\\n```\\r\\n```\\r\\n# Retrieve the best model from the AutoML run\\r\\nbest_run, fitted_model = run.get_output()\\r\\nprint(best_run)\\r\\nprint(fitted_model)\\r\\n```\\r\\n```\\r\\n# Evaluate the best model\'s accuracy using the test data\\r\\n# Assuming test data is a Pandas DataFrame with the same structure as the training data\\r\\nX_test = test_data.drop(\'PRICE\', axis=1)  # Features (drop the target column)\\r\\ny_test = test_data[\'PRICE\']  # True values of the target column\\r\\n```\\r\\n```\\r\\n# Predict using the fitted model\\r\\ny_pred = fitted_model.predict(X_test)\\r\\n```\\r\\n```\\r\\n# Calculate the accuracy or any other performance metrics\\r\\nfrom sklearn.metrics import mean_squared_error, r2_score\\r\\nmse = mean_squared_error(y_test, y_pred)\\r\\nr2 = r2_score(y_test, y_pred)\\r\\n```\\r\\n```\\r\\nprint(f\\"Mean Squared Error: {mse}\\")\\r\\nprint(f\\"R-squared: {r2}\\")\\r\\n```\\r\\n\\r\\nWith the performance metrics calculated, you can now determine whether the model\u2019s predictions are accurate enough for your needs. If they are, you can integrate the model with a hypothetical e-commerce platform. The easiest way to integrate a model is to deploy it using an Azure Machine Learning endpoint:\\r\\n\\r\\n```\\r\\nws = Workspace.from_config() \\r\\n```\\r\\n```\\r\\n# Register the model from the best run\\r\\nmodel = best_run.register_model(model_name=\'price_forecast_model\', model_path=\'outputs/model.pkl\') \\r\\n```\\r\\n```\\r\\n# Download the scoring file produced by AutoML\\r\\nbest_run.download_file(\'outputs/scoring_file_v_1_0_0.py\', \'score.py\')\\r\\n```\\r\\n```\\r\\n# Download the environment file produced by AutoML\\r\\nbest_run.download_file(constants.CONDA_ENV_FILE_PATH, \'environment.yml\')\\r\\n```\\r\\n```\\r\\n# Create the environment\\r\\nenv = Environment.from_conda_specification(name=\'forecasting_environment\', file_path=\'environment.yml\')\\r\\n```\\r\\n```\\r\\n# Create the inference configuration\\r\\ninference_config = InferenceConfig(entry_script=\'score.py\', environment=env)\\r\\n```\\r\\n```\\r\\n# Create the deployment configuration\\r\\ndeployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\\r\\n```\\r\\n```\\r\\n# Deploy the model as a web service\\r\\nservice_name = \'price-forecast-service\'\\r\\nservice = Model.deploy(ws, service_name, [model], inference_config, deployment_config) \\r\\nservice.wait_for_deployment(show_output=True)\\r\\n```\\r\\n```\\r\\n# The web service endpoint URL\\r\\nprint(service.scoring_uri)\\r\\n```\\r\\n\\r\\nAnd with that, you\u2019ve deployed your Azure ML endpoint and are ready for Part 3!\\r\\n\\r\\n### Conclusion\\r\\n\\r\\nIn this tutorial, you extracted data from Cosmos DB, preprocessed it, performed a train/test split, initiated a model training pipeline using Azure Machine Learning, and, finally, tested and deployed the model. These are crucial steps to building a system that can intelligently forecast product prices.\\r\\n\\r\\nIn the third and final article of this series, you\u2019ll build a web interface that displays the generated price forecasts using approachable, simple graphs that help businesses easily make data-informed decisions.\\r\\n\\r\\nTo challenge yourself, learn more about Azure\u2019s AI and ML tooling, and put the skills you\u2019ve learned in this tutorial to work, participate in the [Data Cloud Skill Challenge](https://azure.github.io/Cloud-Native/Build-IA/CloudSkills). You can also register for **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days** at the premier conference for cloud-native technologies, *KubeCon EU 2024*."},{"id":"dynamic-repricing-of-products-using-intelligent-apps-part-1","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1","source":"@site/blog-60daysofIA/2024-03-08/dynamic-repricing-of-products-using-intelligent-apps-part-1.md","title":"3.1 Dynamic Repricing of Products Using Intelligent Apps Part 1","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable. In the first article of this series, you\u2019ll set up and populate the Cosmos DB database with data to use in the later parts of the series.","date":"2024-03-08T09:00:00.000Z","formattedDate":"March 8, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":7.14,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-08T09:00","slug":"dynamic-repricing-of-products-using-intelligent-apps-part-1","title":"3.1 Dynamic Repricing of Products Using Intelligent Apps Part 1","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable. In the first article of this series, you\u2019ll set up and populate the Cosmos DB database with data to use in the later parts of the series.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"3.2 Dynamic Repricing of Products Using Intelligent Apps Part 2","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-2"},"nextItem":{"title":"3. Cosmos DB and Intelligent Apps: A Match Made for Innovation","permalink":"/Cloud-Native/60DaysOfIA/cosmos-db-and-intelligent-apps-a-match-made-for-innovation"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/dynamic-repricing-of-products-using-intelligent-apps-part-1\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable. In the first article of this series, you\u2019ll set up and populate the Cosmos DB database with data to use in the later parts of the series.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-1\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable. In the first article of this series, you\u2019ll set up and populate the Cosmos DB database with data to use in the later parts of the series.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/dynamic-repricing-of-products-using-intelligent-apps-part-1\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Cosmos DB and Intelligent Apps: A Match Made for Innovation](../../static/img/60-days-of-ia/blogs/2024-03-08/3-1-1.jpeg)\\r\\n\\r\\n*This three-part series demonstrates how to use Azure Cosmos DB to build an Intelligent App that uses historical pricing and product data to forecast future price fluctuations for specific products. In the first article of this series, you\u2019ll set up and populate the Cosmos DB database with data to use in the later parts of the series.*\\r\\n\\r\\n## Dynamic Repricing of Products Using Intelligent Apps Part 1: Setting Up and Populating Cosmos DB with Data\\r\\n\\r\\nIntelligent Apps leverage data and artificial intelligence (AI) to provide smart, personalized, and adaptive experiences for users. AI and machine learning (ML) techniques like natural language processing (NLP), computer vision, and deep learning help understand context, intent, and user preferences to deliver relevant and timely insights and actions.\\r\\n\\r\\nSome examples of Intelligent Apps include:\\r\\n\\r\\n- **Virtual assistants**\u2014Interactive applications that understand and execute user commands\\r\\n- **Chatbots**\u2014Automated messaging systems that provide information or assistance\\r\\n- **Recommendation** systems\u2014Algorithms that suggest relevant items based on user preferences and behavior\\r\\n\\r\\nIn this three-part series, you\u2019ll create an Intelligent App powered by Azure Cosmos DB and AI/ML capabilities that dynamically suggests changes to product prices based on demand and historical trends. This app will help optimize revenue and customer satisfaction by adjusting product prices according to market conditions and customer behavior.\\r\\n\\r\\n### Laying the Groundwork for an Intelligent App with Cosmos DB\\r\\n\\r\\nFirst, you\u2019ll set up an Azure Cosmos DB database and populate it with product data and historical information about sales and demand. In part 2, you\u2019ll analyze this data using AI and ML to forecast and suggest price changes.\\r\\n\\r\\n#### Prerequisites\\r\\n\\r\\nTo follow this tutorial, ensure you have the following:\\r\\n\\r\\n- [An Azure account](https://azure.microsoft.com/free/?ocid=buildia24_60days_blogs)\\r\\n- A [Kaggle account](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2F) to download the [dataset](https://www.kaggle.com/datasets/sujaykapadnis/price-quote-data/data) this tutorial uses\\r\\n\\r\\n#### Create an Azure Cosmos DB Account\\r\\n\\r\\nAzure Cosmos DB is a fully managed multi-model database that ensures fast access to data, easy scalability, reliable uptime, and strong data consistency. Cosmos DB supports various data models and APIs, including SQL, MongoDB, Cassandra, Gremlin, and table storage, making it easy to query and manipulate data using familiar tools and languages.\\r\\n\\r\\nAlthough you already have an Azure account, you also need to create an Azure Cosmos DB account by following the steps below:\\r\\n\\r\\n1. Sign in to the [Azure portal](https://portal.azure.com/).\\r\\n\\r\\n2. Click **Create a resource** on the upper-left side of the page.\\r\\n\\r\\n3. Search for \u201cAzure Cosmos DB\u201d and select it. On the **Azure Cosmos DB** page, select **Create**.\\r\\n\\r\\n4. Enter the settings for your new account:\\r\\n\\r\\n    - Select your desired subscription.\\r\\n\\r\\n    - Create a new resource group or select an existing one if you have one you\u2019d like to use.\\r\\n\\r\\n    - Enter a unique account name.\\r\\n\\r\\n    - Select **SQL (Core)** as the API. This is the default API for Azure Cosmos DB and allows you to use SQL syntax to query and manage your data.\\r\\n\\r\\n    - Select a **Location** for the account.\\r\\n\\r\\n    - Click **Review + create**.\\r\\n\\r\\n5. Review your account settings and click **Create** to create the account.\\r\\n\\r\\n:::info\\r\\nComplete the **[Data Skills Challenge](https://aka.ms/intelligent-apps/data-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n#### Create a Database and a Container\\r\\n\\r\\nNext, you\u2019ll create a database and container within Azure Cosmos DB. Databases facilitate management, billing, and scaling, while a container is a schema-agnostic grouping of items (documents) with a partition key and a provisioned throughput. The partition property determines how the data is distributed across physical partitions for scalability and performance.\\r\\n\\r\\nTo create a database and container, follow the steps below:\\r\\n\\r\\n1. From the Azure portal, navigate to your Azure Cosmos DB account and select **Data Explorer** on the left menu. In the **Data Explorer**, select **New Database** on the top menu.\\r\\n\\r\\n2. In the **Add Database** panel, enter a name for the new database, like \u201cProductsDB.\u201d\\r\\n\\r\\n3. Check **Provision database throughput** if you want to enable shared throughput for the database. This shares the throughput (RU/s) you provision among all containers in the database. You can also activate or deactivate autoscale, which automatically adjusts the throughput based on your application\u2019s usage patterns.\\r\\n\\r\\n4. Select **OK** to create the database.\\r\\n\\r\\n5. In **Data Explorer**, expand the **ProductsDB** database and select **New Container** on the top menu. Then, open the **Add Container** panel and create a new container:\\r\\n\\r\\n    - Enter \u201cProducts\u201d as the container name.\\r\\n\\r\\n    - Enter \u201c/ITEM_ID\u201d as the container\u2019s partition key. This will [partition](https://learn.microsoft.com/en-us/azure/cosmos-db/partitioning-overview) the data by its `ITEM_ID` property, since columns with a wide range of values make excellent partition keys.\\r\\n\\r\\n    - Use the default value of 400 throughput units. If you\u2019d like, you can also deactivate autoscale for the container.\\r\\n\\r\\n6. Select **OK** to create the container.\\r\\n\\r\\n#### Populate the Container\\r\\n\\r\\nNow that you\u2019ve created your database and container, you need to populate them with some data. For this demonstration, you\u2019ll use a CSV file that contains [UK inflation data](https://www.ons.gov.uk/economy/inflationandpriceindices/datasets/consumerpriceindicescpiandretailpricesindexrpiitemindicesandpricequotes). The dataset contains over 100,000 rows of data representing 600 products sold in UK shops over 12 months.\\r\\n\\r\\nTo populate the container with this data, follow these steps:\\r\\n\\r\\n1. Download the [CSV file](https://www.kaggle.com/datasets/sujaykapadnis/price-quote-data/data).\\r\\n\\r\\n2. In the Azure portal, navigate to your Azure Cosmos DB account and select **Data Explorer** on the left menu.\\r\\n\\r\\n3. In **Data Explorer**, expand the **ProductsDB** database and the **Products** container, and select **Items**.\\r\\n\\r\\n##### *Upload the CSV File*\\r\\n\\r\\nNow, upload the CSV file:\\r\\n\\r\\n1. From the top menu, select **Upload Item**.\\r\\n\\r\\n2. In the **Upload Item** panel, select **Browse**, and choose the CSV file you downloaded previously.\\r\\n\\r\\n3. Select **Upload** to upload the file to the container.\\r\\n\\r\\n4. After the upload finishes, you should see the items in the container, each representing a row in the CSV file. You can select an item to view its properties and values in JSON format.\\r\\n\\r\\n##### *Verify the Data in the Container*\\r\\n\\r\\nTo verify that the data in the container is correct and consistent, you can use the SQL query editor in the Data Explorer.\\r\\n\\r\\n1. Select **New SQL Query**.\\r\\n\\r\\n2. The query editor lets you execute SQL queries against the data in the container. For example, run the following query to get the container\u2019s item count:\\r\\n\\r\\n    ```SELECT VALUE COUNT(1) FROM c```\\r\\n\\r\\n    You should get a result of `10000`, which matches the number of rows in the CSV file.\\r\\n\\r\\n  3. You can also run queries to check the data quality and integrity, like the following:\\r\\n      - **Get the distinct values of ITEM_ID** \u2014 `SELECT DISTINCT VALUE c.ITEM_ID FROM c`\\r\\n      \\r\\n      - **Get the average price of each product** \u2014 `SELECT c.ITEM_ID, c.ITEM_DESC, AVG(c.PRICE) AS avg_price FROM c GROUP BY c.ITEM_ID, c.ITEM_DESC`\\r\\n      - **Get the price trend of a product over time** \u2014 `SELECT c.QUOTE_DATE, c.PRICE FROM c WHERE c.ITEM_ID = \'210102\' ORDER BY c.QUOTE_DATE`\\r\\n4. You can also use the built-in charts to visualize the query results. In the top-right corner of the query editor, select **Chart** and choose the chart type you want to use, such as line, bar, or pie.\\r\\n\\r\\n### Next Steps\\r\\n\\r\\nIn this article, you configured an Azure Cosmos DB database and populated it with data about product price changes. You also verified the data in the container using SQL queries and charts.\\r\\n\\r\\nIn the [next part of the series](https://azure.github.io/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-2), you\u2019ll learn how to use Azure\u2019s AI and ML capabilities to analyze the data and suggest product price forecasts. \\r\\n\\r\\nIf you want to challenge yourself and learn more about Azure, Cosmos DB, and AI/ML, we encourage you to participate in the **[Data Cloud Skill Challenge](https://azure.github.io/Cloud-Native/Build-IA/CloudSkills)**. You can also register for **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days** at the premier conference for cloud-native technologies, *KubeCon EU 2024*."},{"id":"cosmos-db-and-intelligent-apps-a-match-made-for-innovation","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/cosmos-db-and-intelligent-apps-a-match-made-for-innovation","source":"@site/blog-60daysofIA/2024-03-07/cosmos-db-and-intelligent-apps-a-match-made-for-innovation.md","title":"3. Cosmos DB and Intelligent Apps: A Match Made for Innovation","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","date":"2024-03-07T09:00:00.000Z","formattedDate":"March 7, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":5.8,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-07T09:00","slug":"cosmos-db-and-intelligent-apps-a-match-made-for-innovation","title":"3. Cosmos DB and Intelligent Apps: A Match Made for Innovation","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"3.1 Dynamic Repricing of Products Using Intelligent Apps Part 1","permalink":"/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1"},"nextItem":{"title":"2.3 Forecasting Energy Usage with Intelligent Apps Part 3","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-3"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/cosmos-db-and-intelligent-apps-a-match-made-for-innovation\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/cosmos-db-and-intelligent-apps-a-match-made-for-innovation\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/cosmos-db-and-intelligent-apps-a-match-made-for-innovation\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Cosmos DB and Intelligent Apps: A Match Made for Innovation](../../static/img/60-days-of-ia/blogs/2024-03-07/3-1.jpeg)\\r\\n\\r\\n## Cosmos DB and Intelligent Apps: A Match Made for Innovation\\r\\n\\r\\nIntelligent Apps represent the next frontier in application development. Merging machine learning (ML), data analytics, and artificial intelligence (AI), Intelligent Apps help drive and automate informed decisions within everyday workflows. These applications can offer predictive insights and personalized experiences by understanding user intent, making predictions, and automating tasks.\\r\\n\\r\\nThe core of Intelligent Apps lies in their ability to harness vast amounts of data, analyze it for patterns, and use these insights to improve decision-making processes, enhance user experiences, and streamline operations.\\r\\n\\r\\n[Azure Cosmos DB](https://azure.microsoft.com/free/cosmos-db?ocid=buildia24_60days_blogs) plays an instrumental role in building these advanced applications. Its scalability, multi-model support, and seamless integration with Azure AI Services make it a solid foundation for Intelligent Apps. Using Cosmos DB, you can manage and analyze large volumes of diverse data worldwide with minimal latency, ensuring the apps you build are intelligent, highly responsive, and globally available. Moreover, the service\u2019s ability to handle real-time data updates and queries empowers Intelligent Apps to deliver dynamic, up-to-the-minute insights and actions.\\r\\n\\r\\nOur three-part series demonstrates how to use Cosmos DB alongside Azure AI Services to create an Intelligent App that forecasts price fluctuations based on historical pricing and product data. In completing this series, you\u2019ll learn why Cosmos DB is an ideal choice for powering such applications\u2014and how it makes building Intelligent Apps accessible and approachable.\\r\\n\\r\\nJoin us as we embark on this journey to unlock the potential of Intelligent Apps with Cosmos DB!\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Data Skills Challenge](https://aka.ms/intelligent-apps/data-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge. \\r\\n:::\\r\\n\\r\\n### Building an Intelligent Forecasting Demo for E-Commerce\\r\\n\\r\\nIn the competitive e-commerce landscape, the ability to adapt pricing in real time based on demand and historical data is a valuable asset. So, this project focuses on developing a forecasting model that leverages AI/ML capabilities to predict future price changes. By integrating this model into your projects, you can enhance your applications with data-driven decision-making tools that respond effectively to market trends.\\r\\n\\r\\nAt the heart of this project is Azure Cosmos DB, chosen for its robust data management and analysis features. Cosmos DB facilitates the handling of large datasets required for accurate forecasting, providing a scalable, globally distributed database environment that supports real-time updates and queries. This capability is crucial for applying AI algorithms to historical price data, enabling the app to generate timely predictions that can inform pricing strategies.\\r\\n\\r\\n### Laying the Groundwork with Cosmos DB\\r\\n\\r\\n[Part 1 of our series](https://azure.github.io/Cloud-Native/60DaysOfIA/dynamic-repricing-of-products-using-intelligent-apps-part-1) starts with the foundation: setting up an Azure Cosmos DB environment tailored for the intelligent forecasting application. We\u2019ll guide you through the initial steps of creating and configuring your Cosmos DB instance to ensure it\u2019s ready to handle the complexities of historical pricing data.\\r\\n\\r\\nThis installment reviews how to populate your database with relevant data that will serve as the backbone for the dynamic repricing model. Once the Cosmos DB environment is established and filled with historical pricing data, you\u2019ll be in a strong position to start leveraging Azure AI Services to analyze this data and predict future price trends.\\r\\n\\r\\nBut the first article isn\u2019t just about setting up a database: It\u2019s about preparing the stage for a sophisticated application that can dynamically adjust e-commerce prices. Through this exercise, you\u2019ll learn the importance of a well-structured data foundation and how it enables the creation of more responsive and intelligent e-commerce platforms.\\r\\n\\r\\n### Analyzing Data with Azure AI Services\\r\\n\\r\\nIn part 2 of this series, the spotlight turns to Azure AI Services. You\u2019ll explore how to harness Azure\u2019s powerful AI capabilities to sift through the dataset, identifying patterns and trends that are key to understanding future price fluctuations.\\r\\n\\r\\nThis stage is all about bridging the gap between raw data and actionable insights, demonstrating how to apply AI capabilities to accurately forecast prices. We\u2019ll walk step-by-step through integrating Azure AI Services with Cosmos DB, helping you create a seamless workflow that brings the dynamic repricing model to life.\\r\\n\\r\\nThis hands-on exploration will equip you with the skills to implement intelligent forecasting within your own e-commerce platforms\u2014something that helps you make data-driven decisions on inventory pricing. By the end of part 2, you\u2019ll have a fully operational forecasting model capable of predicting price trends based on historical data.\\r\\n\\r\\n### Building the Web Interface\\r\\n\\r\\nIn part 3 of this series, you\u2019ll create a simple, yet effective web interface for the Intelligent App. This interface will serve as the window through which you can easily view and interact with the results of the dynamic repricing tool. We\u2019ll guide you through the development process, showcasing how to use popular web technologies to build an interface.\\r\\n\\r\\nThis web interface is critical in making the Intelligent App not just a powerful analytical tool but also a practical solution for e-commerce businesses. By providing a clear and intuitive way to access and understand the pricing forecasts, you can efficiently make informed decisions about pricing.\\r\\n\\r\\nThis final piece of the series ties together all the components of the project and highlights the importance of user experience in the deployment of Intelligent Apps.\\r\\n\\r\\n:::info\\r\\nCheck out the [Azure Cosmos DB Ask The Expert](https://aka.ms/intelligent-apps/ate-cosmos?ocid=buildia24_60days_blogs) session to learn how to build RAG solutions, manage chat history by seamlessly connecting with Azure OpenAI, as well as explore the power of Azure Cosmos DB\'s copilot. The experts will also cover how to seamlessly integrate your operational and transactional data with AI frameworks and sdks like Semantic Kernel, Langchain, and LlamaIndex.\\r\\n:::\\r\\n\\r\\n### Harnessing Cosmos DB for Intelligent Apps\\r\\n\\r\\nIn this exploration of how to build an Intelligent App with Cosmos DB, you\u2019ll have completed a project that showcases the power of Azure services and demonstrates the practical applications of these technologies in forecasting for e-commerce. And by walking through the steps needed to use Cosmos DB alongside Azure AI Services, you\u2019re walking away with a blueprint for building apps that can dynamically adjust pricing based on historical data and market trends.\\r\\n\\r\\nStay tuned for our series to dive deeper into the creation of this forecasting tool. Whether you\u2019re looking to enhance your technical skills or implement intelligent solutions in your own projects, following along will shine light onto the value of using Cosmos DB for Intelligent Apps."},{"id":"forecasting-energy-usage-with-intelligent-apps-3","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-3","source":"@site/blog-60daysofIA/2024-03-05/forecasting-energy-usage-with-intelligent-apps-3.md","title":"2.3 Forecasting Energy Usage with Intelligent Apps Part 3","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","date":"2024-03-05T09:10:00.000Z","formattedDate":"March 5, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":8.315,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-05T09:10","slug":"forecasting-energy-usage-with-intelligent-apps-3","title":"2.3 Forecasting Energy Usage with Intelligent Apps Part 3","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"3. Cosmos DB and Intelligent Apps: A Match Made for Innovation","permalink":"/Cloud-Native/60DaysOfIA/cosmos-db-and-intelligent-apps-a-match-made-for-innovation"},"nextItem":{"title":"2.2 Forecasting Energy Usage with Intelligent Apps Part 2","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/forecasting-energy-usage-with-intelligent-apps-3\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Join us on a learning journey to build intelligent apps on Azure. Read all about the upcoming #BuildIntelligentApps initiative on this post!\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-3\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"This three-part series demonstrates how to create an Intelligent App that forecasts future energy consumption and pricing based on historical data.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-3\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Graphic with a bar chart in a computer-like window in the top-right corner. To the left of the graph is a circle with a lightning bolt in it. At the bottom of the graphic is text that reads, \\"Forecasting Energy Usage with Intelligent Apps: Adding a Website Interface.\\"](../../static/img/60-days-of-ia/blogs/2024-03-05/2-3-1.png)\\r\\n\\r\\n*This three-part series demonstrates how to create an Intelligent App that forecasts future energy consumption and pricing based on historical data. In this final installment of the series, you\u2019ll create a basic web interface that enables the user to input energy usage data and parameters, output the results and the model-generated report into the web interface for easy viewing. Finally, you\u2019ll deploy using the AKS environment set up in Part 1. *\\r\\n\\r\\n## Forecasting Energy Usage with Intelligent Apps Part 3: Adding a Web Interface\\r\\n\\r\\nIn [Part 1 of this series](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1), you set up an [Azure Kubernetes Service](https://azure.microsoft.com/products/kubernetes-service?ocid=buildia24_60days_blogs) (AKS) cluster and prepared it for automated deployment with the help of [Kubernetes AI Toolchain Operator](https://azure.microsoft.com/updates/preview-ai-toolchain-operator-addon-for-aks/?ocid=buildia24_60days_blogs) (KAITO) and Llama 2. Then, in [Part 2](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2), you built a model that predicts future energy usage/pricing based on parameters input by the user and set up the Intelligent App\u2019s back end.\\r\\n\\r\\nIn this third and final article of the series, you\u2019ll create a primary web interface that empowers users to input energy usage data and parameters to generate forecasts. Through this interface, users can gain insights into future energy demands, aiding in strategic decision-making and resource allocation.\\r\\n\\r\\nLet\u2019s dive in!\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nTo follow along, ensure you have:\\r\\n\\r\\n - Completed Parts [1](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1) and [2](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2) of this series\\r\\n - A code editor like [Visual Studio Code](https://code.visualstudio.com/)\\r\\n - Python 3.10 or higher\\r\\n - The new Forecast web app [source code](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/tree/main/Part%202) downloaded\\r\\n\\r\\nFor a sneak peek of the final product, check out the [complete project code](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/tree/main/Part%203).\\r\\n\\r\\n### Building an Intelligent App with Azure Kubernetes Service and KAITO\\r\\n\\r\\nIn this tutorial, you\u2019ll create a basic web interface that enables the user to input or upload energy usage data and parameters to generate future predictions of usage/pricing. Then, you\u2019ll output the results and the report generated from the model into the web interface for easy viewing. Finally, you\u2019ll deploy the Intelligent App using the AKS environment you set up in Part 1.\\r\\n\\r\\n:::info\\r\\nCheckout the **[Intelligent Apps on AKS: Episode 4](https://aka.ms/learn-live-building-intelligent-apps-aks-ep4?ocid=buildia24_60days_blogs)**, a technical deep dive hands-on training with an expert on how to use AKS and Azure to take your intelligent app global.\\r\\n:::\\r\\n\\r\\n#### Creating the Web Interface\\r\\n\\r\\nTo develop your web interface, you\u2019ll use [streamlit](https://streamlit.io/)\u2014a Python framework for creating web apps. This combination offers flexibility and ease of development, enabling seamless data processing and integration of visualization components.\\r\\n\\r\\n*The User Interface*\\r\\n\\r\\nThe core of your web interface is a streamlit form where users can input relevant parameters. The form includes fields for adding data related to electricity generation from different sources. Upon submitting the form, users trigger the prediction process. \\r\\n\\r\\nLocate the directory of the source code you have downloaded and open the [app.py file](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/blob/main/Part%202/app.py). It centralizes the logic needed by the new Forecast app to process user input and produce the price prediction and analysis.\\r\\n\\r\\nFor simplicity, let\u2019s review just the most pertinent parts of the file:\\r\\n\\r\\n```\\r\\n# omitted for brevity \\r\\n\\r\\ndef generate_report(user_input, price):  \\r\\n\\r\\n    # Get the IP address from the environment variable \\r\\n    IP_ADDRESS = os.environ.get(\'ENERGYFORECASTAPI_IP\') \\r\\n\\r\\n    # Endpoint URL \\r\\n    url = f\'http://{IP_ADDRESS}/predict-chat\' \\r\\n\\r\\n    # Request payload \\r\\n\\r\\n    payload = { \\r\\n\\r\\n# omitted for brevity \\r\\n\\r\\n    } \\r\\n\\r\\n    # Header \\r\\n    headers = {\'Content-Type\': \'application/json\'} \\r\\n\\r\\n    # Perform the request \\r\\n    response = requests.post(url, json=payload, headers=headers) \\r\\n\\r\\n    # Check the response \\r\\n    if response.status_code == 200: \\r\\n        print(\\"Response:\\", response.json()) \\r\\n        json_data = response.json() \\r\\n        report = json_data[\'results\'][0][1][\'content\'] \\r\\n        return report \\r\\n    else: \\r\\n        st.header(\'Error\', divider=\'rainbow\') \\r\\n        print(\\"Error:\\", response.text) \\r\\n        return response.text \\r\\n\\r\\ndef get_forecast_price(user_input): \\r\\n    model = load(\'xgb_model.joblib\')        \\r\\n    price = np.float64(model.predict(user_input)[0]) \\r\\n    price = np.around(price, 2) \\r\\n    return price \\r\\n\\r\\nst.title(\\"Predicting Energy Pricing\\") \\r\\nst.write(\\"This Intelligent App analyzes data on energy consumption and predicts the electricity price for the next hour. It then creates a report summarizing the electricity usage and price.\\") \\r\\n\\r\\nwith st.form(\\"my_form\\"): \\r\\n\\r\\n# some parts were omitted for brevity \\r\\n\\r\\n       user_input = [[np.float64(generation_fossil_brown_coal_lignite), np.float64(generation_fossil_gas),  \\r\\n                      np.float64(generation_fossil_hard_coal), np.float64(generation_fossil_oil), \\r\\n                      np.float64(generation_hydro_pumped_storage_consumption), np.float64(generation_other_renewable), \\r\\n                      np.float64(generation_wind_onshore), np.float64(total_load_actual), hour, weekday, month, business_hour, \\r\\n                      weekend]] \\r\\n        \\r\\n       price = get_forecast_price(user_input) \\r\\n\\r\\n       st.header(\'Forecast Price\', divider=\'rainbow\') \\r\\n       st.write(f\\"{str(round(price, 2))} EUR/MW\\") \\r\\n\\r\\n       report = generate_report(user_input, price) \\r\\n\\r\\n       st.header(\'Analysis\', divider=\'rainbow\') \\r\\n       st.write(report) \\r\\n```\\r\\n\\r\\n##### Building and Pushing to ACR\\r\\nOpen your terminal at the root directory of the Forecast app\u2019s source code you downloaded earlier. Run the pair of commands below to initiate and use a Python virtual environment:\\r\\n\\r\\n```\\r\\npython -m venv .env \\r\\n\\r\\n.env\\\\Scripts\\\\activate \\r\\n```\\r\\n\\r\\nThen, run the following command to complete the installation of dependencies of your Python project: \\r\\n\\r\\n```\\r\\npip install -r requirements.txt\\r\\n```\\r\\n\\r\\nExecute the following commands to build the image locally and push it to your Azure Container Registry (ACR). Be sure to replace `<username>` and `<password>` with your username and password.\\r\\n\\r\\n```\\r\\nsudo docker build --no-cache -f Dockerfile -t forecast-web -t <YOUR-ACR-NAME>.azurecr.io/forecast-web:latest . \\r\\n\\r\\ndocker login <YOUR-ACR-NAME>.azurecr.io --username <username> --password-stdin <password> \\r\\n\\r\\ndocker push <YOUR-ACR-NAME>.azurecr.io/forecast-web:latest \\r\\n```\\r\\n\\r\\n##### Connecting to AKS\\r\\n\\r\\nStart by making sure you\u2019re logged in to Azure and that you have the correct AKS credentials by running the following command: \\r\\n\\r\\n```\\r\\naz login --tenant <YOUR-AZURE-TENANT-ID>\\r\\n```\\r\\n\\r\\nNext, run the following commands to enable access to your AKS cluster via your terminal: \\r\\n\\r\\n```\\r\\nexport RESOURCE_GROUP=<YOUR-RESOURCE-GROUP> \\r\\nexport MY_CLUSTER=<YOUR-AKS-CLUSTER-NAME> \\r\\nexport LOCATION=<YOUR-LOCATION> \\r\\nexport SUBSCRIPTION=<YOUR-AZURE-SUBSCRIPTION> \\r\\n\\r\\naz account set --subscription $SUBSCRIPTION \\r\\naz aks get-credentials --resource-group $RESOURCE_GROUP --name $MY_CLUSTER \\r\\n```\\r\\n\\r\\n#### Deployment\\r\\n\\r\\nBefore deploying, you need to retrieve the cluster IP of the Forecast API service running on your AKS cluster. Execute the following command in your terminal:\\r\\n\\r\\n```\\r\\n> kubectl get svc forecast-api-service \\r\\n```\\r\\n\\r\\nCopy the inference service\u2019s cluster IP:\\r\\n\\r\\n```\\r\\nNAME                   TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)        AGE \\r\\nforecast-api-service   LoadBalancer   <CLUSTERIP>  <EXTERNAL-IP>   80:32306/TCP   46h \\r\\n```\\r\\n\\r\\nNext, modify the `deployment.yaml` file using the code below, replacing the `<ENERGY-FORECAST-API-IP>` placeholder below with the Forecast API service\u2019s cluster IP value you copied above:\\r\\n\\r\\n```\\r\\napiVersion: apps/v1 \\r\\nkind: Deployment \\r\\nmetadata: \\r\\n  name: forecast-web-deployment \\r\\nspec: \\r\\n  replicas: 1 \\r\\n  selector: \\r\\n    matchLabels: \\r\\n      app: forecast-web \\r\\n  template: \\r\\n    metadata: \\r\\n      labels: \\r\\n        app: forecast-web \\r\\n    spec: \\r\\n      containers: \\r\\n      - name: forecast-web \\r\\n        image: openaidemoacr.azurecr.io/forecast-web:latest \\r\\n        ports: \\r\\n        - containerPort: 8501 \\r\\n        env: \\r\\n        - name: ENERGYFORECASTAPI_IP \\r\\n          value: <ENERGY-FORECAST-API-IP> \\r\\n```\\r\\n\\r\\nThen, save the updated `deployment.yaml` file.\\r\\n\\r\\nExecute the following commands to provision a new pod and deploy the service to your AKS cluster:\\r\\n\\r\\n```\\r\\nsnap install kubectl --classic \\r\\nkubectl apply -f deployment.yaml \\r\\nkubectl apply -f service.yaml \\r\\n```\\r\\n\\r\\n**Note:** After the deployment commands have been applied, the Forecast web app may take a few minutes to get up and running.\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge. \\r\\n:::\\r\\n\\r\\n##### Running the Web App\\r\\n\\r\\nNow that the Forecast web app is deployed, let\u2019s try it out!\\r\\n\\r\\nRun the command below and grab your app\u2019s external IP:\\r\\n\\r\\n```\\r\\n> kubectl get svc forecast-web-service \\r\\n\\r\\nNAME                   TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)        \\r\\nforecast-web-service   LoadBalancer   10.0.81.68   <EXTERNAL-IP>   80:30805/TCP \\r\\n```\\r\\n\\r\\nNow, paste the `<External-IP>` into a new web browser tab to test your Forecast web app:\\r\\n\\r\\n![Screenshot of the Predicting Energy Pricing app open in a browser.](../../static/img/60-days-of-ia/blogs/2024-03-05/2-3-2.png)\\r\\n\\r\\nFill in the form with the energy fields, plus the date and time, and hit **Submit**.\\r\\n\\r\\nOnce you submit the form, you\u2019ll see predictions for energy prices categorized and a detailed report summarizing the electricity usage and price.\\r\\n\\r\\nOnce the form is submitted, the Forecast web queries the model trained in Part 2 and obtains the forecast price. Then, it accesses the Forecast API service, which is hosted in your AKS cluster, to produce the summary report using the generative capabilities of the Llama2 Chat model:\\r\\n\\r\\n![Screenshot of the results in the Forecast  app. It includes an analysis of generation sources and their respective usage, a total for the actual load, and a price forecast.](../../static/img/60-days-of-ia/blogs/2024-03-05/2-3-3.png)\\r\\n\\r\\n### Why Build Intelligent Apps with KAITO?\\r\\n\\r\\nKAITO provides significant advantages when building an AI project. One key benefit is the drastic reduction in time and effort required to deploy AI models. This is because KAITO automates many complex tasks that traditionally demand significant manual intervention.\\r\\n\\r\\nWithout utilizing KAITO, building an AI project within Kubernetes could present several challenges:\\r\\n\\r\\n - You need to manually handle complex tasks, like provisioning infrastructure resources, deploying models, managing endpoints, and optimizing resource utilization. The manual approach takes substantial time and effort and increases the likelihood of errors and inconsistencies across deployments. \\r\\n - The absence of automated infrastructure provisioning may result in suboptimal resource allocation and higher operational costs. \\r\\n\\r\\nBut with the help of KAITO, you can swiftly deploy hosted models from a variety of open-source repositories or custom models\u2014all without the need for extensive expertise in Kubernetes infrastructure management. Moreover, KAITO facilitates the seamless provisioning of infrastructure resources tailored to the specific requirements of AI workloads, optimizing cost efficiency and operational effectiveness.\\r\\n\\r\\nFor more details, refer to this [Microsoft Ignite presentation](https://www.youtube.com/watch?v=9EvA9gbTS9M&t=676s).\\r\\n\\r\\n### Conclusion\\r\\n\\r\\nIn this article, you created a web interface using Streamlit, Docker, and Kubernetes, allowing users to input data and generate insights into energy usage patterns.\\r\\n\\r\\nAzure technologies provide solutions for reducing carbon footprint and promoting sustainability. The [Carbon Aware Keda Operator](https://github.com/Azure/carbon-aware-keda-operator) is one such innovation designed to reduce the carbon footprint of Kubernetes resources. \\r\\n\\r\\nNow that you\u2019ve had hands-on experience in building an Intelligent App, join the [Cloud Skill Challenges](https://aka.ms/intelligent-apps/csc) and check out the [Ask The Expert session](https://aka.ms/intelligent-apps/ate-aks?ocid=buildia24_60days_blogs) with the AKS product team to keep up with the latest in cloud computing. And don\u2019t forget about the **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days** at KubeCon EU, a great opportunity to network with AKS and Azure experts. Let\u2019s work together to drive innovation!"},{"id":"forecasting-energy-usage-with-intelligent-apps-2","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2","source":"@site/blog-60daysofIA/2024-03-05/forecasting-energy-usage-with-intelligent-apps-2.md","title":"2.2 Forecasting Energy Usage with Intelligent Apps Part 2","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","date":"2024-03-05T09:05:00.000Z","formattedDate":"March 5, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":11.065,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-05T09:05","slug":"forecasting-energy-usage-with-intelligent-apps-2","title":"2.2 Forecasting Energy Usage with Intelligent Apps Part 2","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"2.3 Forecasting Energy Usage with Intelligent Apps Part 3","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-3"},"nextItem":{"title":"2.1 Forecasting Energy Usage with Intelligent Apps Part 1: Laying the Groundwork with AKS, KAITO, and LLaMA","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/forecasting-energy-usage-with-intelligent-apps-2\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\'ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-2\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\'ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-2\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Graphic of a bar chart with a magnifying glass in front of it. To the left of the magnifying glass is a lightning bolt. At the bottom of the graphic is text that reads, \\"Forecasting Energy Usage with Intelligent Apps: Making Predictions.\\"](../../static/img/60-days-of-ia/blogs/2024-03-05/2-2-1.png)\\r\\n\\r\\n*This three-part series demonstrates how to create an Intelligent App that forecasts future energy consumption and pricing based on historical data. In this second article, you\u2019ll build out an app that analyzes historical data on energy consumption to build a forecasting model that forecasts future energy usage/pricing based on parameters input by the user.*\\r\\n\\r\\n## Forecasting Energy Usage with Intelligent Apps Part 2: Making Predictions\\r\\n\\r\\nIn [Part 1 of this series](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1), you set up an [Azure Kubernetes Service](https://azure.microsoft.com/products/kubernetes-service?ocid=buildia24_60days_blogs) (AKS) cluster, installed [Kubernetes AI Toolchain Operator](https://azure.microsoft.com/updates/preview-ai-toolchain-operator-addon-for-aks/?ocid=buildia24_60days_blogs) (KAITO), and configured KAITO with Llama 2. In this article, you\u2019ll use the groundwork from Part 1 to build the Intelligent App.\\r\\n\\r\\nUsing historical data analysis and an open-source dataset, you\u2019ll construct a model capable of predicting future energy usage and pricing with the flexibility of user-defined parameters.\\r\\n\\r\\nLet\u2019s get started!\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nTo follow this tutorial, ensure you:\\r\\n\\r\\n - Completed Part 1 of this series\\r\\n - Have a Jupyter notebook or a preferred Python integrated development environment (IDE), like [Visual Studio Code](https://code.visualstudio.com/), downloaded\\r\\n - [kubectl](https://kubernetes.io/docs/tasks/tools/), the Kubernetes command-line tool, installed \\r\\n - The [Forecast API source code](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/tree/main/Part%202) downloaded\\r\\n\\r\\nFor a sneak peek of the final product, check out the [complete project code](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/tree/main/Part%202).\\r\\n\\r\\n:::info\\r\\nCheckout the [Intelligent Apps on AKS: Episode 3](https://aka.ms/learn-live-building-intelligent-apps-aks-ep3?ocid=buildia24_60days_blogs), a technical deep dive hands-on training with an expert on how OpenCost, Prometheus, and Grafana with AKS can improve intelligent apps. \\r\\n:::\\r\\n\\r\\n### Predicting Energy Consumption and Pricing with an Intelligent App\\r\\n\\r\\nThe Intelligent App will analyze historical data on energy consumption to build a regression model. For this tutorial, you\u2019ll use an open-source [Kaggle dataset](https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather) named \u201cHourly energy demand generation and weather.\u201d\\r\\n\\r\\nAt a high level, you\u2019ll take the following steps to build the Intelligent App:\\r\\n\\r\\n - **Dataset analysis and feature engineering**\u2014You\u2019ll start by examining the dataset. For this tutorial, you want to understand the relationships between energy consumption, pricing, and influencing factors like time. You\u2019ll engineer new features from the raw data. This includes working hours, the day of the week, and the month.\\r\\n - **Model training**\u2014You\u2019ll use XGBoost as a suitable regression model, as it accounts for both time elements and external factors. You\u2019ll train it using the processed Kaggle dataset. The goal is to teach our model to identify patterns and make reliable predictions of electricity prices.\\r\\n - **User input**\u2014The app will let users provide parameters, such as a date, time, amount of electricity generated from burning, and more. The model will use these inputs to generate predictions.\\r\\n - **Report generation**\u2014Llama2 will come into play here. It will help you create a clear and informative report summarizing the user\u2019s input, the model\u2019s predictions, and any insights derived from the analysis.\\r\\n\\r\\n**Note:** To keep this article focused, we\u2019ll assume you have a pre-cleaned dataset with engineered features. If you\u2019d like to see the details of this process, [refer to this notebook](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/blob/main/Part%202/data/predicting-future-energy-pricing.ipynb).\\r\\n\\r\\nWith the steps above completed, you need to split the data into training and testing sets using the code below. Doing so allows you to predict \u201cprice actual\u201d \u2014 the target feature for this tutorial.\\r\\n\\r\\n```\\r\\n# Define the target feature \\r\\ntarget = \'price actual\' \\r\\n\\r\\n# Split data into feature matrix (X) and target vector (y) \\r\\nX, y = df.drop(columns=target), df[target] \\r\\n\\r\\n# Split the data into training and testing sets \\r\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \\r\\n```\\r\\n\\r\\nNext, use the code below to define a function to train your model.\\r\\n\\r\\n```\\r\\ndef train_xgboost_regressor(X_train, y_train, X_test, y_test): \\r\\n    \\"\\"\\" \\r\\n    Train an XGBoost regressor using cross-validation, tune hyperparameters, and evaluate the model. \\r\\n\\r\\n    Parameters: \\r\\n    X_train (DataFrame): The DataFrame containing the training features. \\r\\n    y_train (Series): The Series containing the training target variable. \\r\\n    X_test (DataFrame): The DataFrame containing the testing features. \\r\\n    y_test (Series): The Series containing the testing target variable. \\r\\n\\r\\n    Returns: \\r\\n    float: Mean squared error (MSE) of the model. \\r\\n    float: R-squared (R2) score of the model. \\r\\n    \\"\\"\\" \\r\\n\\r\\n    # Define the XGBoost regressor \\r\\n    xgb_regressor = xgb.XGBRegressor() \\r\\n\\r\\n    # Define parameter grid for hyperparameter tuning \\r\\n    param_grid = { \\r\\n        \'learning_rate\': [0.05, 0.1], \\r\\n        \'max_depth\': [3, 5, 7], \\r\\n        #\'min_child_weight\': [1, 3, 5], \\r\\n        #\'subsample\': [0.6, 0.8, 1.0], \\r\\n        #\'colsample_bytree\': [0.6, 0.8, 1.0], \\r\\n        #\'gamma\': [0, 0.1, 0.2] \\r\\n    }\\r\\n\\r\\n    # Perform GridSearchCV for hyperparameter tuning \\r\\n    grid_search = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid, cv=5, scoring=\'neg_mean_squared_error\', verbose=1) \\r\\n    grid_search.fit(X_train, y_train) \\r\\n\\r\\n    # Get the best parameters \\r\\n    best_params = grid_search.best_params_ \\r\\n\\r\\n    # Initialize XGBoost regressor with best parameters \\r\\n    best_xgb_regressor = xgb.XGBRegressor(**best_params) \\r\\n\\r\\n    # Perform cross-validation \\r\\n    cv_scores = cross_val_score(best_xgb_regressor, X_train, y_train, cv=5, scoring=\'neg_mean_squared_error\') \\r\\n\\r\\n    # Train the XGBoost regressor on the full training set \\r\\n    best_xgb_regressor.fit(X_train, y_train) \\r\\n\\r\\n    # Make predictions on the test set \\r\\n    y_pred = best_xgb_regressor.predict(X_test) \\r\\n\\r\\n    # Save the model \\r\\n    save_model(best_xgb_regressor, \'xgb_model.joblib\') \\r\\n\\r\\n    # Calculate MSE and R2 score \\r\\n    rmse = root_mean_squared_error(y_test, y_pred) \\r\\n    r2 = r2_score(y_test, y_pred) \\r\\n\\r\\n    return rmse, r2 \\r\\n\u202f \\r\\n\\r\\nrmse, r2 = train_xgboost_regressor(X_train, y_train, X_test, y_test) \\r\\nprint(\\"Test RMSE:\\", rmse) \\r\\nprint(\\"Test R2 Score:\\", r2) \\r\\n```\\r\\nThe output of the code snippet above should look like the following: \\r\\n\\r\\n![Screenshot of output code in the terminal. It reads, \\"Fitting 5 folds for each of 6 candidates, totalling 30 fits\\r\\nTest RMSW: 5.738308690044228\\r\\nTest R2 Score: 0.8378464489048971](../../static/img/60-days-of-ia/blogs/2024-03-05/2-2-2.png)\\r\\n\\r\\nAfter fitting the model with cross-validation and hyperparameter tuning, you\u2019ll see an average root mean squared error (RMSE) of approximately 5.74 and an R-squared (R2) score of about 0.84 on the test data. So, the R-squared values show promising performance in predicting energy prices!\\r\\n\\r\\nNext, you\u2019ll create the Forecast API, set up its communication with Llama2, and deploy it onto your AKS cluster.\\r\\n\\r\\n:::info\\r\\nComplete the [Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs) to compete for the leaderboard and earn a Microsoft Learn Badge. \\r\\n:::\\r\\n\\r\\n#### Setting Up the Backend Service\\r\\n\\r\\nIn the sections that follow, you\u2019ll deploy a simple Python service named \u201cForecast API.\u201d This service will have an endpoint called `predict-chat` that will interact with the Llama2 Chat inference service within your AKS cluster. But first, you\u2019ll set up a backend service.  \\r\\n\\r\\nUnzip the [source code](https://github.com/contentlab-io/Microsoft-Forecasting-Energy-Usage-with-Intelligent-Apps/tree/main/Part%202) that accompanies this article. It contains the files necessary to run your Python API using the Flask library, including the following:\\r\\n\\r\\n```\\r\\nName \\r\\n---- \\r\\napp.py \\r\\ndeployment.yaml \\r\\nDockerfile \\r\\nrequirements.txt \\r\\nservice.yaml \\r\\n```\\r\\n\\r\\nOpen the `app.py` file:\\r\\n\\r\\n```\\r\\nfrom flask import Flask, request, jsonify \\r\\nimport os \\r\\nimport requests \\r\\n\\r\\napp = Flask(__name__) \\r\\n\\r\\nGENERATE_ENDPOINT_CHAT = os.environ.get(\'GENERATE_ENDPOINT_CHAT\') \\r\\n\\r\\n@app.route(\'/predict-chat\', methods=[\'POST\']) \\r\\ndef predict_chat(): \\r\\n    try: \\r\\n        data = request.get_json() \\r\\n\\r\\n        gen_fossil_brown_coal = data.get(\'gen_fossil_brown_coal\') or 582.0 \\r\\n        gen_fossil_gas = data.get(\'gen_fossil_gas\') or 5537.0 \\r\\n        gen_fossil_hard_coal = data.get(\'gen_fossil_hard_coal\') or 4039.0 \\r\\n        gen_fossil_oil = data.get(\'gen_fossil_oil\') or 331.0 \\r\\n        gen_hydro = data.get(\'gen_hydro\') or 454.0 \\r\\n        gen_other_renewable = data.get(\'gen_other_renewable\') or 97.0 \\r\\n        gen_wind_onshore = data.get(\'gen_wind_onshore\') or 7556.0 \\r\\n        total_load_actual = data.get(\'total_load_actual\') or 31648.0 \\r\\n        price = data.get(\'price\') or 40.61 \\r\\n        max_gen_len = data.get(\'max_gen_len\') or 1024 \\r\\n        temperature = data.get(\'temperature\') or 0.0 \\r\\n\\r\\n        prompt = f\\"\\"\\"Display the following report table based on user inputs in tabular text format and write a single-paragraph report summarizing the electricity usage and forecast price: \\r\\n\\r\\n        ### Table \\r\\n\\r\\n        Generation from fossil brown coal/lignite: {gen_fossil_brown_coal} MW \\r\\n        Generation from fossil gas: {gen_fossil_gas} MW \\r\\n        Generation from fossil hard coal:  {gen_fossil_hard_coal} MW \\r\\n        Generation from fossil oil: {gen_fossil_oil} MW \\r\\n        Generation from hydro pumped storage: {gen_hydro} MW \\r\\n        Generation from other renewable sources: {gen_other_renewable} MW \\r\\n        Generation from onshore wind: {gen_wind_onshore} MW \\r\\n\\r\\n        ### Totals: \\r\\n        Total actual load: {total_load_actual} MW \\r\\n\\r\\n        Forecast price: {price} EUR/MWh \\r\\n\\r\\n        ### Short analysis: \\r\\n        Please write a short analysis on the data above.\\"\\"\\" \\r\\n\\r\\n        generate_response = requests.post(GENERATE_ENDPOINT_CHAT, json={ \\r\\n            \\"input_data\\": {\\"input_string\\":[[ {\\"role\\": \\"user\\", \\"content\\": prompt}]]}, \\r\\n            \\"parameters\\": {\\"max_gen_len\\": max_gen_len, \\"temperature\\": temperature} \\r\\n        }) \\r\\n\\r\\n        if generate_response.status_code == 200: \\r\\n            return generate_response.json(), 200 \\r\\n        else: \\r\\n            return jsonify({\'error\': \'Failed to invoke generate endpoint\'}), 500 \\r\\n\\r\\n    except Exception as e: \\r\\n        return jsonify({\'error\': str(e)}), 500 \\r\\n\\r\\nif __name__ == \'__main__\': \\r\\n    app.run(debug=True) \\r\\n```\\r\\n\\r\\nLet\u2019s review what the code inside the `app.py` file is doing.\\r\\n - The `app.py` file is an API endpoint for generating a report on electricity usage and forecasted price based on user-provided input data.\\r\\n - When a POST request is received by the `/predict-chat` endpoint, the application extracts the input data from the request, including parameters like generation from different energy sources, total actual load, price, and additional settings like `max_gen_len` and `temperature`. Note that for now, the app sends dummy values. In Part 3, you\u2019ll update your app to take real values from the user and predict the electricity price using the prediction model.\\r\\n - Then, the app constructs a prompt containing the input data and sends a request to another endpoint, specified by the `GENERATE_ENDPOINT_CHAT` environment variable, to generate a response. The response includes the report table in tabular text format and a short data analysis.\\r\\n - If the generation request is successful, the application returns a report produced with the generative capabilities of Llama 2 Chat, which is hosted as an inference service in your AKS cluster.\\r\\n\\r\\n ##### Building and Pushing to ACR\\r\\n\\r\\n Run the following commands to build the image locally and push it to your Azure Container Registry (ACR). Be sure to replace username and password with your `username` and `password`.\\r\\n\\r\\n ```\\r\\nsudo docker build --no-cache -f Dockerfile -t forecast-api -t <YOUR-ACR-NAME>.azurecr.io/forecast-api:latest . \\r\\n\\r\\ndocker login <YOUR-ACR-NAME>.azurecr.io --username <username> --password-stdin <password> \\r\\n\\r\\ndocker push <YOUR-ACR-NAME>.azurecr.io/forecast-api:latest \\r\\n``` \\r\\n\\r\\n##### Connecting to AKS\\r\\nStart by making sure you\u2019re logged in to Azure and that you have the correct AKS credentials by running the following: \\r\\n\\r\\n```\\r\\naz login --tenant <YOUR-AZURE-TENANT-ID> \\r\\n```\\r\\n\\r\\nNext, run the following commands:\\r\\n\\r\\n```\\r\\nexport RESOURCE_GROUP=<YOUR-RESOURCE-GROUP> \\r\\nexport MY_CLUSTER=<YOUR-AKS-CLUSTER-NAME> \\r\\nexport LOCATION=<YOUR-LOCATION> \\r\\nexport SUBSCRIPTION=<YOUR-AZURE-SUBSCRIPTION> \\r\\n\\r\\naz account set --subscription $SUBSCRIPTION \\r\\naz aks get-credentials --resource-group $RESOURCE_GROUP --name $MY_CLUSTER \\r\\n```\\r\\n\\r\\n##### Deployment\\r\\nBefore deploying, you need to retrieve the cluster IP of the Llama2 7B chat workspace running on your AKS cluster. Run the following command in your terminal:\\r\\n\\r\\n```\\r\\n> kubectl get svc \\r\\n```\\r\\n\\r\\nCopy the inference service\u2019s cluster IP:\\r\\n\\r\\n```\\r\\nNAME                       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)            AGE \\r\\nworkspace-llama-2-7b-chat  ClusterIP   <CLUSTERIP>  <none>        80/TCP,29500/TCP   10m \\r\\n```\\r\\n\\r\\nNext, open the source code directory. Modify the `deployment.yaml` file, replacing the `WORKSPACE-LLAMA-CHAT-CLUSTER-IP` placeholder below with your inference service cluster IP you copied above:  \\r\\n\\r\\n```\\r\\napiVersion: apps/v1 \\r\\nkind: Deployment \\r\\nmetadata: \\r\\n  name: forecast-api-deployment \\r\\nspec: \\r\\n  replicas: 1 \\r\\n  selector: \\r\\n    matchLabels: \\r\\n      app: forecast-api \\r\\n  template: \\r\\n    metadata: \\r\\n      labels: \\r\\n        app: forecast-api \\r\\n    spec: \\r\\n      containers: \\r\\n      - name: forecast-api \\r\\n        image: <YOUR-ACR-NAME>.azurecr.io/forecast-api:latest \\r\\n        ports: \\r\\n        - containerPort: 5000 \\r\\n        env: \\r\\n        - name: GENERATE_ENDPOINT_CHAT \\r\\n          value: \\"http://<WORKSPACE-LLAMA-CHAT-CLUSTER-IP>/chat\\" \\r\\n```\\r\\n\\r\\nThen, save the updated `deployment.yaml` file. \\r\\n\\r\\nExecute the following commands to deploy the service to your AKS cluster:\\r\\n\\r\\n```\\r\\nsnap install kubectl --classic \\r\\nkubectl apply -f deployment.yaml \\r\\nkubectl apply -f service.yaml \\r\\n```\\r\\n\\r\\n##### Invoking the service\\r\\nNow that the Forecast API service is deployed, try it out!\\r\\n\\r\\nRun the command below and grab your Forecast API external IP:\\r\\n\\r\\n```\\r\\n> kubectl get svc \\r\\n\\r\\nNAME                                 TYPE           CLUSTER-IP    EXTERNAL-IP   \\r\\nforecast-api-service                 LoadBalancer   <CLUSTER-IP>  <FORECAST-API-IP> \\r\\nworkspace-llama-2-7b-chat            ClusterIP      <CLUSTER-IP>  <none>        \\r\\nworkspace-llama-2-7b-chat-headless   ClusterIP      None          <none>  \\r\\n```\\r\\n\\r\\nNow, run the curl command below to test your Forecast API service. Be sure to replace the `<FORECAST-API-IP>` placeholder with your Forecast API external IP:\\r\\n\\r\\n```\\r\\ncurl --location \'http://<FORECAST-API-IP>/predict-chat\' \\\\ \\r\\n--header \'Content-Type: application/json\' \\\\ \\r\\n--data \'{ \\r\\n    \\"gen_fossil_brown_coal\\": 582.0, \\r\\n    \\"gen_fossil_gas\\": 5537.0, \\r\\n    \\"gen_fossil_hard_coal\\": 4039.0, \\r\\n    \\"gen_fossil_oil\\": 331.0, \\r\\n    \\"gen_hydro\\": 454.0, \\r\\n    \\"gen_other_renewable\\": 97.0, \\r\\n    \\"gen_wind_onshore\\": 7556.0, \\r\\n    \\"total_load_actual\\": 31648.0, \\r\\n    \\"price\\": 40.61, \\r\\n    \\"max_seq_len\\": 0, \\r\\n    \\"max_gen_len\\": 2048, \\r\\n    \\"temperature\\": 0.5 \\r\\n}\' \\r\\n```\\r\\n\\r\\nThe Forecast API service will process the request containing the data from energy sources and price, invoke the inference service, and return the response to the user:\\r\\n\\r\\n```\\r\\n\\"content\\": \\"Based on the user inputs provided, the following is the table of electricity generation and forecast price: \\r\\n\\r\\n| Generation Source | MW | \\r\\n| --- | --- | \\r\\n| Fossil Brown Coal/Lignite | 582.0 | \\r\\n| Fossil Gas | 5537.0 | \\r\\n| Fossil Hard Coal | 4039.0 | \\r\\n| Fossil Oil | 331.0 | \\r\\n| Hydro Pumped Storage | 454.0 | \\r\\n| Other Renewable Sources | 97.0 | \\r\\n| Onshore Wind | 7556.0 | \\r\\n\\r\\nTotal Actual Load | 31648.0 | \\r\\n\\r\\nForecast Price | 40.61 EUR/MWh | \\r\\n\\r\\nBased on the data provided, the electricity generation from various sources shows a predominance of fossil fuels, with fossil gas being the largest contributor at 5537.0 MW, followed by fossil hard coal at 4039.0 MW, and fossil brown coal/lignite at 582.0 MW. Onshore wind is the largest renewable source of electricity generation at 7556.0 MW.\\r\\n```\\r\\n\\r\\nThat\u2019s it! Note how the generative capabilities of Llama2 7B chat model can transform the cold numbers of your input into an intelligent analysis. \\r\\n\\r\\n## Next Steps\\r\\n\\r\\nIn this second installment of the series, you analyzed historical data, used a dataset to construct a model capable of predicting future energy pricing, and used LLaMA2 to generate a report on the energy usage. Continue to Part 3, where you\u2019ll build a basic web interface for the Intelligent App, display the report generated by model, and deploy the app. \\r\\n\\r\\nTo keep your learning going, participate in the [Cloud Skill Challenges](https://aka.ms/intelligent-apps/csc), check out the [Ask The Expert session](https://aka.ms/intelligent-apps/ate-aks?ocid=buildia24_60days_blogs) with the AKS product team, and register for **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days** at KubeCon EU to stay abreast of the latest developments in cloud technology."},{"id":"forecasting-energy-usage-with-intelligent-apps-1","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1","source":"@site/blog-60daysofIA/2024-03-05/forecasting-energy-usage-with-intelligent-apps-1.md","title":"2.1 Forecasting Energy Usage with Intelligent Apps Part 1: Laying the Groundwork with AKS, KAITO, and LLaMA","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","date":"2024-03-05T09:00:00.000Z","formattedDate":"March 5, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.14,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-03-05T09:00","slug":"forecasting-energy-usage-with-intelligent-apps-1","title":"2.1 Forecasting Energy Usage with Intelligent Apps Part 1: Laying the Groundwork with AKS, KAITO, and LLaMA","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this series, you\u2019ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"2.2 Forecasting Energy Usage with Intelligent Apps Part 2","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2"},"nextItem":{"title":"2. Power Up: Crafting an Intelligent Energy Forecasting Application Using Azure Kubernetes Service","permalink":"/Cloud-Native/60DaysOfIA/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/forecasting-energy-usage-with-intelligent-apps-1\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this series, you\'ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-1\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this series, you\'ll create an Intelligent App powered by Azure Kubernetes Service (AKS) to forecast energy usage and cost.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/forecasting-energy-usage-with-intelligent-apps-1\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![Forecasting Energy Usage with Intelligent Apps: Laying the Groundwork with AKS, KAITO, and LLaMA](../../static/img/60-days-of-ia/blogs/2024-03-05/2-1-1.jpg)\\r\\n\\r\\n*This three-part series demonstrates how to create an Intelligent App that forecasts future energy consumption and pricing based on historical data. In this first article, you\u2019ll set up an Azure Kubernetes Service (AKS) environment, install KAITO, and set up KAITO to work with the LLaMA 2 model.*\\r\\n\\r\\n## Forecasting Energy Usage with Intelligent Apps Part 1: Laying the Groundwork with AKS, KAITO, and LLaMA\\r\\n\\r\\nIntelligent Apps leverage artificial intelligence (AI) and machine learning (ML) technologies to enhance traditional applications with advanced capabilities. They enable businesses to make smarter decisions, automate tasks, and drive innovation by extracting actionable insights from vast amounts of data.\\r\\n\\r\\nIn this series, you\u2019ll create an Intelligent App powered by [Azure Kubernetes Service](https://azure.microsoft.com/en-ca/products/kubernetes-service) (AKS) to forecast energy usage and cost. Each article will demonstrate the use of core Azure technologies, particularly AKS, to build an application that generates forecasts based on AI capabilities applied to user input and historical data analysis.\\r\\n\\r\\nLet\u2019s get started!\\r\\n\\r\\n### Prerequisites\\r\\n\\r\\nTo follow this tutorial, ensure you have the following:\\r\\n\\r\\n- An [Azure Subscription](https://azure.microsoft.com/en-us/free/search) that supports the GPU-enabled [Standard_NC12s_v3 instance type](https://learn.microsoft.com/en-us/azure/virtual-machines/ncv3-series) in the selected region. You might need to request an increase in vCPU quota.\\r\\n - Basic understanding of [AKS](https://azure.microsoft.com/en-us/products/kubernetes-service) and Kubernetes\\r\\n\\r\\n### Building an Intelligent App with Azure Kubernetes Service and KAITO\\r\\n\\r\\nThis first article walks you through setting up an AKS environment and the Kubernetes AI Toolchain Operator (KAITO) to automate AI/ML model deployment in the AKS cluster.\\r\\n\\r\\n#### Downloading the LLaMA 2 Model\\r\\n\\r\\nA fundamental piece in your Intelligent App\u2019s architecture is the target model. Here, you\u2019ll use LLaMA 2, an open-source project developed by Meta in partnership with Microsoft.\\r\\n\\r\\nLLaMA 2 is a large-scale training and inference framework for ML models. It provides a distributed computing infrastructure that enables executing ML tasks across multiple nodes or clusters, using parallelism and optimization techniques to improve performance.\\r\\n\\r\\nTo configure your model, download LLaMA 2 by following the instructions in [this document](https://github.com/Azure/kaito/tree/main/presets/models/llama2).  Ensure you download the LLaMA 2 7B Chat (llama2-7b-chat) model.\\r\\n\\r\\n#### Configuring the AKS Cluster and KAITO\\r\\n\\r\\n![engergy-usage-aks model](../../static/img/60-days-of-ia/blogs/2024-03-05/2-1-2.jpg)\\r\\n\\r\\nCreating an AKS environment is the first step for onboarding large AI inference models onto Kubernetes. Later, you\u2019ll integrate the node provisioner controller with AKS APIs, letting you dynamically add GPU nodes to the cluster to promote scalability and optimal resource use.\\r\\n\\r\\nAdditionally, AKS facilitates testing service endpoints within the cluster, providing a reliable environment for validating and fine-tuning AI inference services.\\r\\n\\r\\nKAITO is an open-source operator that transforms how you deploy AI models on Kubernetes. It streamlines the process, automating critical tasks like infrastructure provisioning and resource optimization. It intelligently selects the optimal hardware configuration for your specific model, using available CPU and GPU resources on AKS. KAITO eliminates the manual setup complexities, accelerating your deployment time and reducing associated costs.\\r\\n\\r\\nTo set up an AKS cluster and install KAITO, follow [this tutorial](https://github.com/Azure/kaito/blob/main/docs/installation.md), adjusting the KAITO installation steps to match the **llama2-7b** model you downloaded earlier.\\r\\n\\r\\n:::info\\r\\nCheckout the **[Intelligent Apps on AKS: Episode 2](https://aka.ms/learn-live-building-intelligent-apps-aks-ep2?ocid=buildia24_60days_blogs)**, a hands-on training with an expert on how to use AKS to run your own AI Models with KAITO.\\r\\n:::\\r\\n\\r\\n#### Pushing LLaMA 2 Chat Model to Azure Container Registry\\r\\n\\r\\nNow that you have AKS with the KAITO installation, you need to push the local model image to the AKS cluster.\\r\\n\\r\\nCreate an Azure Container Registry (ACR) resource using Azure CLI with the following command, replacing `<YOUR-ACR-NAME>` with a new ACR name:\\r\\n\\r\\n```\\r\\naz acr create --name <YOUR-ACR-NAME> --resource-group $RESOURCE_GROUP --sku Standard --location $LOCATION\\r\\n```\\r\\n\\r\\nNow, push your local LLaMA 2 model\u2019s Docker image to the ACR hosted at `<YOUR-ACR-NAME>.azurecr.io` by running:\\r\\n\\r\\n```\\r\\ndocker push <YOUR-ACR-NAME>.azurecr.io/llama2_7b_chat_model:latest\\r\\n```\\r\\n\\r\\nFinally, run the command to update the AKS cluster to attach it to your ACR, allowing the cluster to pull the model container image from `<YOUR-ACR-NAME>.azurecr.io`:\\r\\n\\r\\n```\\r\\naz aks update -g $RESOURCE_GROUP -n $MY_CLUSTER --attach-acr <YOUR-ACR-NAME>\\r\\n```\\r\\n\\r\\n#### Starting the Inference Service\\r\\n\\r\\nAfter installing KAITO, run the following command to start a `llama-2-7b` inference service, replacing `<YOUR-ACR-NAME>` with the ACR name you created previously:\\r\\n\\r\\n```\\r\\n$ cat examples/kaito_workspace_llama2_7b.yaml\\r\\napiVersion: kaito.sh/v1alpha1\\r\\nkind: Workspace\\r\\nmetadata:\\r\\n  name: workspace-llama-2-7b\\r\\nresource:\\r\\n  instanceType: \\"Standard_NC12s_v3\\"\\r\\n  labelSelector:\\r\\n    matchLabels:\\r\\n      apps: llama-2-7b-chat\\r\\ninference:\\r\\n  preset:\\r\\n    name: \\"llama-2-7b-chat\\"\\r\\n    accessMode: private\\r\\n    presetOptions:\\r\\n      image: <YOUR-ACR-NAME>.azurecr.io/llama2_chat_model:latest\\r\\n      imagePullSecrets:\\r\\n        - energy-usage-secret\\r\\n\\r\\n$ kubectl apply -f examples/kaito_workspace_llama2_7b-chat.yaml\\r\\n```\\r\\n\\r\\nKubernetes uses this YAML code to instantiate a workspace resource with the specified configurations. This enables deploying and managing inference workloads within the cluster.\\r\\n\\r\\nYou can monitor the workspace status by executing the command below. The model deployment has been completed once the `WORKSPACEREADY` column becomes `True`:\\r\\n\\r\\n```\\r\\n$ kubectl get workspace workspace-llama-2-7b-chat\\r\\n| NAME | INSTANCE | RESOURCEREADY | INFERENCEREADY | WORKSPACEREADY | AGE |\\r\\n| workspace-llama-2-7b-chat | Standard_NC12s_v3 | True | True | True | 4d2h |\\r\\n```\\r\\n\\r\\n**Note**: Achieving machine and workspace readiness may take up to 20 minutes.\\r\\n\\r\\nNow, run the command below to find the inference service\u2019s cluster IP:\\r\\n\\r\\n```\\r\\n$ kubectl get svc workspace-llama-2-7b-chat\\r\\n| NAME | TYPE | CLUSTER-IP | EXTERNAL-IP | PORT(S) | AGE |\\r\\n| workspace-llama-2-7b-chat | ClusterIP | <CLUSTERIP> | <none> | 80/TCP,29500/TCP | 4d2h |\\r\\n```\\r\\n\\r\\nFinally, run a curl pod to test the service endpoint in the cluster:\\r\\n\\r\\n```\\r\\nexport CLUSTERIP=$(kubectl get svc workspace-llama-2-7b-chat -o jsonpath=\\"{.spec.clusterIPs[0]}\\")\\r\\n\\r\\n$ kubectl run -it --rm --restart=Never curl --image=curlimages/curl -- curl -X POST http://$CLUSTERIP/generate -H \\"accept: application/json\\" -H \\"Content-Type: application/json\\" -d \\"{\\\\\\"input_data\\\\\\": {\\\\\\"input_string\\\\\\":[[ {\\\\\\"role\\\\\\": \\\\\\"user\\\\\\", \\\\\\"content\\\\\\": \\\\\\"What is the capital of India?\\\\\\"}]]}, \\\\\\"parameters\\\\\\": {\\\\\\"max_gen_len\\\\\\": 64, \\\\\\"temperature\\\\\\": 0}}\\"\\r\\n```\\r\\n\\r\\nYou should receive these results:\\r\\n\\r\\n```\\r\\n{\\"results\\":[[{\\"role\\":\\"User\\",\\"content\\":\\"What is the capital of India?\\"},{\\"role\\":\\"Assistant\\",\\"content\\":\\" The capital of India is New Delhi.\\"}]]}\\r\\n```\\r\\n\\r\\n**Note**: You can test with your own questions, but there may be inaccuracies within the response. This is because AKS hasn\u2019t fine-tuned the model for your scenario.\\r\\n\\r\\nThat\u2019s it! You\u2019ve successfully established your AKS environment and familiarized yourself with setting up KAITO to deploy the LLaMA 2 model within your Kubernetes environment. You\u2019re now ready to analyze a model and make predictions using Azure\u2019s AI services.\\r\\n\\r\\n## Next Steps\\r\\n\\r\\nIn this article, you established an AKS cluster and configured KAITO to integrate with the LLaMA 2 model for advanced ML capabilities. In part 2, you\u2019ll use AKS and KAITO to analyze historical energy consumption data with advanced ML models. You\u2019ll create a dynamic web interface for users to input data, generate predictions, and visualize results seamlessly.\\r\\n\\r\\nBe sure to join the [Cloud Skill Challenge](https://azure.github.io/Cloud-Native/Build-IA/CloudSkills) to level up your cloud computing skills and gain hands-on experience. You can also register for the [next episode](https://aka.ms/learn-live-building-intelligent-apps-aks-ep3?ocid=buildia24_60days_blogs) on **Intelligent Apps with Azure Kubernetes Service**, an instructor led live learning experience to deploy your app on AKS. And, join the AKS product and engineering team at *KubeCon EU 2024*\u2014the premier conference for cloud-native technologies, for **AKS [Customer](https://aka.ms/aks-day) and [Lab](https://aka.ms/aks-lab-day) Days**."},{"id":"power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service","source":"@site/blog-60daysofIA/2024-02-27/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service.md","title":"2. Power Up: Crafting an Intelligent Energy Forecasting Application Using Azure Kubernetes Service","description":"In this article, we\u2019ll will guide you through creating an intelligent app that leverages Azure technologies, including Azure Kubernetes Service (AKS), to build an application that forecasts energy usage and pricing.","date":"2024-02-27T09:00:00.000Z","formattedDate":"February 27, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":5.895,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-02-27T09:00","slug":"power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service","title":"2. Power Up: Crafting an Intelligent Energy Forecasting Application Using Azure Kubernetes Service","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"In this article, we\u2019ll will guide you through creating an intelligent app that leverages Azure technologies, including Azure Kubernetes Service (AKS), to build an application that forecasts energy usage and pricing.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"2.1 Forecasting Energy Usage with Intelligent Apps Part 1: Laying the Groundwork with AKS, KAITO, and LLaMA","permalink":"/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1"},"nextItem":{"title":"1. Harnessing the power of Intelligent Apps","permalink":"/Cloud-Native/60DaysOfIA/harnessing-the-power-of-intelligent-apps"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this article, we\u2019ll spotlight the success stories of some organizations that have leveraged Microsoft Azure to create and deploy intelligent apps in their workflows and products.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this article, we\u2019ll spotlight the success stories of some organizations that have leveraged Microsoft Azure to create and deploy intelligent apps in their workflows and products.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\nAt the forefront of recent technological innovation are intelligent apps: apps that use machine learning (ML), artificial intelligence (AI), and data analytics. These apps support smarter, data-driven decisions, making them particularly useful in sectors like energy management, where efficiency and long-term planning are critical.\\r\\n\\r\\nOur upcoming series will guide you through creating an intelligent app that leverages Azure technologies, including [Azure Kubernetes Service](https://azure.microsoft.com/products/kubernetes-service?ocid=buildia24_60days_blogs) (AKS), to build an application that forecasts energy usage and pricing. \\r\\n\\r\\nYour app will harness AKS for hosting and AI to analyze historical energy consumption data. Then, you\u2019ll integrate the Kubernetes AI Toolchain Operator (KAITO) with with XGBoost and LLaMA 2 to build an intelligent app that underscores the importance of green energy practices and demonstrates the versatility and efficacy of Azure services.\\r\\n\\r\\nWe invite you to join us on this three-part educational series, where you\u2019ll learn the skills needed to construct your own intelligent apps. But, this series is more than a technical walkthrough: It\u2019s an opportunity to engage with cutting-edge technologies and contribute to meaningful advancements in energy management.\\r\\n\\r\\nWhether you\u2019re an experienced developer or new to the AI and ML sphere, this series will give you a glimpse into the future of application development and the strategic impact of Azure technologies in driving forward-thinking solutions.\\r\\n\\r\\n## The Synergy of Azure Kubernetes Service and Intelligent Apps\\r\\n\\r\\nUsing AKS as the backbone of intelligent apps has numerous benefits \u2014 especially when deploying your AI-driven application. AKS provides a managed, cloud-based container orchestration service that simplifies deploying, managing, and scaling AI-backed applications, making it ideal for a project like the one you\u2019ll create in this series.\\r\\n\\r\\nOne of the primary advantages of AKS is its ability to handle distributed applications with evolving demands. For AI-driven apps, the ability to scale resources based on computational demands is crucial. Because AKS allows for automatic scaling, intelligent apps have the necessary resources during peak analysis times without wasting resources during quieter periods. But this dynamic scalability isn\u2019t just about handling loads efficiently: It\u2019s also cost-effective, ensuring that you pay only for the resources you use.\\r\\n\\r\\nIntegrating the KAITO operator with AKS further enhances the deployment of AI models like LLaMA 2 by simplifying the complexities of managing AI workloads. KAITO, designed specifically for Kubernetes environments, acts as a bridge between the advanced AI models and the scalable, managed infrastructure provided by AKS. It offers custom resource definitions (CRDs) tailored for AI applications, facilitating the deployment, updating, and management of AI models within the Kubernetes ecosystem.\\r\\n\\r\\nThis seamless integration enables developers to focus more on the application logic and less on the underlying infrastructure, accelerating the development cycle and reducing the time to market for innovative AI solutions.\\r\\n\\r\\nAKS and KAITO create a robust, flexible, and efficient environment for developing and deploying intelligent applications. This combination not only leverages the cloud\u2019s power and scalability but also optimizes the deployment of AI models, making it easier for developers to bring complex, data-driven applications to life.\\r\\n\\r\\n:::info\\r\\nRegister for **[Intelligent Apps on AKS: Episode 2](https://developer.microsoft.com/en-us/reactor/events/21815/?ocid=buildia24_60days_blogs)**, a live hands-on training with an expert on how to use AKS to run your own AI models with KAITO.\\r\\n:::\\r\\n\\r\\n## Laying the Groundwork with Azure Kubernetes Service\\r\\n\\r\\nIn the [first installment of this series](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1), you\u2019ll roll up your sleeves and set up an AKS environment. This step is foundational to the rest of the series, laying the groundwork for deploying and managing your application \u2014 and accessing the full scalability and flexibility that AKS offers.\\r\\n\\r\\nThe article starts with a straightforward step-by-step guide on establishing the AKS environment, ensuring you have a solid base for the exciting journey ahead. This tutorial is succinct to maintain clarity and speedy development, offering links to additional resources for well-documented steps. \\r\\n\\r\\nNext, you\u2019ll meet KAITO, a tool that streamlines deploying AI applications in Kubernetes environments. The core of this article is configuring [the KAITO operator](https://github.com/Azure/kaito) to work seamlessly with the LLaMA 2 model, providing hands-on instructions, code samples, and screenshots to guide you through each step.\\r\\n\\r\\n## Adding Intelligence to the App\\r\\n\\r\\nThe [second part of this series](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-2) dives into the more practical aspects of building the Intelligent App. You\u2019ll leverage an open-source energy dataset alongside powerful tools like XGBoost and a custom Python API to craft a forecasting model that predicts future energy demands with speed and precision.\\r\\n\\r\\nIntegrating these tools with AKS and Azure Container Registry highlights the high-impact relationship between robust data processing capabilities and scalable cloud infrastructure. Hands-on examples and streamlined code will guide you through setting up the environment, processing the dataset, and deploying the forecasting model.\\r\\n\\r\\nThis practical application reinforces the theoretical foundations laid in Part 1 and sets the stage for advanced analytics and AI-driven predictions. As you progress through the tutorial, the focus will remain on simplicity and efficiency, ensuring that even complex AI-related processes become accessible.\\r\\n\\r\\n:::info\\r\\nComplete the **[Intelligent Apps Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs)** to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n## Building a Web Interface\\r\\n\\r\\nAs the concluding installment of our series, [part 3](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-3) assembles all the pieces by introducing a user-friendly web interface. Here, users can input or upload their energy usage data and parameters, after which the Intelligent App will generate future predictions on usage and pricing.\\r\\n\\r\\nThis web front end serves as the direct point of interaction with your AKS-hosted application, seamlessly displaying the reports and predictions the AI model produces.\\r\\n\\r\\nAfter deploying this interface in the AKS environment established in [part 1](https://azure.github.io/Cloud-Native/60DaysOfIA/forecasting-energy-usage-with-intelligent-apps-1), you\u2019ll experience the complete cycle of developing an intelligent, data-driven application and appreciate how straightforward it is to engineer intelligent apps that can deliver tangible, user-centric outcomes.\\r\\n\\r\\n## Ready to Get Started?\\r\\n\\r\\nTogether, these three articles guide you through creating an innovative, AI-driven energy forecasting app. Setting up a scalable AKS environment with integrated cutting-edge AI models, processing open-source energy data for insightful predictions, and deploying a user-friendly web interface will equip you with the tools you need to build your own Intelligent Apps.\\r\\n\\r\\nStay tuned for each part of the series and get ready to dive into the world of Azure, AI, and application development with us. Join us in this exciting venture and harness the power of technology to make a difference. Register for the **[Intelligent Apps on AKS: Episode 2](https://aka.ms/learn-live-building-intelligent-apps-aks-ep2?ocid=buildia24_60days_blogs)**\u202f to experience live hands-on training with an expert on how to use AKS to run your own AI models with KAITO."},{"id":"harnessing-the-power-of-intelligent-apps","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/harnessing-the-power-of-intelligent-apps","source":"@site/blog-60daysofIA/2024-02-19/harnessing-the-power-of-intelligent-apps.md","title":"1. Harnessing the power of Intelligent Apps","description":"Combine the power of AI, cloud-scale data, and cloud-native app development to create highly differentiated digital experiences. Develop adaptive, responsive, and personalized experiences by building and modernizing intelligent applications with Azure.","date":"2024-02-19T09:00:00.000Z","formattedDate":"February 19, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":6.185,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-02-19T09:00","slug":"harnessing-the-power-of-intelligent-apps","title":"1. Harnessing the power of Intelligent Apps","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"Combine the power of AI, cloud-scale data, and cloud-native app development to create highly differentiated digital experiences. Develop adaptive, responsive, and personalized experiences by building and modernizing intelligent applications with Azure.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"2. Power Up: Crafting an Intelligent Energy Forecasting Application Using Azure Kubernetes Service","permalink":"/Cloud-Native/60DaysOfIA/power-up-crafting-an-intelligent-energy-forecasting-application-using-azure-kubernetes-service"},"nextItem":{"title":"Kick-off #60Days of IA","permalink":"/Cloud-Native/60DaysOfIA/kick-off"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/harnessing-the-power-of-intelligent-apps\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"In this article, we\u2019ll spotlight the success stories of some organizations that have leveraged Microsoft Azure to create and deploy intelligent apps in their workflows and products.\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/harnessing-the-power-of-intelligent-apps\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" />\\r\\n <meta name=\\"twitter:description\\" content=\\"In this article, we\u2019ll spotlight the success stories of some organizations that have leveraged Microsoft Azure to create and deploy intelligent apps in their workflows and products.\\" />\\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/harnessing-the-power-of-intelligent-apps\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\n![intelligent apps on Azure](../../static/img/60-days-of-ia/blogs/2024-02-19/harnessing-the-power-of-intelligent-apps.jpg)\\r\\n\\r\\n## Harnessing the Power of Intelligent Apps\\r\\n\\r\\nOrganizations are increasingly adopting advanced technologies to drive innovation and elevate operational efficiency. Intelligent apps\u2014applications that integrate machine learning (ML), data analytics, and predictive or generative artificial intelligence (AI) to create differentiated digital experiences\u2014are one way to achieve this. According to Gartner\xae, \u201cby 2026, 30% of new applications will use AI to drive personalized adaptive user interfaces, up from less than 5% today\u201d<sup>1</sup>.\\r\\n\\r\\nIntelligent apps tend to fall into one of three categories:\\r\\n\\r\\n* **Outcome-based apps** \u2014 These apps focus on user intent, predictions, and task automation to enable better decision-making.\\r\\n* **Functionality-based apps** \u2014 These apps use ML, AI, or APIs to generate content. They examine user patterns to provide personalized recommendations or feedback.\\r\\n* **Feature-based apps** \u2014 These apps have AI or ML components built in, which means they rely on neural networks and internal LLMs to run advanced algorithms.\\r\\n\\r\\nBecause Intelligent apps help organizations leverage business intelligence and other data to drive and automate organizational decision-making, they\u2019re becoming a pivotal part of modern business strategies. In this article, we\u2019ll spotlight the success stories of some organizations that have leveraged Microsoft Azure to create and deploy intelligent apps in their workflows and products.\\r\\n\\r\\n___\\r\\n\\r\\n## Intelligent Apps in Action\\r\\n\\r\\nIntelligent Apps offer tangible business outcomes by automating complex processes, enhancing decision-making, and providing personalized experiences. By leveraging Azure services, the organizations discussed below have experienced a paradigm shift in their operations \u2014 and a boost in productivity and agility.\\r\\n\\r\\n### How Intelligent Apps Power the Ultimate LEGO Experience\\r\\n\\r\\nDenmark\u2019s ultimate LEGO experience center, LEGO House, found challenges in maintaining its on-premises data center. To keep serving its custom-built digital experiences, the business upgraded its facilities with [Azure Kubernetes Service](https://azure.microsoft.com/en-us/products/kubernetes-service/?ocid=buildia24_60days_blogs) (AKS) in 2023.\\r\\n\\r\\nThis shift in approach to the cloud was a boon for responsiveness \u2014 LEGO House could take on visitor feedback to swiftly update experiences and develop new ones. The containerized, component-based setup on AKS also allowed LEGO House\u2019s digital tech stack to become more scalable and flexible, transforming development efficiency and maintenance.\\r\\n\\r\\nLEGO House continued its partnership with Microsoft to launch experiences like City Architect \u2014 powered by [Azure IoT Edge Device](https://azure.microsoft.com/en-us/products/iot-edge?ocid=buildia24_60days_blogs) \u2014 and Robo Lab. These innovations allowed visitors to interact with digitally projected landscapes and build robots, fostering principles of programming. AKS streamlines integration and supports element reuse, enhancing efficiency and creativity.\\r\\n\\r\\nThe results were remarkable \u2014 improved stability, higher uptime, and positive visitor feedback. Azure services made life easier for LEGO House\u2019s developers and gave the entire LEGO ecosystem a strong foundation for growth. Specifically, by allowing the reuse of elements, AKS provides a common foundation for all LEGO experience builds. The organization\u2019s next move is to rebuild the entire House on the [Azure AI](https://azure.microsoft.com/en-us/solutions/ai?ocid=buildia24_60days_blogs) platform and AKS. Read more about how LEGO [modernized interactive experiences across LEGO House with Azure Kubernetes Service](https://customers.microsoft.com/en-us/story/1703088157691224129-lego-house-azure-kubernetes-service-media-and-entertainment-denmark?ocid=buildia24_60days_blogs).\\r\\n\\r\\n### Using Intelligent Apps to Make Cars Smarter With TomTom\\r\\n\\r\\nTomTom\u2019s navigation solutions have a proven track record of innovating the driving experience. Today, the company continues to adapt to drivers\u2019 evolving needs. Using [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-services/openai-service/?ocid=buildia24_60days_blogs), [Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction?ocid=buildia24_60days_blogs), and [AKS](https://azure.microsoft.com/en-us/products/kubernetes-service/?ocid=buildia24_60days_blogs) to develop Digital Cockpit, TomTom has created smarter, AI-powered vehicles, facilitating huge advancements in user experience.\\r\\n\\r\\nDigital Cockpit is an AI-driven, conversational in-car infotainment system that allows drivers to interact naturally with the vehicle. It can perform tasks like navigation, controlling onboard systems, and even managing work tasks during electric vehicle recharging.\\r\\n\\r\\nLet\u2019s look closer at the Azure services that drive Digital Cockpit:\\r\\n\\r\\n* **Azure OpenAI Service** \u2014 The Azure OpenAI Service supports generative AI chatbots that provide natural-sounding voices and accurate transcription of spoken audio.\\r\\n* **Azure Cosmos DB** \u2014 Azure Cosmos DB, a globally distributed database, retains customer conversations and preferences, allowing the system to continuously learn and tailor driver experiences.\\r\\n* **AKS** \u2014 AKS accelerates service deployment and scaling, enhancing overall architecture efficiency.\\r\\n\\r\\nInternally, integrating Azure services resulted in a significant improvement in TomTom\u2019s development efficiency. For example, the team working on the prototype no longer required ten engineers \u2014 three team members were sufficient to complete the task. Additionally, response times decreased from 12 to 2.5 seconds, and the system demonstrated a 95 percent success rate in understanding complex driver requests. Read more about how [TomTom brings AI-powered, talking cars to life with Azure](https://customers.microsoft.com/en-us/story/1723808815413508250-tomtom-azure-netherlands?ocid=buildia24_60days_blogs).\\r\\n\\r\\n:::info\\r\\nTry the [Intelligent Apps Skills Challenges](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs) to compete for the leaderboard and earn a Microsoft Learn Badge.\\r\\n:::\\r\\n\\r\\n### How Gluwa Leverages Intelligent Apps to Make Banking More Accessible\\r\\n\\r\\nSan Francisco-based startup Gluwa is on a mission to address the financial gap for the unbanked and underbanked, estimated at 1.4 billion people globally. Combining blockchain technology and Azure services, Gluwa connects investors with individuals in emerging markets. \\r\\n\\r\\nGluwa harnesses the capabilities of various Azure services to power its blockchain services. [Azure Container Instances](https://azure.microsoft.com/en-us/products/container-instances?ocid=buildia24_60days_blogs) and AKS play a pivotal role in peer-to-peer discovery, fostering a dynamic and efficient environment. [Azure App Configuration](https://azure.microsoft.com/en-us/products/app-configuration?ocid=buildia24_60days_blogs) streamlines the centralization of app configurations, ensuring seamless control and adaptability.\\r\\n\\r\\nFor secure media delivery, Gluwa relies on the powerful combination of [Azure Content Delivery Network](https://azure.microsoft.com/en-us/products/cdn?ocid=buildia24_60days_blogs) and [Azure Storage](https://learn.microsoft.com/en-us/azure/storage/common/storage-introduction?ocid=buildia24_60days_blogs), which bolsters reliability and safeguards sensitive data. Using [Azure DevOps](https://azure.microsoft.com/en-us/products/devops?ocid=buildia24_60days_blogs) to manage intricate lifecycle builds, Gluwa streamlined the development process. [Azure App Services](https://azure.microsoft.com/en-us/products/app-service?ocid=buildia24_60days_blogs) serve as the backbone for Gluwa\u2019s APIs, complemented by [Azure Redis](https://azure.microsoft.com/en-us/products/cache?ocid=buildia24_60days_blogs) for optimal cache distribution, to enhance overall performance. Finally, [Azure SQL](https://azure.microsoft.com/en-us/products/azure-sql/database?ocid=buildia24_60days_blogs) and Azure Cosmos DB are scalable database solutions that support Gluwa\u2019s infrastructure, ensuring adaptability and responsiveness in meeting evolving demands within the blockchain landscape.\\r\\n\\r\\nGluwa\u2019s decentralized financial platform, Gluwa Invest, encourages stable coin investments, while the Creditcoin blockchain records loans, providing transparency and immutability. Together, they\u2019ve facilitated nearly 4.27 million loan transactions, totaling over $79.7 million. Gluwa turned to Azure\u2019s reliable, scalable cloud foundation to make these innovative and socially impactful initiatives a reality. Read more about how [Gluwa uses blockchain to help investors fund loans to the unbanked](https://customers.microsoft.com/en-us/story/1709609183358710412-gluwa-azure-banking-and-capital-markets-usa?ocid=buildia24_60days_blogs).\\r\\n\\r\\n___\\r\\n\\r\\n## Summary\\r\\n\\r\\nAzure services have empowered organizations developing intelligent apps by offering scalability, flexibility, and seamless integration of ML, data analytics, and AI.\\r\\n\\r\\nAzure\u2019s impact extends beyond immediate efficiency gains, empowering developers to iterate, learn, and keep creating new experiences. Businesses that build with Azure services can streamline collaboration across ecosystems, unlocking collective vision and applied creativity.\\r\\n\\r\\nExplore Microsoft [Customer Stories](https://customers.microsoft.com/en-us/home?sq=aks&ff=&p=1&so=story_publish_date%20desc&ocid=buildia24_60days_blogs) for deeper insights into transformative, AI-powered solutions. Finally, don\u2019t forget to mark your calendar for [Azure Kubernetes Day](https://aka.ms/aks-day?ocid=buildia24_60days_blogs) at [KubeCon EU 2024](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/) to get the latest on cutting-edge developments in cloud technology.\\r\\n\\r\\n<sup>1</sup> Source: [Gartner, Demand Grows for Intelligent Applications Powered by AI, September 27, 2023](https://www.gartner.com/en/articles/demand-grows-for-intelligent-applications-powered-by-ai). GARTNER is a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and is used herein with permission. All rights reserved."},{"id":"kick-off","metadata":{"permalink":"/Cloud-Native/60DaysOfIA/kick-off","source":"@site/blog-60daysofIA/2024-02-15/kickoff-blog.md","title":"Kick-off #60Days of IA","description":"Combine the power of AI, cloud-scale data, and cloud-native app development to create highly differentiated digital experiences. Develop adaptive, responsive, and personalized experiences by building and modernizing intelligent applications with Azure.","date":"2024-02-15T09:00:00.000Z","formattedDate":"February 15, 2024","tags":[{"label":"Build-Intelligent-Apps","permalink":"/Cloud-Native/60DaysOfIA/tags/build-intelligent-apps"},{"label":"60-days-of-IA","permalink":"/Cloud-Native/60DaysOfIA/tags/60-days-of-ia"},{"label":"learn-live","permalink":"/Cloud-Native/60DaysOfIA/tags/learn-live"},{"label":"hack-together","permalink":"/Cloud-Native/60DaysOfIA/tags/hack-together"},{"label":"community-buzz","permalink":"/Cloud-Native/60DaysOfIA/tags/community-buzz"},{"label":"ask-the-expert","permalink":"/Cloud-Native/60DaysOfIA/tags/ask-the-expert"},{"label":"azure-kubernetes-service","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-kubernetes-service"},{"label":"azure-functions","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-functions"},{"label":"azure-openai","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-openai"},{"label":"azure-container-apps","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-container-apps"},{"label":"azure-cosmos-db","permalink":"/Cloud-Native/60DaysOfIA/tags/azure-cosmos-db"},{"label":"github-copilot","permalink":"/Cloud-Native/60DaysOfIA/tags/github-copilot"},{"label":"github-codespaces","permalink":"/Cloud-Native/60DaysOfIA/tags/github-codespaces"},{"label":"github-actions","permalink":"/Cloud-Native/60DaysOfIA/tags/github-actions"}],"readingTime":4.69,"hasTruncateMarker":false,"authors":[{"name":"#60Days Of IA","title":"BuildIA Content Team","url":"https://azure.github.io/Cloud-Native/Build-IA/","imageURL":"https://azure.github.io/Cloud-Native/img/logo-2024.png","key":"cnteam"}],"frontMatter":{"date":"2024-02-15T09:00","slug":"kick-off","title":"Kick-off #60Days of IA","authors":["cnteam"],"draft":false,"hide_table_of_contents":false,"toc_min_heading_level":2,"toc_max_heading_level":3,"keywords":["Cloud-Scale","Data","AI","AI/ML","intelligent apps","cloud-native","60-days","enterprise apps","digital experiences","app modernization"],"image":"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png","description":"Combine the power of AI, cloud-scale data, and cloud-native app development to create highly differentiated digital experiences. Develop adaptive, responsive, and personalized experiences by building and modernizing intelligent applications with Azure.","tags":["Build-Intelligent-Apps","60-days-of-IA","learn-live","hack-together","community-buzz","ask-the-expert","azure-kubernetes-service","azure-functions","azure-openai","azure-container-apps","azure-cosmos-db","github-copilot","github-codespaces","github-actions"]},"unlisted":false,"prevItem":{"title":"1. Harnessing the power of Intelligent Apps","permalink":"/Cloud-Native/60DaysOfIA/harnessing-the-power-of-intelligent-apps"}},"content":"<head> \\r\\n  <meta property=\\"og:url\\" content=\\"https://azure.github.io/cloud-native/60daysofia/kick-off\\"/>\\r\\n  <meta property=\\"og:type\\" content=\\"website\\"/> \\r\\n  <meta property=\\"og:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\"/> \\r\\n  <meta property=\\"og:description\\" content=\\"Join us on a learning journey to build intelligent apps on Azure. Read all about the upcoming #BuildIntelligentApps initiative on this post!\\"/> \\r\\n  <meta property=\\"og:image\\" content=\\"https://github.com/Azure/Cloud-Native/blob/main/website/static/img/ogImage.png\\"/> \\r\\n  <meta name=\\"twitter:url\\" content=\\"https://azure.github.io/Cloud-Native/60daysofIA/kick-off\\" /> \\r\\n  <meta name=\\"twitter:title\\" content=\\"Build Intelligent Apps | AI Apps on Azure\\" /> \\r\\n  <meta name=\\"twitter:description\\" content=\\"Join us on a learning journey to build intelligent apps on Azure. Read all about the upcoming #BuildIntelligentApps initiative on this post!\\" /> \\r\\n  <meta name=\\"twitter:image\\" content=\\"https://azure.github.io/Cloud-Native/img/ogImage.png\\" /> \\r\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" /> \\r\\n  <meta name=\\"twitter:creator\\" content=\\"@devanshidiaries\\" /> \\r\\n  <link rel=\\"canonical\\" href=\\"https://azure.github.io/Cloud-Native/60daysofIA/kick-off\\" /> \\r\\n</head> \\r\\n\\r\\n\x3c!-- End METADATA --\x3e\\r\\n\\r\\nLet\u2019s ride the buzz of AI with the focus on building intelligent apps using cloud-native technologies. Build \'#IntelligentApps\' brings to you a learning journey to build your skills on creating differentiated experiences while modernizing your applications. It\u2019s time to \'learn it all\'. \\r\\n\\r\\n## What We\u2019ll Cover\\r\\n\\r\\n* What is Build Intelligent Apps?\\r\\n* How Can I *participate*?\\r\\n* How Can I *skill up*? (in just 60 Days)\\r\\n* **Exercise:** Take the [Build Intelligent Apps Skills Challenge](https://aka.ms/build-ia/csc)\\r\\n\\r\\n![Build intelligent apps](../../static/img/60-days-of-ia/60-days-of-ia-cloud-skills-banner.jpg)\\r\\n\\r\\n## Get Ready To Build #IntelligentApps starting February 19!\\r\\n\\r\\nToday, we kick off with content and activities for you to skill up on all things Intelligent Apps or AI Apps on Azure with content, events, and community interactions! Read on to learn about what is coming!\\r\\n\\r\\n## Explore Our Initiatives\\r\\n\\r\\nWe have a number of initiatives planned for the month to help you learn and skill up on relevant technologies. Click on the links to visit the relevant pages for each.\\r\\n\\r\\n* [#60Days of IA](https://aka.ms/build-ia/60days) - 8 themed weeks of blogs on AI led application development\\r\\n* [Learn Live Series](https://aka.ms/FallForIA/LearnLive) \u2013 8 weekly live episodes on \'Kubernetes\' and \'Serverless\'\\r\\n* [Ask The Expert](https://aka.ms/build-ia/ATE-series) \u2013 join live Q&A sessions with Product Engineering and Advocacy teams\\r\\n* [Cloud Skills Challenge](https://aka.ms/build-ia/csc) \u2013 skill up by competing with peers to complete modules\\r\\n\\r\\n![Build intelligent apps](../../static/img/60-days-of-ia/60-days-of-ia-cloud-skills-modules.png)\\r\\n\\r\\n:::info\\r\\n## Register for the events!\\r\\n\\r\\nWhat are 4 things you can do today, to jumpstart your learning journey?\\r\\n\\r\\n* **Register** for live Q&A sessions (free, online) \\r\\n  * February 29 \u2013 [Ask The Expert: Intelligent Apps with Azure Kubernetes Service](https://aka.ms/intelligent-apps/ate-aks/?ocid=buildia24_60days_blogs)\\r\\n  * March 7 \u2013 [Ask The Expert: Intelligent Apps with Azure Cosmos DB](https://aka.ms/intelligent-apps/ate-cosmos/?ocid=buildia24_60days_blogs)\\r\\n  * March 21 - [Ask The Expert: Intelligent Apps with Azure AI](https://aka.ms/intelligent-apps/ate-ai/?ocid=buildia24_60days_blogs) \\r\\n  * April 4 \u2013 [Ask The Expert: Intelligent Apps with Azure Functions](https://aka.ms/intelligent-apps/ate-functions/?ocid=buildia24_60days_blogs)\\r\\n* **Register** for the [Learn Live Series: Kubernetes Edition](https://aka.ms/intelligent-apps/aks-learnlive?ocid=buildia24_LL_website&ocid=buildia24_60days_blogs) \u2013 weekly live learning \\r\\n  * February 21 \u2013 [Episode 1: Deploying Intelligent Apps with OpenAI on AKS](https://aka.ms/learn-live-building-intelligent-apps-aks-ep1?ocid=buildia24_60days_blogs) \\r\\n  * February 28 \u2013 [Episode 2: Bring Your Own AI Models to Intelligent Apps on AKS with KAITO](https://aka.ms/learn-live-building-intelligent-apps-aks-ep2?ocid=buildia24_60days_blogs)\\r\\n  * March 6 \u2013 [Episode 3: Enhance Observability of Your Intelligent Apps on AKS](https://aka.ms/learn-live-building-intelligent-apps-aks-ep3?ocid=buildia24_60days_blogs)\\r\\n  * March 13 \u2013 [Episode 4: Taking Your Intelligent App Global with AKS](https://aka.ms/learn-live-building-intelligent-apps-aks-ep4?ocid=buildia24_60days_blogs)\\r\\n* **Register** for the [Azure Kubernetes Day at KubeCon, EU](https://aka.ms/aks-day?ocid=buildia24_60days_blogs) to meet the product engineering teams in-person and learn about the new product capabilities for intelligent apps.\\r\\n* **Complete** the [Cloud Skills Challenge](https://aka.ms/intelligent-apps/apps-csc?ocid=buildia24_60days_blogs) to earn a Microsoft Learn badge \u2013 ends on *April 15*!\\r\\n:::\\r\\n\\r\\n## #60Days Of Intelligent Apps\\r\\n\\r\\n[#60Days of IA](https://aka.ms/build-ia/60days) is a series of blog posts grouped into themed weeks - taking you from core concepts to end-to-end solution examples in 60 days. Each blog will provide conceptual lessons paired with exercises and resources to help you reinforce learnings and take next steps.\\r\\n\\r\\nThis series takes you through learning journey in\u202f**eight stages**, each building on the previous week to help you skill up in a beginner-friendly way:\\r\\n\\r\\n* **Week 1**: Power of [Intelligent Applications](https://azure.microsoft.com/en-us/blog/build-next-generation-ai-powered-applications-on-microsoft-azure/?ocid=buildia24_60days_blogs)\\r\\n* **Week 2**: [Azure Kubernetes Service](https://learn.microsoft.com/en-us/azure/aks/?ocid=buildia24_60days_blogs) is the platform of choice for intelligent apps\\r\\n* **Week 3**: [Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction?ocid=buildia24_60days_blogs) is the database of choice for intelligent apps\\r\\n* **Week 4**: [Azure AI](https://learn.microsoft.com/en-us/azure/ai-services/ai-services-and-ecosystem?ocid=buildia24_60days_blogs) is your choice for [Responsible AI](https://www.microsoft.com/en-us/ai/responsible-ai?ocid=buildia24_60days_blogs)\\r\\n* **Week 5**: Building an intelligent [serverless](https://azure.microsoft.com/en-us/solutions/serverless/?ocid=buildia24_60days_blogs) event driven [functions app](https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview?pivots=programming-language-csharp?ocid=buildia24_60days_blogs)\\r\\n* **Week 6**: Build intelligent microservices with [serverless containers](https://learn.microsoft.com/en-us/azure/container-apps/overview?ocid=buildia24_60days_blogs)\\r\\n* **Week 7**: [Platform engineering](https://learn.microsoft.com/en-us/platform-engineering/?ocid=buildia24_60days_blogs) needs for your intelligent apps\\r\\n* **Week 8**: Managing cost for your intelligent apps \\r\\n\\r\\nWe will start with defining intelligent apps and then expand on how to build with cloud-native technologies like [Azure Kubernetes Service](https://azure.microsoft.com/en-us/products/kubernetes-service/?WT.mc_id=javascript-99907-ninarasi&ocid=buildia24_60days_blogs), [Azure Container Apps](https://azure.microsoft.com/en-us/products/container-apps/?WT.mc_id=javascript-99907-ninarasi&?ocid=buildia24_60days_blogs) and [Azure Functions](https://azure.microsoft.com/en-us/products/functions?WT.mc_id=javascript-99907-ninarasi&?ocid=buildia24_60days_blogs), as well as integrate AI and cloud-scale data. You will learn how to build end-to-end scenarios for real world application development based on [reference architectures](https://learn.microsoft.com/en-us/azure/architecture/??ocid=buildia24_60days_blogs). Before we dive deep on intelligent apps, here is a high-level overview of the **Intelligent Apps** landscape on Azure for you to leverage the most comprehensive, trusted cloud to prime the customer and employee experiences.\\r\\n\\r\\n![intelligent apps on Azure](../../static/img/60-days-of-ia/blogs/2024-02-15/intelligent-apps-on-azure.png)\\r\\n\\r\\nBring your applications to a modern application platform in the cloud, which leverages a cloud data platform at scale and agile development methods with DevOps is the best way to prime the customer and employee experiences. Azure offers the latest apps, data, AI and is the most comprehensive, trusted cloud.\\r\\n\\r\\n![intelligent apps](../../static/img/60-days-of-ia/blogs/2024-02-15/intelligent-apps.png)\\r\\n\\r\\n**Containers on Azure**\u202fservices offer you a wide range of capabilities, from simplicity to control to suit your different needs.\\r\\n\\r\\n![containers on azure](../../static/img/fallforia/blogs/2023-09-17/Containers-on-Azure.jpg)\\r\\n\\r\\n![containers on azure](../../static/img/fallforia/blogs/2023-09-17/Containers-on-Azure-2.jpg)\\r\\n\\r\\nTo start with the basics for developing [Kubernetes](https://azure.microsoft.com/en-us/products/kubernetes-service/?WT.mc_id=javascript-99907-ninarasi&ocid=buildia24_60days_blogs) applications, explore [#30Days of CloudNative](https://azure.github.io/Cloud-Native/cnny-2023).\\r\\n\\r\\nCloud-native development when paired with **serverless computing** enhances your solution architecture for building cost optimized, resilient applications.\\r\\n\\r\\n![serverless on Azure](../../static/img/60-days-of-ia/blogs/2024-02-15/serverless-on-azure.jpg)\\r\\n\\r\\nTo start with the basics for [serverless computing](https://azure.microsoft.com/solutions/serverless/?WT.mc_id=javascript-99907-ninarasi&?ocid=buildia24_60days_blogs), explore [#30DaysOfServerless](https://azure.github.io/Cloud-Native/blog).\\r\\n\\r\\n## Let\u2019s Get Started\\r\\n\\r\\nNow you know everything! We hope you are as excited as we are to dive into a full month of active learning and doing! Don\'t forget to\u202f[subscribe](https://azure.github.io/Cloud-Native/60DaysOfIA/rss.xml)\u202ffor updates in your favorite feed reader.\u202f**And, look out for our first Intelligent Apps blog on Monday, February 19!**"}]}')}}]);